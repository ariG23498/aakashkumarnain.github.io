[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "papers\n\n\nsummary\n\n\ntransformers\n\n\nresearch\n\n\nLLMs\n\n\n \n\n\n\n\nOct 23, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\ntransformers\n\n\nresearch\n\n\nLLMs\n\n\nmodel_merging\n\n\n \n\n\n\n\nOct 15, 2024\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html#paper-summaries",
    "href": "blog.html#paper-summaries",
    "title": "Blog Posts",
    "section": "",
    "text": "papers\n\n\nsummary\n\n\ntransformers\n\n\nresearch\n\n\nLLMs\n\n\n \n\n\n\n\nOct 23, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\ntransformers\n\n\nresearch\n\n\nLLMs\n\n\nmodel_merging\n\n\n \n\n\n\n\nOct 15, 2024\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Normalized Transformer\n\n\n\n\n\n\n\n\n\nOct 23, 2024\n\n\n\n\n\n\n\n\nWhat Matters for Model Merging at Scale?\n\n\n\n\n\n\n\n\n\nOct 15, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/paper_summaries/nGPT.html",
    "href": "posts/paper_summaries/nGPT.html",
    "title": "Normalized Transformer",
    "section": "",
    "text": "arXiv\nWe have been using transformers daily for a couple of years now, but the advancements on the architectural side have remained minimal. This paper from Nvidia proposes a normalized Transformer (nGPT), which performs representation learning on a hypersphere. Here is a summary for the same:\n\nToken embeddings and output logits\n\nBoth input and output embedding matrices are normalized after each training step.\nThe logits are bounded in the range [-1, 1] because of the normalization, which limits the confidence of the probability distribution generated by the softmax.\nTo adjust this during training, the authors introduce a trainable scaling parameter sz that scales the logits element-wise.\n\n\n\nLayers and Blocks\n\nA typical transformer block looks like this where L layers of transformations are applied to the hidden state h, consisting of alternating the self-attention and MLP blocks: \nIf we are on a hypersphere, we can use spherical linear interpolation to compute the geodesic distance between two points a and b on the hypersphere. We can approximate SLERP with approximate linear interpolation (LERP) as shown below: \nIf point a is our hidden state h, and point b represents the point suggested by the attention or MLP block, we can represent our equations for the transformer blocks like this: \nHere αA ≥0 and αM ≥0, with dimensionality dmodel, are learnable parameters applied to the normalized outputs of the attention and MLP blocks, respectively. The norm function normalizes any vector x to have a unit norm and, unlike RMSNorm or LayerNorm, does not introduce any element-wise scaling factors. This normalization can be viewed as the retraction step in Riemannian optimization, mapping the updated solution back to the manifold.\n\n\n\nSelf-Attention Blocks\n\nThe qkv values produced by the weight matrics Wq, Wk, and Wv in the original transformer are unconstrained, leading to unbounded values in q.\nIn nGPT the authors normalize Wq, Wk, Wv and Wo along their embedding dimension so that the computed dot products with h can be interpreted as cosine similarity between unit norm vectors bounded in [−1, 1]. Thus, all attention matrices can be viewed as collections of normalized embedding vectors to be compared.\nThough each element of q and k is now bounded, the norms of these two vectors can still vary. Also, the addition of positional embeddings can further distort q and k. To this end, the authors additionally normalize q and k by introducing a scaling factor sqk for each head, ensuring that the dot product of every query and key is under control.\nIn the original Transformer, the query-key dot product is scaled by 1/√dk before applying softmax to account for the expected variance of dk in the dot product of non-normalized query and key vectors. In the normalized Transformer, the expected variance of the dot product between the normalized query and key vectors is 1/dk. The softmax scaling factor should instead be √dk to restore a variance of 1\n\n\n\n\nMLP\n\nThe input hidden state h of the MLP block of a classical transformer is first normalized using RMSNorm(or LayerNorm) and then passed through two separate linear projections, producing two intermediate vectors u and v, which are then combined using SwiGLU.\nThe weight matrices Wu and Wv in nGPT are normalized. The authors introduce scaling factors su and sν to control their impact. They also rescale ν by √dmodel to optimize SiLU performance.\n\n\n\n\nSummary of all modifications\n\nRemove all normalization layers like RMSNorm or LayerNorm.\nAfter each training step, normalize all matrices (Einput, Eoutput, Wq, Wk, Wv, Wo, Wu, Wν, and Wo) along their embedding dimension.\nReplace the updates as follows where αA (and also αM) is treated with αA,init = 0.05 (in order of 1/n_layers) and αA,scale = 1/√dmodel.\nChange the softmax scaling factor in attention from 1/√dk to √dk.\nImplement the rescaling and normalization of q and k where sqk is treated with sqk,init = 1 and sqk,scale = 1/√dmodel.\nImplement the rescaling of the intermediate state of the MLP block where su (and also sν) is treated with su,init = 1 and su,scale = 1\nImplement the rescaling of logits using equation 3, where sz is treated with sz,init = 1, sz,scale = 1/√dmodel.\nRemove weight decay and learning rate warmup.\n\n\n\n\nHow fast is nGPT compared to GPT?\nA lot! Training the 0.5B and 1B nGPT models is approximately 4x, 10x, and 20x faster at context lengths of 1k, 4k, and 8k tokens, respectively.\n\n\n\nWhat about the params and hparams?\n  \n\n\nThere should be a catch somewhere, right?\nOf course, there is always a catch! Quoting directly from the paper: The time cost per step for nGPT is approximately 80% higher with a 4k context length, and 60% higher with an 8k context length. This overhead is not only due to nGPT having 6 normalization steps (2 of them are applied for q and k) per layer instead of 2 but also because nGPT’s normalizations are not yet fully optimized, unlike GPT, where normalization layers are fused with other operations.\n\n\n\nBest thing about nGPT?\nIn standard GPTs, perplexity tends to increase dramatically when tested on sequences longer than pre-training length. In contrast, nGPT maintains a stable perplexity range even at extrapolated lengths."
  },
  {
    "objectID": "posts/paper_summaries/model_merging_at_scale.html",
    "href": "posts/paper_summaries/model_merging_at_scale.html",
    "title": "What Matters for Model Merging at Scale?",
    "section": "",
    "text": "arXiv annotated_paper\nYesterday, I read this banger paper titled: What Matters For Model Merging At Scale?. Though I recommend reading the full paper, I am including a summary here in case you are interested in the main points. I will also provide the link to an annotated version of this paper.\n  \n\nIntroduction\nModel merging is not a new concept. It has been tried and tested enough to create a better or more powerful model by combining two or more (expert) models. Model merging has several advantages:\n\nReduced storage and serving costs\nImproved generalization to new tasks due to combined capabilities.\nDecentralized and modular model development.\n\nOne potential gap in this area is the lack of a comprehensive study to evaluate its effectiveness as we scale the model size. Most people are either merging models at a small scale (7B-13B models) or merging a limited number of expert models. This paper provides insights into the scalability of model merging.\n\n\nProblem Statement\n\nFocuses on model merging with large models\nN expert tasks and a base model\nExpert for each of these N tasks obtained by fully fine-tuning the base model on a specific expert task.\nFour merging methods, including Averaging, Task Arithmetic, TIES, and DARE.\n\n\n\nExperimental Design for Large-Scale Evaluation of Model Merging\n\nData\n\nData settings from the T0 mixture containing 8 held-in and 4 held-out task categories.\nThe 8 held-in task categories (with a total of 16 datasets) include Multiple-choice QA, Extractive Qa, Closed-Book QA, Sentiment Analysis, Topic Classification, Structure-to-text, Summarization, and Paraphrase Identification. The 4 held-out task categories are Sentence Completion, Natural Language Inference, Co-reference Resolution, and Word Sense Disambiguation.\n\n\n\n\nModels\n\nPaLM-2 with sizes 1B, 8B, 24B, and 64B as the base models. For all these models, the authors also build an instruction-tuned version of PaLM-2-IT.\nFor each of the two base model types(non-IT Vs IT) and 4 model sizes, they perform full fine-tuning on the 8 held-in task categories resulting in 64 specialized expert models.\nThe authors create a large merging experiment grid with the two base models (PaLM-2 and PaLM-2-IT), four model sizes (1B, 8B, 24B, 64B), four Merging methods (Averaging, Task Arithmetic, Dare-TIES, and TIES), the number of constituent models (2, 4, 6, 8), and 3 seeds to randomly select the constituent tasks for the experiment resulting in a total of 384 merging experiments.\n\n\n\n\n\nEvaluation\n\nPerformance evaluated on both held-in and held-out tasks\n∼9000 model evaluations across all the experiments.\n\n\n\n\nMetrics\n\nFor held-in tasks, the merged model performance is normalized against the corresponding task expert model’s performance.\nFor held-out tasks, normalization is performed relative to the base model’s performance.\n\n\n\n\nExperimental Results and Insights\n\nInstruction-Tuned Models Facilitate Easier Merging\n\nMerging experiments done with fully fine-tuned experts from PaLM-2 and PaLM-2-IT\nHeld-in performance is measured over three trials to minimize the impact of selected expert models and their data distributions.\nPaLM-2-IT models consistently outperform PaLM-2 base models for all merging methods. The authors think large-scale instruction tuning further disentangles model weights, facilitating effective model merging and improving the base model zero-shot performance.\n\n\n\n\nModel Merging Becomes Easier With Bigger Models\n\nThe authors noticed that merged models outperform their corresponding base models in zero-shot generalization to held-out tasks irrespective of the model size, merging method, or number of constituent models.\nFor weak base models (PaLM-2), increasing model size significantly improved the merged model performance over the base model. Strong base models (PaLM-2-IT) show a different trend, and the zero-shot generalization improves monotonically with more expert models.\n\n\n\n\nBigger Model Sizes Can Merge More Experts\n\nFor weak base models (PaLM-2) that are of small size (1B-8B), merging more models leads to a significant drop in performance, whereas for strong base models (PaLM-2), the drop is negligible.\nThe above trend doesn’t hold for bigger model sizes (64B). Merging more experts for a weak base model (PaLM-2 64B) leads to significant improvements in performance, whereas for strong base models(PaLM-2-IT), it leads to better generalization.   \n\nMerging Methods Become Similar at Scale\n\nAt scale, all merging methods for strong base models exhibit very similar performance, suggesting that we can simply use Averaging strategy to get the optimal performance at scale.   \n\n\n\n\nResults"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About me",
    "section": "",
    "text": "Hello, I am Aakash Kumar Nain.\nI am a Machine Learning Engineer with 8 years of experience in the industry. Currently, I am working as a Senior ML Engineer at Merlyn Mind. I am also a Google Developers Expert in Machine Learning and JAX. I actively contribute to Keras, TensorFlow, and the JAX ecosystem. I am one of the core collaborators for the Keras core project, and I am also one of the maintainers of the TensorFlow-addons package.\nIn terms of work, my knowledge Machine Learning and Deep Learning spans in T-shape. I have worked in multiple domains, but Computer Vision with Deep Learning is my favorite field. I love simplifying Data Science and Machine Learning concepts, especially the maths behind them. Love and support OSS because, without OSS, we wouldn’t have witnessed so many breakthroughs in Machine Learning and Deep Learning. Python is my favorite language to work with, and I absolutely love coding in it. Always up for a discussion involving anything related to Machine Learning, Deep Learning, MLOps, and API design. Simply put: Python in breakfast, Machine Learning in lunch, and Deep Learning in dinner!\nFollow me on Twitter to get the latest updates related to Machine Learning."
  },
  {
    "objectID": "index.html#popular-projects",
    "href": "index.html#popular-projects",
    "title": "About me",
    "section": "Popular Projects",
    "text": "Popular Projects\nAnnotated Research Papers   \nMost researchers and Machine Learning engineers want to remain up-to-date with the latest research work, but the field of machine learning moves crazy fast. The number of papers uploaded on arXiv is witnessing exponential growth. Reading all these published papers is an impossible task. To help with this, I maintain this repository where I annotate and upload the papers that I find exceptionally good. Annotations make it easier to understand and grasp the main concepts explained in the paper. \nTF-JAX Tutorials   \nIt is a series of tutorials built to teach the fundamental concepts and the underlying working of two famous libraries: TensorFlow and JAX. These tutorials aren’t typical documentation-type tutorials. It doesn’t matter whether you are a beginner or an advanced user, these tutorials will give you a fresh perspective on building things using TensorFlow or JAX. \nDiffusion Models   \nDiffusion models are a class of likelihood-based generative models that recently have been used to produce very high-quality images compared to other existing generative models like GANs. It’s hard to find quality resources to learn about diffusion models. The mathematics behind the diffusion models is also a bit harder to understand. The material presented in this repo is enough to make you understand the working of diffusion models and the underlying math. \nMistral-7B in JAX   \nThis is a direct port of Mitral-7B model in JAX and Equinox. This port comes with two implementations: The first one shows how to port models from PyTorch to Equinox and JAX. This is a 1:1 mapping, and not fully optimized. The second implementation is fully optimized for JAX, targeted for the more advanced users."
  },
  {
    "objectID": "index.html#keras-core-contributions",
    "href": "index.html#keras-core-contributions",
    "title": "About me",
    "section": "Keras Core Contributions",
    "text": "Keras Core Contributions\n\nJAX NN ops\nMerging layers\nMetrics\nLoss functions\nApplications\nData adapters\nCode examples"
  },
  {
    "objectID": "index.html#keras-contributions",
    "href": "index.html#keras-contributions",
    "title": "About me",
    "section": "Keras Contributions",
    "text": "Keras Contributions\n\nDenoising Diffusion Models\nOCR model for reading captchas\nHandwriting recognition\nImage captioning\nWGAN-GP\nCycleGAN\nModel interpretability with Integrated Gradients"
  },
  {
    "objectID": "index.html#equinox-contributions",
    "href": "index.html#equinox-contributions",
    "title": "About me",
    "section": "Equinox Contributions",
    "text": "Equinox Contributions\n\nMistral-7B implementation\nBug fix in RMSNorm Layer\nAMP in Normalization Layers"
  },
  {
    "objectID": "index.html#tensorflow-contributions",
    "href": "index.html#tensorflow-contributions",
    "title": "About me",
    "section": "TensorFlow Contributions",
    "text": "TensorFlow Contributions\n\nKappa\nGelu activation\nFocal loss\nThresholded Linear Unit"
  }
]