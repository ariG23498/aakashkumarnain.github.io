[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "papers\n\n\nsummary\n\n\nresearch\n\n\ntransformers\n\n\nscaling\n\n\n \n\n\n\n\nNov 4, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nVLMs\n\n\nMLLMs\n\n\n \n\n\n\n\nOct 28, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\ntransformers\n\n\nresearch\n\n\nLLMs\n\n\n \n\n\n\n\nOct 23, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\ntransformers\n\n\nresearch\n\n\nLLMs\n\n\nmodel_merging\n\n\n \n\n\n\n\nOct 15, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nagents\n\n\n \n\n\n\n\nSep 24, 2024\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html#paper-summaries",
    "href": "blog.html#paper-summaries",
    "title": "Blog Posts",
    "section": "",
    "text": "papers\n\n\nsummary\n\n\nresearch\n\n\ntransformers\n\n\nscaling\n\n\n \n\n\n\n\nNov 4, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nVLMs\n\n\nMLLMs\n\n\n \n\n\n\n\nOct 28, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\ntransformers\n\n\nresearch\n\n\nLLMs\n\n\n \n\n\n\n\nOct 23, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\ntransformers\n\n\nresearch\n\n\nLLMs\n\n\nmodel_merging\n\n\n \n\n\n\n\nOct 15, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nagents\n\n\n \n\n\n\n\nSep 24, 2024\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "A Hitchhiker’s Guide to Scaling Law Estimation\n\n\n\n\n\n\n\n\n\nNov 4, 2024\n\n\n\n\n\n\n\n\nOmniParser for Pure Vision Based GUI Agent\n\n\n\n\n\n\n\n\n\nOct 28, 2024\n\n\n\n\n\n\n\n\nNormalized Transformer\n\n\n\n\n\n\n\n\n\nOct 23, 2024\n\n\n\n\n\n\n\n\nWhat Matters for Model Merging at Scale?\n\n\n\n\n\n\n\n\n\nOct 15, 2024\n\n\n\n\n\n\n\n\nAgent WorkFlow Memory\n\n\n\n\n\n\n\n\n\nSep 24, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/paper_summaries/scaling_laws_estimation.html",
    "href": "posts/paper_summaries/scaling_laws_estimation.html",
    "title": "A Hitchhiker’s Guide to Scaling Law Estimation",
    "section": "",
    "text": "arXiv\nScaling laws have been discussed a lot in the past few years. The OG paper on scaling laws is still one of the best, but this latest paper from IBM and MIT provides a fresh perspective. Here is a quick summary in case you are interested:\n\nWhy should one care about scaling laws?\n\nDesign choices like architectural modification, data changes, etc., are crucial, but every option is expensive to evaluate at scale.\nWhat if we use smaller models to assess the design choices? Will the findings be upheld when using a larger model?\nA scaling law extrapolates the performance of a target model from the performance of a set of models with fewer parameters or smaller training sets. A high-quality scaling law accurately predicts the test performance of the target model.\n\n\n\n\nDefining a scaling law\n\nA scaling law estimates the loss of a costly model by training cheaper ones that share a pretraining procedure and differ by some hyperparameters, typically model size (num_params) and number of tokens seen during training (num_toks).\nThe authors define a scaled model family f as a set of models, with each f ∈ F differing only in size num_params(f ) and number of tokens num_toks(f ).\nThey divide this into subsets. First, the maximal parameter family max params (F) containing models in F with the largest number of parameters. This family will generally include the target model(s) whose behavior we wish to predict t ∈ Ftarget. Second, the q-maximal token family max num_toks (F, q) contains all models in f trained on at least a q-sized fraction of the training set.\nA scaling law L(f | F ) estimates the performance of a new model f given a model family F and is defined as shown below:\n\n  \nE is a baseline capturing the general performance of the scaled family. A and α describe the scaling effect of num_params, while B and β describe the scaling effect of num_toks. These parameters are estimated by first collecting a set of training models Ftrain, then minimizing the reconstruction error as shown above. L(f ) denotes the empirical negative log-likelihood of some held-out data under the model.\n\n\nHow Well Can I Expect a Scaling Law to Predict?\n\nTo estimate this, we need to know what is considered a meaningful change when comparing two models (of the same family). The authors found that any design change that yields less than a 4% change in performance has not been considered meaningful in the literature.\nAlso, the variance across the same model with restarts reaches a 3.5% difference. Thus, a 4% difference bounds the best goodness of fit we should expect or require of scaling laws.\nThe Absolute Relative Error (ARE) is bounded by 4% and can reach 20% with certain design choices.\n\n\n\n\nWhen I Train a New Model, Do I Even Need a New Scaling Law?\n\nDifferent model families exhibit different scaling behavior.\nEvery design choice like the choices of architecture, training procedure, or the dataset, affects the form of scaling laws. Hence, the behavior of a new model family may require a new scaling law.\nCan’t we borrow the scaling behaviors between two different model families even if it results in poor or biased estimates? To that end, the authors set the num_params scaling parameters (A, α) to fixed values reported in the paper titled Scaling data-constrained language models and estimate the remaining parameters for individual model families.\nThe authors found that predictions generalize, and a constant num_params scaling factor is enough for most models (except the encoder-decoder T5-Pile). However, error rates are bigger than in the source family, and predictions for larger models are worse.\n\n  \n\n\nCan I Train the Target Model a Bit Instead of Many Small Models?\n\nA reasonable assumption to make at this point is that training the target model for a bit can be much better than wasting time on training many small models.\nA good thing about the above assumption is that extrapolation is only required for num_tokens as the number of parameters is fixed now.\nAs of now, reliable estimates with this approach require up to 30% of the full training run, but this is a good option for future research. \n\n\n\nAre Even Simpler Baselines Enough?\n\nLet us take another example. Suppose we have two models belonging to the same small model family. One model has the best performance, and the other model has seen more data. Are these baselines enough to estimate the performance of the target model without fitting a scaling law?\nThe model with the best performance is expected to be closer to the performance of the target model. The authors found that the baselines suffer more than 15% error, mostly above 10%, rarely get below 5%, and 18% ARE is observed on average across all scaled families.\n\n  \n\n\nWhich is good for estimates: Intermediate checkpoints or the final checkpoint?\n\nA common methodology is to train different models with different numbers of tokens. Most people never use intermediate checkpoints as the assumption is the learning rate schedule renders losses from intermediate checkpoints uninformative.\nThe authors find that almost all, except for the first few checkpoints, help scale laws. Specifically, models trained on less than 10B tokens are noisy and should be excluded from scaling laws. These models or checkpoints can introduce significant ARE in estimating the performance of the target model.\n\n  \n\n\nHow Big a Model Should I Train?\n\nIt is implicit that bigger models where the num_params are closer to the target model are more effective for reliable scaling laws.\nHowever, the authors found that the effect is neither strong nor monotonic and varies between model families. For example, fitting on all F results in the lowest ARE in most cases. However, in the case of GPT, Gopher, and OPT, predicting with the smallest four models is enough to achieve less than 10% error. OTOH, the smallest models in Pythia are not predictive enough.\nThe authors suggest diversifying and training each model on differing hyperparameters (seed, num_params, num_tokens) and maximizing the information gained.\n\n  \n\n\nHow Many Models Are Required for Reliable Predictions?\n\nLarger models are better, but a group of smaller models is equally good.\nIncreasing the number of models helps decrease ARE. Overall, five models are a safe bet, and more would improve the robustness of the result. And these models can be small. \n\n\n\nWhat Parameters Do I Need to Estimate?\n\nThe authors computed the PCA of five learned parameters and found that three parameters explain 99.49% of the variance.\nThough the above finding holds for a large number of model families, exceptions exist, especially in encoder-decoder models like T5-Pile.\nOverall, scaling laws might have fewer degrees of freedom than described in the literature. num_params and num_tokens remain the most important ones. The authors left out the learning rate schedule for future research."
  },
  {
    "objectID": "posts/paper_summaries/omniparser.html",
    "href": "posts/paper_summaries/omniparser.html",
    "title": "OmniParser for Pure Vision Based GUI Agent",
    "section": "",
    "text": "arXiv\nWith the Ferret-v2 paper, Apple demonstrated the power of building multimodal models for mobile devices. Now, Microsoft has done the same, but for the desktop UI. Presenting Omniparser, the latest advancement from Microsoft for vision-based multimodal workflows. Here is a summary in case you are interested:\n\nWhy does UI understanding matter?\nSince the last few months, there has been a lot of buzz about agents and system 2. A common theme across these explorations is that we want to automate many complex yet mundane tasks with the current generation of LLMs and MLLMs.\nFor example, an automatic trip planner is an excellent idea. You provide the trip details e.g. starting-ending dates, places you want to explore, etc. The system takes this information as input and completes the whole task of planning the trip, including but not limited to booking flight tickets, hotels, etc.\nSome of the sub-tasks in the above example can be completed using DOM alone, but things like captcha completion require visual understanding.\n\n\nHypothesis\nMultimodal models like GPT-4V, when asked to predict the xy coordinates of the interactable regions on the screen directly from UI (e.g. screenshots of a web page) perform poorly because of their inability to correctly identify the semantic information associated with the icons and other elements on the screen.\n\n\nMethod\nTo elevate the above problem, the authors suggest to break down the task into two steps:\n\nUnderstand the content present on screen at the current step to figure out the interactable regions and their corresponding functionality.\nBased on the above information, predict the next action required to complete the task.\n\nTo achieve the above workflow, OmniParser combines the outputs from three different models:\n\nInteractable icon detection model.\nIcon description model.\nOCR module\n\nThe output is a structured DOM-like representation of the UI, with a screenshot overlaid with bounding boxes for potential interactable elements.\n  \n\n\nInteractable Region Detection\n\nUses the Set-of-Marks approach to overlay bounding boxes of interactable icons on top of UI screenshots.\nFinetune a detection model to extract interactable icons/buttons.\nThe authors curated a dataset of interactable icon detection dataset, containing 67k unique screenshot images, each labeled with bounding boxes of interactable icons derived from the DOM tree.\nFinetune the YOLOv8 model on the interactable icon region detection dataset for 20 epoch with a batch size of 256, learning rate of 1e−3, and the Adam optimizer on 4 GPUs.\nAn OCR module is also used to extract bounding boxes of text. The authors merge the bounding boxes from the OCR detection module and icon detection module while removing the boxes that have high overlap (90% as a threshold). Every bounding box is assigned a unique ID.\n\n\n\nIncorporating Local Semantics of Functionality\n\nIncorporate local semantics directly into the prompt.\nGenerate a description of functionality using another model for each of the interactable regions detected by the interactable region detection model. For text boxes, detected text and the labels are injected into the prompt.\nUses BLIP-v2 as the icon description model for generating descriptions for the interactable regions.\nFor fine-tuning BLIP-v2, the authors curate a dataset of 7k icon-description pairs using GPT-4o. For the description, the authors ask GPT-4o whether the object presented in the parsed bounding box is an app icon. If GPT-4o decides the image is an icon, it outputs a sentence description of the icon about the potential functionality. And if not, GPT-4o will output ’this is not an icon’, while still including this in the dataset.\nBLIP-v2 is fine-tuned on this dataset for 1 epoch with a constant learning rate of 1e−5, no weight decay, and Adam optimizer.\n\n  \n\n\nResults\n  \n\n\nFailure Modes\n\nRepeated Icons/Texts: When the screenshot contains repeated elements, and it is required to click/select one of the elements, GPT-4V still fails many times. More granularity in the description of the repeated elements may alleviate the problem.\nCoarse Prediction of Bounding Boxes: The current OCR cannot differentiate between normal text and hyperlinks, hence in some cases elements of the predicted coordinatese of the clickable texts may not be correct.\nIcon Misinterpretation: Given that the icon-description model only sees a part (elements with a bbox) of the full screenshot, the lack of a global view can lead to a wrong description of the detected interactable icons. For example, three dots can be pretty confusing for the current description model"
  },
  {
    "objectID": "posts/paper_summaries/nGPT.html",
    "href": "posts/paper_summaries/nGPT.html",
    "title": "Normalized Transformer",
    "section": "",
    "text": "arXiv\nWe have been using transformers daily for a couple of years now, but the advancements on the architectural side have remained minimal. This paper from Nvidia proposes a normalized Transformer (nGPT), which performs representation learning on a hypersphere. Here is a summary for the same:\n\nToken embeddings and output logits\n\nBoth input and output embedding matrices are normalized after each training step.\nThe logits are bounded in the range [-1, 1] because of the normalization, which limits the confidence of the probability distribution generated by the softmax.\nTo adjust this during training, the authors introduce a trainable scaling parameter sz that scales the logits element-wise.\n\n\n\nLayers and Blocks\n\nA typical transformer block looks like this where L layers of transformations are applied to the hidden state h, consisting of alternating the self-attention and MLP blocks: \nIf we are on a hypersphere, we can use spherical linear interpolation to compute the geodesic distance between two points a and b on the hypersphere. We can approximate SLERP with approximate linear interpolation (LERP) as shown below: \nIf point a is our hidden state h, and point b represents the point suggested by the attention or MLP block, we can represent our equations for the transformer blocks like this: \nHere αA ≥0 and αM ≥0, with dimensionality dmodel, are learnable parameters applied to the normalized outputs of the attention and MLP blocks, respectively. The norm function normalizes any vector x to have a unit norm and, unlike RMSNorm or LayerNorm, does not introduce any element-wise scaling factors. This normalization can be viewed as the retraction step in Riemannian optimization, mapping the updated solution back to the manifold.\n\n\n\nSelf-Attention Blocks\n\nThe qkv values produced by the weight matrics Wq, Wk, and Wv in the original transformer are unconstrained, leading to unbounded values in q.\nIn nGPT the authors normalize Wq, Wk, Wv and Wo along their embedding dimension so that the computed dot products with h can be interpreted as cosine similarity between unit norm vectors bounded in [−1, 1]. Thus, all attention matrices can be viewed as collections of normalized embedding vectors to be compared.\nThough each element of q and k is now bounded, the norms of these two vectors can still vary. Also, the addition of positional embeddings can further distort q and k. To this end, the authors additionally normalize q and k by introducing a scaling factor sqk for each head, ensuring that the dot product of every query and key is under control.\nIn the original Transformer, the query-key dot product is scaled by 1/√dk before applying softmax to account for the expected variance of dk in the dot product of non-normalized query and key vectors. In the normalized Transformer, the expected variance of the dot product between the normalized query and key vectors is 1/dk. The softmax scaling factor should instead be √dk to restore a variance of 1\n\n\n\n\nMLP\n\nThe input hidden state h of the MLP block of a classical transformer is first normalized using RMSNorm(or LayerNorm) and then passed through two separate linear projections, producing two intermediate vectors u and v, which are then combined using SwiGLU.\nThe weight matrices Wu and Wv in nGPT are normalized. The authors introduce scaling factors su and sν to control their impact. They also rescale ν by √dmodel to optimize SiLU performance.\n\n\n\n\nSummary of all modifications\n\nRemove all normalization layers like RMSNorm or LayerNorm.\nAfter each training step, normalize all matrices (Einput, Eoutput, Wq, Wk, Wv, Wo, Wu, Wν, and Wo) along their embedding dimension.\nReplace the updates as follows where αA (and also αM) is treated with αA,init = 0.05 (in order of 1/n_layers) and αA,scale = 1/√dmodel.\nChange the softmax scaling factor in attention from 1/√dk to √dk.\nImplement the rescaling and normalization of q and k where sqk is treated with sqk,init = 1 and sqk,scale = 1/√dmodel.\nImplement the rescaling of the intermediate state of the MLP block where su (and also sν) is treated with su,init = 1 and su,scale = 1\nImplement the rescaling of logits using equation 3, where sz is treated with sz,init = 1, sz,scale = 1/√dmodel.\nRemove weight decay and learning rate warmup.\n\n\n\n\nHow fast is nGPT compared to GPT?\nA lot! Training the 0.5B and 1B nGPT models is approximately 4x, 10x, and 20x faster at context lengths of 1k, 4k, and 8k tokens, respectively.\n\n\n\nWhat about the params and hparams?\n  \n\n\nThere should be a catch somewhere, right?\nOf course, there is always a catch! Quoting directly from the paper: The time cost per step for nGPT is approximately 80% higher with a 4k context length, and 60% higher with an 8k context length. This overhead is not only due to nGPT having 6 normalization steps (2 of them are applied for q and k) per layer instead of 2 but also because nGPT’s normalizations are not yet fully optimized, unlike GPT, where normalization layers are fused with other operations.\n\n\n\nBest thing about nGPT?\nIn standard GPTs, perplexity tends to increase dramatically when tested on sequences longer than pre-training length. In contrast, nGPT maintains a stable perplexity range even at extrapolated lengths."
  },
  {
    "objectID": "posts/paper_summaries/model_merging_at_scale.html",
    "href": "posts/paper_summaries/model_merging_at_scale.html",
    "title": "What Matters for Model Merging at Scale?",
    "section": "",
    "text": "arXiv annotated_paper\nYesterday, I read this banger paper titled: What Matters For Model Merging At Scale?. Though I recommend reading the full paper, I am including a summary here in case you are interested in the main points. I will also provide the link to an annotated version of this paper.\n  \n\nIntroduction\nModel merging is not a new concept. It has been tried and tested enough to create a better or more powerful model by combining two or more (expert) models. Model merging has several advantages:\n\nReduced storage and serving costs\nImproved generalization to new tasks due to combined capabilities.\nDecentralized and modular model development.\n\nOne potential gap in this area is the lack of a comprehensive study to evaluate its effectiveness as we scale the model size. Most people are either merging models at a small scale (7B-13B models) or merging a limited number of expert models. This paper provides insights into the scalability of model merging.\n\n\nProblem Statement\n\nFocuses on model merging with large models\nN expert tasks and a base model\nExpert for each of these N tasks obtained by fully fine-tuning the base model on a specific expert task.\nFour merging methods, including Averaging, Task Arithmetic, TIES, and DARE.\n\n\n\nExperimental Design for Large-Scale Evaluation of Model Merging\n\nData\n\nData settings from the T0 mixture containing 8 held-in and 4 held-out task categories.\nThe 8 held-in task categories (with a total of 16 datasets) include Multiple-choice QA, Extractive Qa, Closed-Book QA, Sentiment Analysis, Topic Classification, Structure-to-text, Summarization, and Paraphrase Identification. The 4 held-out task categories are Sentence Completion, Natural Language Inference, Co-reference Resolution, and Word Sense Disambiguation.\n\n\n\n\nModels\n\nPaLM-2 with sizes 1B, 8B, 24B, and 64B as the base models. For all these models, the authors also build an instruction-tuned version of PaLM-2-IT.\nFor each of the two base model types(non-IT Vs IT) and 4 model sizes, they perform full fine-tuning on the 8 held-in task categories resulting in 64 specialized expert models.\nThe authors create a large merging experiment grid with the two base models (PaLM-2 and PaLM-2-IT), four model sizes (1B, 8B, 24B, 64B), four Merging methods (Averaging, Task Arithmetic, Dare-TIES, and TIES), the number of constituent models (2, 4, 6, 8), and 3 seeds to randomly select the constituent tasks for the experiment resulting in a total of 384 merging experiments.\n\n\n\n\n\nEvaluation\n\nPerformance evaluated on both held-in and held-out tasks\n∼9000 model evaluations across all the experiments.\n\n\n\n\nMetrics\n\nFor held-in tasks, the merged model performance is normalized against the corresponding task expert model’s performance.\nFor held-out tasks, normalization is performed relative to the base model’s performance.\n\n\n\n\nExperimental Results and Insights\n\nInstruction-Tuned Models Facilitate Easier Merging\n\nMerging experiments done with fully fine-tuned experts from PaLM-2 and PaLM-2-IT\nHeld-in performance is measured over three trials to minimize the impact of selected expert models and their data distributions.\nPaLM-2-IT models consistently outperform PaLM-2 base models for all merging methods. The authors think large-scale instruction tuning further disentangles model weights, facilitating effective model merging and improving the base model zero-shot performance.\n\n\n\n\nModel Merging Becomes Easier With Bigger Models\n\nThe authors noticed that merged models outperform their corresponding base models in zero-shot generalization to held-out tasks irrespective of the model size, merging method, or number of constituent models.\nFor weak base models (PaLM-2), increasing model size significantly improved the merged model performance over the base model. Strong base models (PaLM-2-IT) show a different trend, and the zero-shot generalization improves monotonically with more expert models.\n\n\n\n\nBigger Model Sizes Can Merge More Experts\n\nFor weak base models (PaLM-2) that are of small size (1B-8B), merging more models leads to a significant drop in performance, whereas for strong base models (PaLM-2), the drop is negligible.\nThe above trend doesn’t hold for bigger model sizes (64B). Merging more experts for a weak base model (PaLM-2 64B) leads to significant improvements in performance, whereas for strong base models(PaLM-2-IT), it leads to better generalization.   \n\nMerging Methods Become Similar at Scale\n\nAt scale, all merging methods for strong base models exhibit very similar performance, suggesting that we can simply use Averaging strategy to get the optimal performance at scale.   \n\n\n\n\nResults"
  },
  {
    "objectID": "posts/paper_summaries/agent_workflow_memory.html",
    "href": "posts/paper_summaries/agent_workflow_memory.html",
    "title": "Agent WorkFlow Memory",
    "section": "",
    "text": "arXiv annotated_paper\nFor almost six months, I have been vocal about the differentiation between tool usage with LLMs and an agent. Most people correlate pure tool usage with agentic behavior, but IMHO that is only a subset of it. If we want to deploy agents, we need some planning and reasoning capabilities. Reasoning is abstract and hard to solve. Planning on the other hand comes in many flavors. If we want to go to system 2, we must augment planning capabilities with memory. After all, an intelligent system should be able to extract and learn from past behavior.\nThe paper Agent WorkFlow Memory, which came out last week, tries to solve some of these problems, and I think their choice is a good direction.\n  \n\nWhy do we want to implement a memory module?\n\nCurrent language model-based agents struggle with long-horizon tasks and complex action trajectories.\nWhen presented with a new task, a common way to induce knowledge about the task is to perform in-context learning or fine-tuning.\nThis approach works for a given task but fails to generalize across different tasks even if they share the same pattern. For example, planning a trip to a foreign country vs planning a local trip share common workflows with minor differences in the booking procedure.\nInspired by how humans abstract common task routines from past experiences, we can implement a workflow memory module where each workflow represents a goal with a common routine extracted from available action trajectories.\n\n  \n\n\nAgent Workflow Memory\nLet us define the system first. Suppose we have an LLM L, responsible for carrying out the workflows, a text-based Memory M, where the base memory contains documentation of built-in actions such as CLICK and TYPE.\nUNTIL ai != STOP or curr_step &gt; MAX_STEPS_ALLOWED do;\n\nTo solve a task specified by an instruction in natural language q, the agent acts in an environment defined by a transition function T.\nAt each time step ti, the environment state si gives observation oi. This observation is then passed into the model to generate action ai via L(q, M, oi) → ai.\nThe action is executed in the environment and changes the state as T(si, ai) → si+1.\n\n\nEach completed task forms an experience. This experience is used to extend the workflow memory. For each experience that we store, it includes:\n\nThe instruction provided: q\nThe trajectory contains the steps used to solve the workflow. Each step p comprises the current state, and the action taken i.e. p=(o, a)\nAn induction module is to induce useful workflows W = {w} from the set of experiences E = {e} constructed from past or collected examples. These workflows are then added to the memory or subsequent task-solving.\n\n\n\nLM-based Workflow Induction\n\nThe induction module I induces a set of workflows W from one or more past agent experiences E.\nModule I prompts the agent to extract common sub-routines from one or more input experiences.\nIf we want to extract common patterns from different workflows, we need to provide more fine-grained instructions. For example, instead of prompting “buy a cover for pixel9”, the prompt should be “buy a black leather cover for my pixel from Amazon and deliver it to my address”. Fine-grained instructions help extract common tasks or patterns easily.\nAfter the workflows W are induced, they are then integrated into the agent as auxiliary memory, M + W →Mw.\nWhen solving a given instruction q, the agent then produces a series of actions defined by  L(q, Mw, o) = L(q, M + W, o) → a\n\n\n\n\nOffline Induction\n\nAssume that you have annotated examples that represent some experience for some workflow.\nInduction involves two standalone processes:\n\nConcatenate all training examples into a single prompt and feed them to the LM to create a set of workflows at ‘training’ time; I(Etrain) → Woffline\nIncorporates all induced workflows into the agent memory at inference time to solve test instructions L(q, M + W, otest ) → atest\n\n\n  \n\n\nOnline Induction\nAddresses the question: What if the agent has not seen this workflow in the past? Works in a supervision-free setting where only test queries are needed. Agents process test queries in a streaming fashion, where the agents conduct the loop of inducing, integrating, and utilizing workflows after running inference for each test task.\n\nAssume the default memory is M.\nAt test timestep t, instruction qt is passed to the agent. The agent tries to solve the task by creating a trajectory. This instruction paired with the trajectory is the experience formed at this timestep.\nUsing LLM as a judge, a binary label {0, 1} is assigned to the output of the trajectory denoting failure/success.\nEvery time an experience is considered a success, it is transformed into workflow(s) I(et) → {wt}. {wt} is added into the agent memory Mt + {wt} → M(t+1)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About me",
    "section": "",
    "text": "Hello, I am Aakash Kumar Nain.\nI am a Machine Learning Engineer with 8 years of experience in the industry. Currently, I am working as a Senior ML Engineer at Merlyn Mind. I am also a Google Developers Expert in Machine Learning and JAX. I actively contribute to Keras, TensorFlow, and the JAX ecosystem. I am one of the core collaborators for the Keras core project, and I am also one of the maintainers of the TensorFlow-addons package.\nIn terms of work, my knowledge Machine Learning and Deep Learning spans in T-shape. I have worked in multiple domains, but Computer Vision with Deep Learning is my favorite field. I love simplifying Data Science and Machine Learning concepts, especially the maths behind them. Love and support OSS because, without OSS, we wouldn’t have witnessed so many breakthroughs in Machine Learning and Deep Learning. Python is my favorite language to work with, and I absolutely love coding in it. Always up for a discussion involving anything related to Machine Learning, Deep Learning, MLOps, and API design. Simply put: Python in breakfast, Machine Learning in lunch, and Deep Learning in dinner!\nFollow me on Twitter to get the latest updates related to Machine Learning."
  },
  {
    "objectID": "index.html#popular-projects",
    "href": "index.html#popular-projects",
    "title": "About me",
    "section": "Popular Projects",
    "text": "Popular Projects\nAnnotated Research Papers   \nMost researchers and Machine Learning engineers want to remain up-to-date with the latest research work, but the field of machine learning moves crazy fast. The number of papers uploaded on arXiv is witnessing exponential growth. Reading all these published papers is an impossible task. To help with this, I maintain this repository where I annotate and upload the papers that I find exceptionally good. Annotations make it easier to understand and grasp the main concepts explained in the paper. \nTF-JAX Tutorials   \nIt is a series of tutorials built to teach the fundamental concepts and the underlying working of two famous libraries: TensorFlow and JAX. These tutorials aren’t typical documentation-type tutorials. It doesn’t matter whether you are a beginner or an advanced user, these tutorials will give you a fresh perspective on building things using TensorFlow or JAX. \nDiffusion Models   \nDiffusion models are a class of likelihood-based generative models that recently have been used to produce very high-quality images compared to other existing generative models like GANs. It’s hard to find quality resources to learn about diffusion models. The mathematics behind the diffusion models is also a bit harder to understand. The material presented in this repo is enough to make you understand the working of diffusion models and the underlying math. \nMistral-7B in JAX   \nThis is a direct port of Mitral-7B model in JAX and Equinox. This port comes with two implementations: The first one shows how to port models from PyTorch to Equinox and JAX. This is a 1:1 mapping, and not fully optimized. The second implementation is fully optimized for JAX, targeted for more advanced users."
  },
  {
    "objectID": "index.html#keras-core-contributions",
    "href": "index.html#keras-core-contributions",
    "title": "About me",
    "section": "Keras Core Contributions",
    "text": "Keras Core Contributions\n\nJAX NN ops\nMerging layers\nMetrics\nLoss functions\nApplications\nData adapters\nCode examples"
  },
  {
    "objectID": "index.html#keras-contributions",
    "href": "index.html#keras-contributions",
    "title": "About me",
    "section": "Keras Contributions",
    "text": "Keras Contributions\n\nDenoising Diffusion Models\nOCR model for reading captchas\nHandwriting recognition\nImage captioning\nWGAN-GP\nCycleGAN\nModel interpretability with Integrated Gradients"
  },
  {
    "objectID": "index.html#equinox-contributions",
    "href": "index.html#equinox-contributions",
    "title": "About me",
    "section": "Equinox Contributions",
    "text": "Equinox Contributions\n\nMistral-7B implementation\nBug fix in RMSNorm Layer\nAMP in Normalization Layers"
  },
  {
    "objectID": "index.html#tensorflow-contributions",
    "href": "index.html#tensorflow-contributions",
    "title": "About me",
    "section": "TensorFlow Contributions",
    "text": "TensorFlow Contributions\n\nKappa\nGelu activation\nFocal loss\nThresholded Linear Unit"
  }
]