[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Multimodal Autoregressive Pre-training of Large Vision Encoders\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nMLLMs\n\n\n \n\n\n\n\nNov 27, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\nHarmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nMLLMs\n\n\ngeneration\n\n\n \n\n\n\n\nNov 25, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nLLMs\n\n\n \n\n\n\n\nNov 20, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nLLMs\n\n\n \n\n\n\n\nNov 13, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nvision\n\n\n \n\n\n\n\nNov 8, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\ntransformers\n\n\nscaling\n\n\n \n\n\n\n\nNov 4, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nVLMs\n\n\nMLLMs\n\n\n \n\n\n\n\nOct 28, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\ntransformers\n\n\nresearch\n\n\nLLMs\n\n\n \n\n\n\n\nOct 23, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\ntransformers\n\n\nresearch\n\n\nLLMs\n\n\nmodel_merging\n\n\n \n\n\n\n\nOct 15, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nagents\n\n\n \n\n\n\n\nSep 24, 2024\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html#paper-summaries",
    "href": "blog.html#paper-summaries",
    "title": "Blog Posts",
    "section": "",
    "text": "Multimodal Autoregressive Pre-training of Large Vision Encoders\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nMLLMs\n\n\n \n\n\n\n\nNov 27, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\nHarmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nMLLMs\n\n\ngeneration\n\n\n \n\n\n\n\nNov 25, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nLLMs\n\n\n \n\n\n\n\nNov 20, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nLLMs\n\n\n \n\n\n\n\nNov 13, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nvision\n\n\n \n\n\n\n\nNov 8, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\ntransformers\n\n\nscaling\n\n\n \n\n\n\n\nNov 4, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nVLMs\n\n\nMLLMs\n\n\n \n\n\n\n\nOct 28, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\ntransformers\n\n\nresearch\n\n\nLLMs\n\n\n \n\n\n\n\nOct 23, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\ntransformers\n\n\nresearch\n\n\nLLMs\n\n\nmodel_merging\n\n\n \n\n\n\n\nOct 15, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nagents\n\n\n \n\n\n\n\nSep 24, 2024\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html#ml-dl-concepts",
    "href": "blog.html#ml-dl-concepts",
    "title": "Blog Posts",
    "section": "ML-DL Concepts",
    "text": "ML-DL Concepts\n\n\n\n\n\n\nRotary Position Embeddings\n\n\nA figure among cyphers: Part-1\n\n\n\n\n\n\nDec 8, 2024\n\n\n16 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Rotary Position Embeddings\n\n\nA figure among cyphers: Part-1\n\n\n\n\n\n\nDec 8, 2024\n\n\n\n\n\n\n\n\nAIMv2\n\n\nMultimodal Autoregressive Pre-training of Large Vision Encoders\n\n\n\n\n\n\nNov 27, 2024\n\n\n\n\n\n\n\n\nJanusFlow\n\n\nHarmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation\n\n\n\n\n\n\nNov 25, 2024\n\n\n\n\n\n\n\n\nCut Your Losses in Large-Vocabulary Language Models\n\n\n\n\n\n\n\n\n\nNov 20, 2024\n\n\n\n\n\n\n\n\nThe Super Weight in Large Language Models\n\n\n\n\n\n\n\n\n\nNov 13, 2024\n\n\n\n\n\n\n\n\nDepth Pro\n\n\n\n\n\n\n\n\n\nNov 8, 2024\n\n\n\n\n\n\n\n\nA Hitchhiker’s Guide to Scaling Law Estimation\n\n\n\n\n\n\n\n\n\nNov 4, 2024\n\n\n\n\n\n\n\n\nOmniParser for Pure Vision Based GUI Agent\n\n\n\n\n\n\n\n\n\nOct 28, 2024\n\n\n\n\n\n\n\n\nNormalized Transformer\n\n\n\n\n\n\n\n\n\nOct 23, 2024\n\n\n\n\n\n\n\n\nWhat Matters for Model Merging at Scale?\n\n\n\n\n\n\n\n\n\nOct 15, 2024\n\n\n\n\n\n\n\n\nAgent WorkFlow Memory\n\n\n\n\n\n\n\n\n\nSep 24, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/paper_summaries/the_super_weight.html",
    "href": "posts/paper_summaries/the_super_weight.html",
    "title": "The Super Weight in Large Language Models",
    "section": "",
    "text": "arXiv\nA few papers in the past showcased that at a certain scale, a small set of hidden state features contains outliers with enormous magnitude. These outliers account for a small percentage of all activations but are crucial for preserving the compressed model quality.\nThis paper from Apple takes that finding to the extreme, claiming that a tiny subset, at most six scalers, is more important than the rest of the others. The authors call them super weights, and pruning them destroys model quality.\n  \n\nSuper weights create super activations\n\nThe paper Massive Activations in Large Language Models showcased that LLMs contain massive activations that persist across many layers at the same position irrespective of the input. These activations have a constant magnitude and are crucial to the model’s performance.\nThe authors observed another interesting thing, i.e., the activation channel of these massive activations aligns with the channel of the super weights. To validate if both are related, the authors prune the super weight and check its effect on the magnitude of these activations.\nThey found that pruning the super weight reduces the magnitudes of these activations drastically. Hence, these activations are created by the super weight. They term these massive activations as super activations.\n\n\n\nIdentifying super weight by activation spikes\n\nGiven the findings in the above section, the authors hypothesize that super weight can be located by detecting the spikes in the input-output distributions of down_proj across the layers. This detection only requires a single input prompt rather than a set of validation data or use-case examples.\nSuppose X is the input with dimensions (L x H), and the weight matrix of the down_proj is W with dimensions (D x H). We can then compute output Y as Y= X WT. If Yij is a super activation, and Xik and Wik are outliers, we can say that Yij ≈ Xik Wjk. We can then identify the row from the super weight from the channel index given by the input distribution of the activations across all layers. Similarly, we can identify the column of the super weight from the channel index of the corresponding layer in the output distribution of the activations.\nThe authors found that the Phi-3-mini-4k-instruct contains the maximum number of super weights, a total of six.\nThe authors also found that super weights in the instruct-tuned models are located at the exact coordinates as in the pre-trained models, suggesting that instruct fine-tuning does not change the position of super weights.\n\n\n\nWhat impacts the model’s quality more: Super weights or super activations?\n\nExperimented with three conditions:\n\nOriginal model\nPrune the super weight (setting it to zero)\nPrune the super weight but restore the super activation\n\nWhen the authors prune the super weight, it significantly impairs quality - reducing accuracy on zero-shot datasets and increasing perplexity by orders of magnitude.\nPruning the super weight but restoring the super activation recovers the quality partially but only up to a (low) extent. It suggests that the super weight and super activation need special handling to preserve quality.\n\n  \n\n\nEffect of super weight on the output token probability distribution\n\nSo, what happens to the output of an LLM when you remove the super weight? The probability of generating the stopwords amplifies.\nIf super weights are so important, why don’t we amplify them more? The authors multiply the super weights by a scaling factor ranging from 0.0 to 3.0 to record the impact on the model performance. They noticed that even though the improvements are minuscule, there exists a consistent trend of improvement to a certain scaling factor.\n\n  \n\n\nWeight and Activation Quantization\nGiven that we know the importance of the super activation and super weights, how should we quantize these?\n\nActivation Quantization\n\nSimulate W8A8 quantization with FP16 arithmetic and focus solely on addressing one critical activation outlier. Experiments done using round-to-nearest quantization but with some modifications.\nReplace the super activation with the median value (REPLACE), quantize (Q), and dequantize (Q−1) activations. Then restore the super activation in fp16 (RESTORE)\n\nWeight Quantization\n\nProposed a simple method to improve INT4 quantization with large block sizes.\nIdentify the super weight as described in the previous section. Clip the outlier weight, including the super weight, to improve the inlier fit.\nQuantize (Q) and dequantize (Q−1) the clipped weights. Restore the half-precision super weight after dequantization (RESTORE)\n\n\n  \n\n\nWhich models did the authors user in their experiments?\nHere is table showcasing all the models used in the experiments. All these models are coming from HF.   \n\n\nHow does the quantization scheme proposed by the authors perform?\nThere have been other quantization schemes that were proposed for these outlier weights and the massive activations observed in these models. How does the new scheme fare with those?"
  },
  {
    "objectID": "posts/paper_summaries/agent_workflow_memory.html",
    "href": "posts/paper_summaries/agent_workflow_memory.html",
    "title": "Agent WorkFlow Memory",
    "section": "",
    "text": "arXiv annotated_paper\nFor almost six months, I have been vocal about the differentiation between tool usage with LLMs and an agent. Most people correlate pure tool usage with agentic behavior, but IMHO that is only a subset of it. If we want to deploy agents, we need some planning and reasoning capabilities. Reasoning is abstract and hard to solve. Planning on the other hand comes in many flavors. If we want to go to system 2, we must augment planning capabilities with memory. After all, an intelligent system should be able to extract and learn from past behavior.\nThe paper Agent WorkFlow Memory, which came out last week, tries to solve some of these problems, and I think their choice is a good direction.\n  \n\nWhy do we want to implement a memory module?\n\nCurrent language model-based agents struggle with long-horizon tasks and complex action trajectories.\nWhen presented with a new task, a common way to induce knowledge about the task is to perform in-context learning or fine-tuning.\nThis approach works for a given task but fails to generalize across different tasks even if they share the same pattern. For example, planning a trip to a foreign country vs planning a local trip share common workflows with minor differences in the booking procedure.\nInspired by how humans abstract common task routines from past experiences, we can implement a workflow memory module where each workflow represents a goal with a common routine extracted from available action trajectories.\n\n  \n\n\nAgent Workflow Memory\nLet us define the system first. Suppose we have an LLM L, responsible for carrying out the workflows, a text-based Memory M, where the base memory contains documentation of built-in actions such as CLICK and TYPE.\nUNTIL ai != STOP or curr_step &gt; MAX_STEPS_ALLOWED do;\n\nTo solve a task specified by an instruction in natural language q, the agent acts in an environment defined by a transition function T.\nAt each time step ti, the environment state si gives observation oi. This observation is then passed into the model to generate action ai via L(q, M, oi) → ai.\nThe action is executed in the environment and changes the state as T(si, ai) → si+1.\n\n\nEach completed task forms an experience. This experience is used to extend the workflow memory. For each experience that we store, it includes:\n\nThe instruction provided: q\nThe trajectory contains the steps used to solve the workflow. Each step p comprises the current state, and the action taken i.e. p=(o, a)\nAn induction module is to induce useful workflows W = {w} from the set of experiences E = {e} constructed from past or collected examples. These workflows are then added to the memory or subsequent task-solving.\n\n\n\nLM-based Workflow Induction\n\nThe induction module I induces a set of workflows W from one or more past agent experiences E.\nModule I prompts the agent to extract common sub-routines from one or more input experiences.\nIf we want to extract common patterns from different workflows, we need to provide more fine-grained instructions. For example, instead of prompting “buy a cover for pixel9”, the prompt should be “buy a black leather cover for my pixel from Amazon and deliver it to my address”. Fine-grained instructions help extract common tasks or patterns easily.\nAfter the workflows W are induced, they are then integrated into the agent as auxiliary memory, M + W →Mw.\nWhen solving a given instruction q, the agent then produces a series of actions defined by  L(q, Mw, o) = L(q, M + W, o) → a\n\n\n\n\nOffline Induction\n\nAssume that you have annotated examples that represent some experience for some workflow.\nInduction involves two standalone processes:\n\nConcatenate all training examples into a single prompt and feed them to the LM to create a set of workflows at ‘training’ time; I(Etrain) → Woffline\nIncorporates all induced workflows into the agent memory at inference time to solve test instructions L(q, M + W, otest ) → atest\n\n\n  \n\n\nOnline Induction\nAddresses the question: What if the agent has not seen this workflow in the past? Works in a supervision-free setting where only test queries are needed. Agents process test queries in a streaming fashion, where the agents conduct the loop of inducing, integrating, and utilizing workflows after running inference for each test task.\n\nAssume the default memory is M.\nAt test timestep t, instruction qt is passed to the agent. The agent tries to solve the task by creating a trajectory. This instruction paired with the trajectory is the experience formed at this timestep.\nUsing LLM as a judge, a binary label {0, 1} is assigned to the output of the trajectory denoting failure/success.\nEvery time an experience is considered a success, it is transformed into workflow(s) I(et) → {wt}. {wt} is added into the agent memory Mt + {wt} → M(t+1)"
  },
  {
    "objectID": "posts/paper_summaries/cut_your_losses.html",
    "href": "posts/paper_summaries/cut_your_losses.html",
    "title": "Cut Your Losses in Large-Vocabulary Language Models",
    "section": "",
    "text": "arXiv\nWe all want to save GPU memory and increase its utilization, right? This latest paper from Apple is exactly what we all need (apart from attention, of course!).\nAs we scale models, the vocabulary for these models will also grow (and we want to be in an ideal scenario). One side effect of this is that during the training, the memory footprint of the last layer/op responsible for cross-entropy calculation grows disproportionately. In the case of small models, the memory consumed in the final layer can be an order of magnitude higher than the memory consumption in the rest of the LLM combined. The authors propose Cut Cross-Entropy (CCE), a method that computes the cross-entropy loss without materializing the logits for all tokens into global memory.\nA natural question to ask is that someone should have noticed this before, and it can not be the first one. Well, the answer is yes. Let me put it this way: Optimizing the quadratic cost of attention has been the main focus. We made a couple of improvements in that area, and now we are shifting to this. Though this is not the first time someone has implemented a similar idea (I will provide links to a few references later in this thread), I believe this is the first feature-complete implementation of this concept.\n\nPreliminaries\nBefore discussing the idea presented in this paper, take a step back and look at some notations described below as a refresher.\n\nThe LLM parameterizes an autoregressive distribution over all possible tokens xi ∈ V given the preceding N−1 tokens. V is our vocabulary.\nWe can split the LLM into a “feature backbone” f: x1 … xi−1 → RD and a “classifier” C ∈ RD×|V|\nThe softmaxk(v) produces the probability over all vocabulary entries from the unnormalized log probabilities (logits).\nAt training time, the LLM maximizes the log-likelihood of the next token.\n\n\n\nCut-Cross Entropy\nCross-entropy loss can be written as a combination of two terms, as shown below. The first term is a combination of an indexing operation and matrix multiplication. The second term is a joint log-sum-exp and matrix multiplication operation.\n  \n\n\nMemory Efficient Indexed Matrix Multiplication\nA naive computation of indexed matrix multiplication expressed in the first term above involves either explicit computation of the logits CTE with an O(N|V|) memory cost or indexing into the classifier Cx = [Cx1…..CxN] with an O(ND) memory cost.\nTo achieve better efficiency, the authors fuse the classifier indexing Cx with the consecutive dot product between columns Cxi and Ei in a single CUDA/Triton kernel.\n\nThe kernel retrieves the value xi, the xi-th column from C, and the ith column from E and stores them in SRAM.\nPerform a dot product between the retrieved values and write the results to the global memory.\n\nAll the operations are performed in SRAM, and no GPU memory is allocated at any point. Also, the computations are performed block-wise for better GPU efficiency. The backward pass can be merged with the backward pass of the log-sum-exp operation, which will be discussed shortly.\n  \n\n\nMemory Efficient Linear log-sum-exp\n\nForward pass\n\nUses an old trick of blocks and parallelization.\nDivide the output O = CTE ∈ R|V|×N into a set of blocks of size MB × NB.\nRetrieve the corresponding parts En of E with size (D × NB) and blocks Cm of C with size (D × MB) using independent CUDA blocks. Perform the inner product Onm = CTmEn along the dimension D.\nLoop over smaller size (DB X NB) and (DB X MB) blocks and accumulate Onm = Σd(CTmdEnd) in SRAM.\nEach CUDA block then writes Onm back into global memory.\nThe same blocking and parallelization strategy is applied to produce log-sum-exp(CTE). Each block first computes a matrix multiplication, then the log-sum-exp (or LSE for short) along the vocabulary dimension m for its block, and finally updates log-sum-exp with its result.\nThe authors use a spin-lock on an atomic operation in global memory to synchronize the updates by different CUDA blocks for simplicity.\n\nBackward pass\n\n\nNeeds two gradient updates. The gradient is defined as shown above.\nTo calculate CTE, we can take the same approach used in the forward pass and can compute this in SRAM.\nSince softmax(CTE) = exp(CTE − LSE), we do not need to compute the normalization constant of the softmax, allowing us to reuse the global synchronization of the forward pass and compute the softmax efficiently in parallel.\nThe authors implement the second matrix multiplication in the GPU memory as a blockwise implementation would require storing or synchronizing S.\nThe authors use two techniques to improve the memory access pattern in this algo: gradient filtering and vocabulary sorting.\nGradient filtering is necessary because when items are in bf16, any value below ε = 2e−12 will likely be ignored due to truncation in the summation or rounding in the normalization, directly affecting the softmax matrix. For any column in that matrix, at most 1/ε = 4096 entries have non-trivial values and contribute to the gradient computation. All other values are either rounded to zero or truncated. This sparsity increases with the size of the vocab. The authors exploit this sparsity and skip gradient computation for any block whose corresponding softmax matrix Snm has only negligible elements. A threshold of ε = 2e−12 is chosen as the smallest value for no truncation.\nEfficient gradient filtering depends directly on the block-level sparsity of the softmax matrix. Ideally, blocks should be empty (hence skipped) or populated entirely but not partially. The authors group the non-trivial gradients by ordering the tokens by their average logit. In the case of a forward pass, they achieve this using an atomic addition to compute the average logit per token. In the case of the backward pass, divide the vocabulary dimension |V| into blocks with similar average logit instead of arbitrarily.\n\n\n  \n\n\nRuntime And Memory\n\nModel: Gemma 2B\nVocab size: 256, 000\nHidden dim: 2,304\nBatch size: 8,192 tokens\n\nHere are the results with different methods"
  },
  {
    "objectID": "posts/paper_summaries/model_merging_at_scale.html",
    "href": "posts/paper_summaries/model_merging_at_scale.html",
    "title": "What Matters for Model Merging at Scale?",
    "section": "",
    "text": "arXiv annotated_paper\nYesterday, I read this banger paper titled: What Matters For Model Merging At Scale?. Though I recommend reading the full paper, I am including a summary here in case you are interested in the main points. I will also provide the link to an annotated version of this paper.\n  \n\nIntroduction\nModel merging is not a new concept. It has been tried and tested enough to create a better or more powerful model by combining two or more (expert) models. Model merging has several advantages:\n\nReduced storage and serving costs\nImproved generalization to new tasks due to combined capabilities.\nDecentralized and modular model development.\n\nOne potential gap in this area is the lack of a comprehensive study to evaluate its effectiveness as we scale the model size. Most people are either merging models at a small scale (7B-13B models) or merging a limited number of expert models. This paper provides insights into the scalability of model merging.\n\n\nProblem Statement\n\nFocuses on model merging with large models\nN expert tasks and a base model\nExpert for each of these N tasks obtained by fully fine-tuning the base model on a specific expert task.\nFour merging methods, including Averaging, Task Arithmetic, TIES, and DARE.\n\n\n\nExperimental Design for Large-Scale Evaluation of Model Merging\n\nData\n\nData settings from the T0 mixture containing 8 held-in and 4 held-out task categories.\nThe 8 held-in task categories (with a total of 16 datasets) include Multiple-choice QA, Extractive Qa, Closed-Book QA, Sentiment Analysis, Topic Classification, Structure-to-text, Summarization, and Paraphrase Identification. The 4 held-out task categories are Sentence Completion, Natural Language Inference, Co-reference Resolution, and Word Sense Disambiguation.\n\n\n\n\nModels\n\nPaLM-2 with sizes 1B, 8B, 24B, and 64B as the base models. For all these models, the authors also build an instruction-tuned version of PaLM-2-IT.\nFor each of the two base model types(non-IT Vs IT) and 4 model sizes, they perform full fine-tuning on the 8 held-in task categories resulting in 64 specialized expert models.\nThe authors create a large merging experiment grid with the two base models (PaLM-2 and PaLM-2-IT), four model sizes (1B, 8B, 24B, 64B), four Merging methods (Averaging, Task Arithmetic, Dare-TIES, and TIES), the number of constituent models (2, 4, 6, 8), and 3 seeds to randomly select the constituent tasks for the experiment resulting in a total of 384 merging experiments.\n\n\n\n\n\nEvaluation\n\nPerformance evaluated on both held-in and held-out tasks\n∼9000 model evaluations across all the experiments.\n\n\n\n\nMetrics\n\nFor held-in tasks, the merged model performance is normalized against the corresponding task expert model’s performance.\nFor held-out tasks, normalization is performed relative to the base model’s performance.\n\n\n\n\nExperimental Results and Insights\n\nInstruction-Tuned Models Facilitate Easier Merging\n\nMerging experiments done with fully fine-tuned experts from PaLM-2 and PaLM-2-IT\nHeld-in performance is measured over three trials to minimize the impact of selected expert models and their data distributions.\nPaLM-2-IT models consistently outperform PaLM-2 base models for all merging methods. The authors think large-scale instruction tuning further disentangles model weights, facilitating effective model merging and improving the base model zero-shot performance.\n\n\n\n\nModel Merging Becomes Easier With Bigger Models\n\nThe authors noticed that merged models outperform their corresponding base models in zero-shot generalization to held-out tasks irrespective of the model size, merging method, or number of constituent models.\nFor weak base models (PaLM-2), increasing model size significantly improved the merged model performance over the base model. Strong base models (PaLM-2-IT) show a different trend, and the zero-shot generalization improves monotonically with more expert models.\n\n\n\n\nBigger Model Sizes Can Merge More Experts\n\nFor weak base models (PaLM-2) that are of small size (1B-8B), merging more models leads to a significant drop in performance, whereas for strong base models (PaLM-2), the drop is negligible.\nThe above trend doesn’t hold for bigger model sizes (64B). Merging more experts for a weak base model (PaLM-2 64B) leads to significant improvements in performance, whereas for strong base models(PaLM-2-IT), it leads to better generalization.   \n\nMerging Methods Become Similar at Scale\n\nAt scale, all merging methods for strong base models exhibit very similar performance, suggesting that we can simply use Averaging strategy to get the optimal performance at scale.   \n\n\n\n\nResults"
  },
  {
    "objectID": "posts/paper_summaries/depthpro.html",
    "href": "posts/paper_summaries/depthpro.html",
    "title": "Depth Pro",
    "section": "",
    "text": "arXiv\nZero-shot monocular depth estimation in real-time is one of the most challenging problems in vision. Applications like novel view synthesis from a single image require a strong depth estimation model. The paper DepthPro from Apple presents another model that can perform zero-shot depth estimation on high-resolution images with low latency.\n\nDesired characteristics of a depth estimation model\n\nIt should not be restricted to a single domain and should produce zero-shot depth estimation on any image.\nThe model should produce metric depth maps in a zero-shot regime to accurately reproduce object shapes, scene layouts, and absolute scales.\nThe model should produce metric depth maps with absolute scale even if no camera intrinsics (such as focal length) are provided with the image. It helps enable novel view synthesis from an arbitrary image.\nThe model can operate on high-resolution images and should be able to produce high-quality depth maps even for complex objects like hair, fur, etc.\nIt should run with extremely low latency to support applications like view synthesis on demand.\n\nThe DepthPro model ticks all the things listed above. \n\n\nNetwork\n\nUses pretrained Vision Transformers (ViTs) as encoders.\nThe key idea is to apply plain ViT encoders on patches extracted at multiple scales and fuse the patch predictions into a single high-resolution dense prediction in an end-to-end trainable mode.\nTwo encoders in total: Patch encoder and image encoder.\nThe patch encoder is applied on patches extracted at multiple scales, allowing the model to learn scale-invariant representations as weights are shared across scales.\nThe image encoder is applied to the whole image, down sampled from the original resolution to the resolution accepted by the base encoder (384 x 384 in this case). The image encoder anchors the patch predictions in a global context.\nThe network operates at a fixed resolution of 1536 × 1536 chosen as a multiple of the ViT’s 384×384.\nThe original image with a resolution of 1536 x 1536 is processed with two more down sampled resolutions of 784 x 784, and 384 x 384. For each resolution, the image is divided into patches of size 384x384. At each scale, the patches are fed into the patch encoder that produces a feature tensor at a resolution of 24 × 24 per input patch. Intermediate features for the finest scale are also extracted to capture fine-grained details.\nThe feature patches are then merged into maps and are fed into a DPT-like decoder module.\n\n  \n\n\nTraining Objectives\n\nThe network f predicts a canonical inverse depth image C = f (I) for an input image I. The dense metric depth map is then obtained as Dm = fpx / wC, where fpx is the focal length and w is the width.\nAll objectives use canonical inverse depth as it prioritizes areas close to the camera over farther areas or the whole scene. This supports visual quality in applications such as novel view synthesis.\nFor all metric datasets, the authors compute mean absolute error (MAE) per pixel i and discard pixels with an error in the top 20% per image for real-world (not for synthetic) datasets.\nFor all non-metric datasets (i.e., those without reliable camera intrinsics or inconsistent scale), the authors normalize predictions and ground truth via the mean absolute deviation from the median before applying a loss. They also define a multi-scale derivative loss over M scales. The ∇∗ indicate a spatial derivative operator ∗, such as Scharr or Laplace (L), and p is the error norm. The scales j are computed by blurring and down sampling the inverse depth maps by a factor of 2 per scale.\n\n  \nI did not find this in the code, but I have asked the authors to give some implementation details. Let us see how it goes. \n\n\nTraining Curriculum\nThe authors note down three important observations before launching a training run: * Training on a large mix of real-world and synthetic datasets improves generalization. * Real-world datasets are messy, and many times the labels are noisy (missing areas, mismatched depth, or false measurements on object boundaries.) * Predictions get sharper throughout the training.\nBased on these observations, the authors design a two-stage training curriculum. In the first stage, they aim to learn robust features that allow the network to generalize across domains, and train the model on a mix of all labeled training sets. They use MAE for the metric datasets and the normalized MAE for non-metric datasets. They also apply scale-and-shift-invariant loss on gradients, but only to synthetic datasets.\nThe second stage of training is designed to sharpen boundaries and reveal fine details in the predicted depth maps. Given that the synthetic dataset provides high-quality pixel-accurate ground truth, the authors use it to minimize the effect of inaccurate ground truth. MAE is supplemented with Mean Absolute Gradient Error (MAGE), Mean Absolute Laplace Error (MALE), and the Mean Squared Gradient Error (MSGE).\n\n\n\nEvaluation metrics for sharp boundaries.\n\nCommon benchmarks for monocular depth prediction rarely take boundary sharpness into account. This may be attributed in part to the lack of diverse and realistic datasets with precise pixel-accurate ground-truth depth.\nThe authors aim to leverage existing high-quality annotations for matting, saliency, or segmentation as ground truth for depth boundaries.\nThey treat annotations for these tasks as binary maps, which define a foreground/background relationship between an object and its environment. To ensure that the relationship holds, they only consider pixels around edges in the binary map.\nThe hypothesis is that if the depth of pixel i and pixel j differs by more than t%, it suggests the presence of an occluding contour between those pixels. Occluding contour c~d(i, j)~, and overall precision and recall for the neighboring pixels are then calculated as shown below:\n\n  \n\nThe authors report the weighted F1 score with thresholds that range linearly from tmin = 5 to tmax = 25, with stronger weights towards high threshold values.\nThe beauty of this method is that it doesn’t require any manual edge annotations. It only needs pixel-wise ground truth that can be easily obtained for synthetic datasets.\nThe authors also apply non-maximum suppression to values of cˆd within the valid bounds of cˆd(i, j) connected components.\nOccluding contours from binary maps are similarly obtained \n\n\n\nFocal Length Estimation\n\nTrained the network to predict focal length to handle cases where the EXIF metadata is either missing or is noisy for the input image.\nThe focal estimation head is a small convolutional network that ingests frozen features from the depth estimation network and task-specific features from a separate ViT image encoder to predict the horizontal angular field of view.\nThis head is trained after the training for depth prediction is complete. \n\n\n\nResults\n  \n\n\nLimitations\nThe model is limited in dealing with translucent surfaces and volumetric scattering, where the definition of single-pixel depth is ill-posed and ambiguous."
  },
  {
    "objectID": "posts/paper_summaries/nGPT.html",
    "href": "posts/paper_summaries/nGPT.html",
    "title": "Normalized Transformer",
    "section": "",
    "text": "arXiv\nWe have been using transformers daily for a couple of years now, but the advancements on the architectural side have remained minimal. This paper from Nvidia proposes a normalized Transformer (nGPT), which performs representation learning on a hypersphere. Here is a summary for the same:\n\nToken embeddings and output logits\n\nBoth input and output embedding matrices are normalized after each training step.\nThe logits are bounded in the range [-1, 1] because of the normalization, which limits the confidence of the probability distribution generated by the softmax.\nTo adjust this during training, the authors introduce a trainable scaling parameter sz that scales the logits element-wise.\n\n\n\nLayers and Blocks\n\nA typical transformer block looks like this where L layers of transformations are applied to the hidden state h, consisting of alternating the self-attention and MLP blocks: \nIf we are on a hypersphere, we can use spherical linear interpolation to compute the geodesic distance between two points a and b on the hypersphere. We can approximate SLERP with approximate linear interpolation (LERP) as shown below: \nIf point a is our hidden state h, and point b represents the point suggested by the attention or MLP block, we can represent our equations for the transformer blocks like this: \nHere αA ≥0 and αM ≥0, with dimensionality dmodel, are learnable parameters applied to the normalized outputs of the attention and MLP blocks, respectively. The norm function normalizes any vector x to have a unit norm and, unlike RMSNorm or LayerNorm, does not introduce any element-wise scaling factors. This normalization can be viewed as the retraction step in Riemannian optimization, mapping the updated solution back to the manifold.\n\n\n\nSelf-Attention Blocks\n\nThe qkv values produced by the weight matrics Wq, Wk, and Wv in the original transformer are unconstrained, leading to unbounded values in q.\nIn nGPT the authors normalize Wq, Wk, Wv and Wo along their embedding dimension so that the computed dot products with h can be interpreted as cosine similarity between unit norm vectors bounded in [−1, 1]. Thus, all attention matrices can be viewed as collections of normalized embedding vectors to be compared.\nThough each element of q and k is now bounded, the norms of these two vectors can still vary. Also, the addition of positional embeddings can further distort q and k. To this end, the authors additionally normalize q and k by introducing a scaling factor sqk for each head, ensuring that the dot product of every query and key is under control.\nIn the original Transformer, the query-key dot product is scaled by 1/√dk before applying softmax to account for the expected variance of dk in the dot product of non-normalized query and key vectors. In the normalized Transformer, the expected variance of the dot product between the normalized query and key vectors is 1/dk. The softmax scaling factor should instead be √dk to restore a variance of 1\n\n\n\n\nMLP\n\nThe input hidden state h of the MLP block of a classical transformer is first normalized using RMSNorm(or LayerNorm) and then passed through two separate linear projections, producing two intermediate vectors u and v, which are then combined using SwiGLU.\nThe weight matrices Wu and Wv in nGPT are normalized. The authors introduce scaling factors su and sν to control their impact. They also rescale ν by √dmodel to optimize SiLU performance.\n\n\n\n\nSummary of all modifications\n\nRemove all normalization layers like RMSNorm or LayerNorm.\nAfter each training step, normalize all matrices (Einput, Eoutput, Wq, Wk, Wv, Wo, Wu, Wν, and Wo) along their embedding dimension.\nReplace the updates as follows where αA (and also αM) is treated with αA,init = 0.05 (in order of 1/n_layers) and αA,scale = 1/√dmodel.\nChange the softmax scaling factor in attention from 1/√dk to √dk.\nImplement the rescaling and normalization of q and k where sqk is treated with sqk,init = 1 and sqk,scale = 1/√dmodel.\nImplement the rescaling of the intermediate state of the MLP block where su (and also sν) is treated with su,init = 1 and su,scale = 1\nImplement the rescaling of logits using equation 3, where sz is treated with sz,init = 1, sz,scale = 1/√dmodel.\nRemove weight decay and learning rate warmup.\n\n\n\n\nHow fast is nGPT compared to GPT?\nA lot! Training the 0.5B and 1B nGPT models is approximately 4x, 10x, and 20x faster at context lengths of 1k, 4k, and 8k tokens, respectively.\n\n\n\nWhat about the params and hparams?\n  \n\n\nThere should be a catch somewhere, right?\nOf course, there is always a catch! Quoting directly from the paper: The time cost per step for nGPT is approximately 80% higher with a 4k context length, and 60% higher with an 8k context length. This overhead is not only due to nGPT having 6 normalization steps (2 of them are applied for q and k) per layer instead of 2 but also because nGPT’s normalizations are not yet fully optimized, unlike GPT, where normalization layers are fused with other operations.\n\n\n\nBest thing about nGPT?\nIn standard GPTs, perplexity tends to increase dramatically when tested on sequences longer than pre-training length. In contrast, nGPT maintains a stable perplexity range even at extrapolated lengths."
  },
  {
    "objectID": "posts/paper_summaries/omniparser.html",
    "href": "posts/paper_summaries/omniparser.html",
    "title": "OmniParser for Pure Vision Based GUI Agent",
    "section": "",
    "text": "arXiv\nWith the Ferret-v2 paper, Apple demonstrated the power of building multimodal models for mobile devices. Now, Microsoft has done the same, but for the desktop UI. Presenting Omniparser, the latest advancement from Microsoft for vision-based multimodal workflows. Here is a summary in case you are interested:\n\nWhy does UI understanding matter?\nSince the last few months, there has been a lot of buzz about agents and system 2. A common theme across these explorations is that we want to automate many complex yet mundane tasks with the current generation of LLMs and MLLMs.\nFor example, an automatic trip planner is an excellent idea. You provide the trip details e.g. starting-ending dates, places you want to explore, etc. The system takes this information as input and completes the whole task of planning the trip, including but not limited to booking flight tickets, hotels, etc.\nSome of the sub-tasks in the above example can be completed using DOM alone, but things like captcha completion require visual understanding.\n\n\nHypothesis\nMultimodal models like GPT-4V, when asked to predict the xy coordinates of the interactable regions on the screen directly from UI (e.g. screenshots of a web page) perform poorly because of their inability to correctly identify the semantic information associated with the icons and other elements on the screen.\n\n\nMethod\nTo elevate the above problem, the authors suggest to break down the task into two steps:\n\nUnderstand the content present on screen at the current step to figure out the interactable regions and their corresponding functionality.\nBased on the above information, predict the next action required to complete the task.\n\nTo achieve the above workflow, OmniParser combines the outputs from three different models:\n\nInteractable icon detection model.\nIcon description model.\nOCR module\n\nThe output is a structured DOM-like representation of the UI, with a screenshot overlaid with bounding boxes for potential interactable elements.\n  \n\n\nInteractable Region Detection\n\nUses the Set-of-Marks approach to overlay bounding boxes of interactable icons on top of UI screenshots.\nFinetune a detection model to extract interactable icons/buttons.\nThe authors curated a dataset of interactable icon detection dataset, containing 67k unique screenshot images, each labeled with bounding boxes of interactable icons derived from the DOM tree.\nFinetune the YOLOv8 model on the interactable icon region detection dataset for 20 epoch with a batch size of 256, learning rate of 1e−3, and the Adam optimizer on 4 GPUs.\nAn OCR module is also used to extract bounding boxes of text. The authors merge the bounding boxes from the OCR detection module and icon detection module while removing the boxes that have high overlap (90% as a threshold). Every bounding box is assigned a unique ID.\n\n\n\nIncorporating Local Semantics of Functionality\n\nIncorporate local semantics directly into the prompt.\nGenerate a description of functionality using another model for each of the interactable regions detected by the interactable region detection model. For text boxes, detected text and the labels are injected into the prompt.\nUses BLIP-v2 as the icon description model for generating descriptions for the interactable regions.\nFor fine-tuning BLIP-v2, the authors curate a dataset of 7k icon-description pairs using GPT-4o. For the description, the authors ask GPT-4o whether the object presented in the parsed bounding box is an app icon. If GPT-4o decides the image is an icon, it outputs a sentence description of the icon about the potential functionality. And if not, GPT-4o will output ’this is not an icon’, while still including this in the dataset.\nBLIP-v2 is fine-tuned on this dataset for 1 epoch with a constant learning rate of 1e−5, no weight decay, and Adam optimizer.\n\n  \n\n\nResults\n  \n\n\nFailure Modes\n\nRepeated Icons/Texts: When the screenshot contains repeated elements, and it is required to click/select one of the elements, GPT-4V still fails many times. More granularity in the description of the repeated elements may alleviate the problem.\nCoarse Prediction of Bounding Boxes: The current OCR cannot differentiate between normal text and hyperlinks, hence in some cases elements of the predicted coordinatese of the clickable texts may not be correct.\nIcon Misinterpretation: Given that the icon-description model only sees a part (elements with a bbox) of the full screenshot, the lack of a global view can lead to a wrong description of the detected interactable icons. For example, three dots can be pretty confusing for the current description model"
  },
  {
    "objectID": "posts/paper_summaries/scaling_laws_estimation.html",
    "href": "posts/paper_summaries/scaling_laws_estimation.html",
    "title": "A Hitchhiker’s Guide to Scaling Law Estimation",
    "section": "",
    "text": "arXiv\nScaling laws have been discussed a lot in the past few years. The OG paper on scaling laws is still one of the best, but this latest paper from IBM and MIT provides a fresh perspective. Here is a quick summary in case you are interested:\n\nWhy should one care about scaling laws?\n\nDesign choices like architectural modification, data changes, etc., are crucial, but every option is expensive to evaluate at scale.\nWhat if we use smaller models to assess the design choices? Will the findings be upheld when using a larger model?\nA scaling law extrapolates the performance of a target model from the performance of a set of models with fewer parameters or smaller training sets. A high-quality scaling law accurately predicts the test performance of the target model.\n\n\n\n\nDefining a scaling law\n\nA scaling law estimates the loss of a costly model by training cheaper ones that share a pretraining procedure and differ by some hyperparameters, typically model size (num_params) and number of tokens seen during training (num_toks).\nThe authors define a scaled model family f as a set of models, with each f ∈ F differing only in size num_params(f ) and number of tokens num_toks(f ).\nThey divide this into subsets. First, the maximal parameter family max params (F) containing models in F with the largest number of parameters. This family will generally include the target model(s) whose behavior we wish to predict t ∈ Ftarget. Second, the q-maximal token family max num_toks (F, q) contains all models in f trained on at least a q-sized fraction of the training set.\nA scaling law L(f | F ) estimates the performance of a new model f given a model family F and is defined as shown below:\n\n  \nE is a baseline capturing the general performance of the scaled family. A and α describe the scaling effect of num_params, while B and β describe the scaling effect of num_toks. These parameters are estimated by first collecting a set of training models Ftrain, then minimizing the reconstruction error as shown above. L(f ) denotes the empirical negative log-likelihood of some held-out data under the model.\n\n\nHow Well Can I Expect a Scaling Law to Predict?\n\nTo estimate this, we need to know what is considered a meaningful change when comparing two models (of the same family). The authors found that any design change that yields less than a 4% change in performance has not been considered meaningful in the literature.\nAlso, the variance across the same model with restarts reaches a 3.5% difference. Thus, a 4% difference bounds the best goodness of fit we should expect or require of scaling laws.\nThe Absolute Relative Error (ARE) is bounded by 4% and can reach 20% with certain design choices.\n\n\n\n\nWhen I Train a New Model, Do I Even Need a New Scaling Law?\n\nDifferent model families exhibit different scaling behavior.\nEvery design choice like the choices of architecture, training procedure, or the dataset, affects the form of scaling laws. Hence, the behavior of a new model family may require a new scaling law.\nCan’t we borrow the scaling behaviors between two different model families even if it results in poor or biased estimates? To that end, the authors set the num_params scaling parameters (A, α) to fixed values reported in the paper titled Scaling data-constrained language models and estimate the remaining parameters for individual model families.\nThe authors found that predictions generalize, and a constant num_params scaling factor is enough for most models (except the encoder-decoder T5-Pile). However, error rates are bigger than in the source family, and predictions for larger models are worse.\n\n  \n\n\nCan I Train the Target Model a Bit Instead of Many Small Models?\n\nA reasonable assumption to make at this point is that training the target model for a bit can be much better than wasting time on training many small models.\nA good thing about the above assumption is that extrapolation is only required for num_tokens as the number of parameters is fixed now.\nAs of now, reliable estimates with this approach require up to 30% of the full training run, but this is a good option for future research. \n\n\n\nAre Even Simpler Baselines Enough?\n\nLet us take another example. Suppose we have two models belonging to the same small model family. One model has the best performance, and the other model has seen more data. Are these baselines enough to estimate the performance of the target model without fitting a scaling law?\nThe model with the best performance is expected to be closer to the performance of the target model. The authors found that the baselines suffer more than 15% error, mostly above 10%, rarely get below 5%, and 18% ARE is observed on average across all scaled families.\n\n  \n\n\nWhich is good for estimates: Intermediate checkpoints or the final checkpoint?\n\nA common methodology is to train different models with different numbers of tokens. Most people never use intermediate checkpoints as the assumption is the learning rate schedule renders losses from intermediate checkpoints uninformative.\nThe authors find that almost all, except for the first few checkpoints, help scale laws. Specifically, models trained on less than 10B tokens are noisy and should be excluded from scaling laws. These models or checkpoints can introduce significant ARE in estimating the performance of the target model.\n\n  \n\n\nHow Big a Model Should I Train?\n\nIt is implicit that bigger models where the num_params are closer to the target model are more effective for reliable scaling laws.\nHowever, the authors found that the effect is neither strong nor monotonic and varies between model families. For example, fitting on all F results in the lowest ARE in most cases. However, in the case of GPT, Gopher, and OPT, predicting with the smallest four models is enough to achieve less than 10% error. OTOH, the smallest models in Pythia are not predictive enough.\nThe authors suggest diversifying and training each model on differing hyperparameters (seed, num_params, num_tokens) and maximizing the information gained.\n\n  \n\n\nHow Many Models Are Required for Reliable Predictions?\n\nLarger models are better, but a group of smaller models is equally good.\nIncreasing the number of models helps decrease ARE. Overall, five models are a safe bet, and more would improve the robustness of the result. And these models can be small. \n\n\n\nWhat Parameters Do I Need to Estimate?\n\nThe authors computed the PCA of five learned parameters and found that three parameters explain 99.49% of the variance.\nThough the above finding holds for a large number of model families, exceptions exist, especially in encoder-decoder models like T5-Pile.\nOverall, scaling laws might have fewer degrees of freedom than described in the literature. num_params and num_tokens remain the most important ones. The authors left out the learning rate schedule for future research."
  },
  {
    "objectID": "posts/paper_summaries/aim_v2.html",
    "href": "posts/paper_summaries/aim_v2.html",
    "title": "AIMv2",
    "section": "",
    "text": "arXiv\nThe multimodality space is now evolving in a much better way. The focus has shifted to finding the bottlenecks and fixing things on the fundamental level in multimodality. This paper from Apple introduces AIMv2, and effort is in a similar direction, except that they only do it for the autoregressive models.\n  \nAIMv2, a family of open vision models pre-trained to generate image patches and text tokens autoregressively. Unlike the other models in this category, it does not require extremely large batch sizes or specialized inter-batch communication methods.\n\nPretraining\n\nIntegrates both images and text into a unified sequence.\nAn image is split into I non-overlapping patches and the text is broken down into subwords. These sequences are concatenated, allowing text tokens to attend to image tokens. The image patches are normalized.\n(image, text) is chosen as the desired sequence to enable stronger conditioning on the visual features. NTP is applied to the above sequence regardless of the modality.\nSeparate loss functions for the image and text parts are defined as shown below: \nThe overall objective is to minimize \\(L= L(text) + α ∗ L(img)\\). \\(L(text)\\) is a standard cross-entropy loss applied to the text domain, whereas \\(L(img)\\) is an l2 pixel-level regression loss for the image domain. The model predicts the next token for the text and the next patch for the image part.\nSeparate linear layers are used for different modalities to map the output of the multimodal decoder to appropriate output dimensions for image patches and vocabulary, respectively.\n\n\n\nArchitecture\n\nViT as the vision encoder. The authors experimented with different-sized encoders ranging from 300M to 3B params. An image resolution of 224px is used.\nSelf-attention with the vision encoder utilizes a prefix attention mask. This strategy facilitates the use of bidirectional attention during inference without additional tuning.\nRandomly sample a prefix length M from the uniform distribution as \\(M ∼ U\\{1, 2, . . . , I − 1\\}\\) where \\(I\\) is the number of image patches.\nThe pixel loss (l2 loss ) is computed exclusively for non-prefix patches \\(\\{ x_i \\ | \\ i &gt; M \\}\\).\nSwiGLU as the FFN layer, and RMSNorm as the normalization layer both in the vision encoder and the multimodal decoder.\nA single multimodal decoder that decodes the next token for both modalities concurrently in an autoregressive fashion. The decoder receives concatenated sequences of image and text features as input and employs causal attention in the self-attention operations. Image features and raw text tokens are each linearly projected and embedded into d dimensional vectors. The outputs of the decoder are processed through two separate linear heads, one for image tokens and another for text tokens, to predict the next token in each modality, respectively.\nIrrespective of the encoder size, the capacity of the decoder is fixed for all experiments.\n\n\n\nPretraining Data Mixture\n\nCombination of public and private datasets containing pairs of images and text data.\nPublic datasets: DFN-2B and COYO\nPrivate datasets: High-Quality Image-Text Pairs (HQITP)\nThe authors generated and used synthetic captions as well.\n\n  \n\n\nHyperparameters for pretraining\nHere is the list of all the hyperparameters used in the pretraining stage. They use SigLIP tokenizer and truncate any text longer than 77 tokens.\n  \nI am once again asking people not to use AdamW anymore and to use SOAP at a bare minimum. This will make your training runs more efficient, and you are likely to save enough $\n\n\nPost-Training\n\nHigh-resolution Adaptation\n\n\nPretraining was done with an image resolution of 224px, but many downstream tasks like detection and segmentation benefit from high-resolution images.\nThe authors fine-tuned the AIMv2 models. The high-resolution adaptation stage utilizes 2 billion image-text pairs sampled from the same pool as the pretraining stage but without any synthetic captions.\nZero for weight decay is important for stable training at this stage.\n\n\nNative Resolution Fine-tuning\n\n\nSeeks to make the model more robust by training it with variable aspect ratios and resolutions.\nFor a mini-batch i, they randomly sample area A and resize the images to fit within this area while maintaining their aspect ratios. Then they adjust the mini-batch size B_i such that \\(C = A_i B_i\\). Here \\(C\\) is the total number of image patches in the mini-batch.\nThe authors chose \\(A = 2^n\\), where n is sampled from a truncated normal distribution within the range \\([−1, 1]\\).\n\n  \n\n\nAttentive Probing\nAim to assess the quality of AIMv2 models as off-the-shelf backbones for recognition benchmarks. Here, the vision encoder remains frozen, and only an attentive probe classifier is trained on top of the last layerfeatures.\n\nAIMV2 significantly outperforms generative unsupervised methods such as MAE and AIM, even with much smaller capacity models.\nAIMV2-1B and the smaller AIMV2-H provide competitive performance, outperforming DINOv2 on several benchmarks, including the IN-1k, Food101, DTD, and Cars datasets.\n\n  \n\n\nObject Detection and Grounding\n\nEvaluate the performance of AIMv2 on Open-Vocabulary Detection (OVD) and Referring Expression Comprehension (REC).\nAIMV2 outperforms DINOv2 and other vision-language pre-trained models on all benchmarks but one, showing strong performance on LVIS.\n\n  \n\n\nMultimodal Understanding\nEvaluate AIMv2 both on instruction tuning and in-context learning.\n\n2-layer MLP connector between the vision encoder and the LLM (e.g., Llama 3.0). The parameters of the vision encoder are frozen during this stage.\nThe connector and the LLM are jointly trained in this stage, but the learning rate for the connector is scaled by a factor of 8.\nThe authors use Llava SFT mixture and Llama-3.0 8B LLM decoder. All models are trained for a single epoch in this stage.\nThe smallest model, AIMV2-L, outperforms CLIP, SigLIP, and DINOv2 on most benchmarks, even when the baselines use larger capacities or higher input resolutions.\nFor in-context learning, the vision encoder in the MM1 settings is replaced by the AIMv2 vision encoder, and the same evaluation setup is established as in MM1."
  },
  {
    "objectID": "posts/paper_summaries/janusflow.html",
    "href": "posts/paper_summaries/janusflow.html",
    "title": "JanusFlow",
    "section": "",
    "text": "arXiv annotated_paper\nWe all have been impressed by the quality of models produced by Deepseek. I thought Qwen was good, but the main highlight is JanusFlow. Apart from the MM1 paper from Apple, I believe JanusFlow is one of the best papers on modern MLLMs. It combines both image understanding and image generation in a single model. The JanusFlow paper is crisp and to the point.\n\nMultiModal Understanding\nIn multimodal understanding tasks, the LLM processes an input sequence consisting of interleaved text and image data.\n\nText is tokenized into discrete tokens. Each token is transformed into an embedding of dimension 𝐷𝑒𝑚𝑏.\nAn image encoder f_enc encodes images into feature maps of shape 𝐻𝑖𝑚 × 𝑊𝑖𝑚 × 𝐷𝑒𝑛𝑐. The authors use a pre-trained SigLIP-Large-Patch/16 model as the image encoder (fenc).\nThe image feature map is flattened and projected through a linear transformation layer into a sequence of embeddings with shape (𝐻𝑖𝑚 𝑊𝑖𝑚) × 𝐷𝑒𝑚𝑏.\nThe text and image embeddings are concatenated to form the input sequence to the LLM. Special tokens |BOI| and |EOI| are inserted before and after the image tokens to help the model locate the image embeddings in the sequence.\nBased on the above sequence of input embeddings, the LLM predicts the next token autoregressively.\n\n  \n\n\nImage Generation\nThe same LLM used for multimodal understanding employs rectified flow for image generation.\n\nGeneration occurs in the latent space using a pre-trained SDXL-VAE.\nThe LLM takes a text sequence 𝑥𝑐𝑜𝑛 as a condition and generates a corresponding image using rectified flow.\nStart by sampling Gaussian noise z0 of shape 𝐻𝑙𝑎𝑡𝑒𝑛𝑡 × 𝑊𝑙𝑎𝑡𝑒𝑛𝑡 × 𝐷𝑙𝑎𝑡𝑒𝑛𝑡 in the latent space.\nA generation encoder genc transforms the above image(noise in the beginning) into a sequence of embeddings (H𝑔𝑒𝑛 W𝑔𝑒𝑛) × 𝐷𝑒𝑚𝑏. The authors use ConvNext blocks initialized from scratch in the generation encoder here.\nThe above sequence is then concatenated with a time embedding representing the current time step 𝑡 (𝑡 = 0 at the beginning), resulting in a sequence of length (𝐻𝑔𝑒𝑛 𝑊𝑔𝑒𝑛 + 1). Special token |BOI| is prepended to indicate the start of image generation in the sequence.\nThe LLM predicts the next token autoregressively. The output is then transformed to a velocity vector of shape 𝐻𝑙𝑎𝑡𝑒𝑛𝑡 × 𝑊𝑙𝑎𝑡𝑒𝑛𝑡 × 𝐷𝑙𝑎𝑡𝑒𝑛𝑡 by a generation decoder gdec. The authors use ConvNext blocks initialized from scratch in the generation decoder.\nThe transformed output state is then updated by a standard Euler solver: 𝑧𝑡+d𝑡 = 𝑧𝑡 + 𝑣(𝑧𝑡, 𝑡)d𝑡. The step size dt is defined by the user.\nUse zdt as the input, replacing z0 in the above steps iteratively till we obtain z1.\nTo improve the image generation quality, the authors use the good old classifier-free guidance (CFG). 𝑤 ⩾ 1 controls the magnitude of CFG, and increasing 𝑤 yields higher semantic alignment.\n\nPay attention to the details here. Not only do the authors employ separate image encoders for understanding and generation, but they also use different kinds of models, SigLIP for understanding and ConvNext for generation.\n\n\nTraining Schemes\nThe authors adopt a three-stage sequential training:\n\nAdaptation: Randomly initialized components such as the linear layers, generation encoder, and generation decoder are trained in this stage.\nUnified pre-training: Once the adaptation phase is complete, the authors train the entire model except for the visual encoder. The training incorporates three data types; multimodal understanding, image generation, and text-only data. At the start, the data mixture contains more multimodal understanding data, and the authors then increase the ratio of image generation data gradually.\nSupervised Fine-tuning (SFT): At this stage, the SigLIP image encoder is also unfrozen. The model is fine-tuned using instruction-tuning data, which comprises dialogues, task-specific conversations, and high-quality text-conditioned image generation examples.\n\nThis training scheme reminds me of the old transfer learning followed by the layer-wise fine-tuning paradigm widely used in the peak CNN era of 2014-18.\n  \nThe first two stages use three types of data: multimodal understanding data, image generation data, and text-only data. The multimodal understanding data contains image-caption pairs, charts and tables, and task data (ShareGPT4V). The image generation data comes from different sources like LAION-Aesthetics, SAM, etc., and 2 million in-house samples. All samples go through a filtration process to ensure only high quality. For text-only data, the authors use the text corpus of DeepSeek-LLM. Similarly, the SFT uses three types of data: Multimodal instruction data, image generation data, and text-only data.\n\n\nTraining Objective\nTraining JanusFlow involves two types of data, multimodal understanding data and image generation data. Both types of data contain two parts: “condition” and “response”. The data is formatted in pairs as 𝑥 = (𝑥𝑐𝑜𝑛, 𝑥𝑟𝑒𝑠), where the superscript 𝑐𝑜𝑛 denotes “condition” and 𝑟𝑒𝑠 denotes “response”. The sequence length of x is denoted by L, while the sequence lengths of xcon and xres are denoted by Lcon and Lres respectively. 𝜃 denotes all the trainable parameters.\n\nAutoregression Objective: For multimodal understanding tasks, 𝑥𝑟𝑒𝑠 contains only text tokens, and the model is trained using the maximum likelihood principle where the loss is calculated over tokens in xres.\nRectified Flow Objective: For image generation tasks, 𝑥𝑐𝑜𝑛 consists of text tokens and 𝑥𝑟𝑒𝑠 is the corresponding image, and the model is trained with the rectified flow objective. 10% of the text prompt is dropped during training to enable CFG.\n\n  \n\nRepresentation Alignment Regularization: For generation tasks, features from the understanding encoder 𝑓𝑒𝑛𝑐 are aligned with the intermediate features of the LLM as shown below. The function sim(...) computes the mean of element-wise cosine similarity between embeddings. This alignment loss helps the LLM’s internal feature space (given noisy input 𝑧𝑡) align with the understanding encoder’s semantic feature space, thereby improving generation quality when producing images from new random noise and text conditions during inference.\n\n  \nAll three objectives are applied across all training stages. Multimodal understanding tasks use L𝐴𝑅, while image generation tasks employ the combined loss L𝑅𝐹 + L𝑅𝐸𝑃𝐴.\n\n\nHparams used in training\nHere is the list of all the hparams used in the training at different stages:\n  \n\n\nResults\nHere are the results for both image generation and multimodal understanding tasks. The one big complaint I have in these evaluations is the use of FID. I think it is high time to ditch the FID. It is meaningless most of the time."
  },
  {
    "objectID": "posts/ml_dl_concepts/rope.html",
    "href": "posts/ml_dl_concepts/rope.html",
    "title": "Rotary Position Embeddings",
    "section": "",
    "text": "Why bother with position encoding?\nSelf-attention in transformer-based models is one of the most used operations in the history of deep learning. Though self-attention is extremely powerful, one of the weaknesses of self-attention is that it treats a sequence as a set of tokens. It is not position-aware, and it is permutation equivariant.\nThe order of words in a language matters; hence, we need a mechanism to insert word order information in our model. This is where position encoding kicks in.\n\n\nPreliminary\nLet \\(S_N=\\{w_i\\}^N_i=1\\) be a sequence of \\(N\\) input tokens with \\(w_i\\) being the ith token. Each token(word in this case) in the sequence is defined by a \\(d\\)-dimensional vector embedding containing no position information. The self-attention layer first incorporates position information into the word embeddings and then transforms them into queries, keys, and value representations. We can define these transforms as shown below:\n\\[\n\\begin{align}\n    & q_m = f_q(x_m, m) \\\\\n    & k_n = f_k(x_n, n) \\tag{1} \\\\\n    & v_n = f_v(x_n, n)\n\\end{align}\n\\]\nwhere \\(m\\) and \\(n\\) represents the \\(mth\\) and \\(nth\\) positions respectively.\n\n\nDesired properties of a position encoding\nIf we encode positions for the model, what would be the ideal design? The simplest thing we can do is initialize a learnable position embedding and let the model figure out the values during training. Not a bad idea for starters, but can we do better? What are the desirable properties of an encoding scheme if it is not learned implicitly? Here are a few of them:\n\nA unique value for every position in a sequence: The encoding scheme should assign a unique value for every position irrespective of the sequence length. \nConsistent relative distance between two positions: Irrespective of the length of a sequence, the relative distance between two positions should be consistent across sequences. For example, the relative distance between encodings of the 2nd position and 3rd position should be the same in sequences of different lengths. \nLong-term decay: An inductive bias about words is that words far distant from the current word carry less relevant information. That means our position encoding should follow a long-term decay effect for relatively distant positions. \nExtensible: What if we encounter a lengthier sequence at test time than the length of any sequence encountered during training? Ideally, we want our encoding scheme to be extensible, without much effort and breaking any other assumption. \nDeterministic: Determinism is mostly a nice-to-have property. It can help debug a few aspects if we encounter something unexpected.\n\nThere are different encoding schemes, e.g., absolute position encoding, binary encoding, relative position encoding, rotary position encoding, etc. Here, we will discuss the two most widely used position encodings: sinusoidal position encoding and rotary position encoding. Assuming the reader is familiar with concepts like absolute and binary position encodings, we will skip that discussion here. We could have skipped sinusoidal position encoding, but it lays the foundation of rotary position encoding. Also, we will try to keep the notations in line with the Roformer paper as much as possible. \n\n\nSinusoidal Positional Encoding\nA typical choice for equation(1) is to formulate it this way: \\[\nf_t(x_i, i) = W_t(x_i + p_i) \\tag{2} \\\\\n\\]\nwhere \\(t \\in (q, k, v)\\), and \\(p_i\\) is represents a \\(d\\) dimensional vector depending of the position of token \\(x_i\\). It simply means that we encode the position and add it to the token embedding.\nSinusoidal encoding was proposed in the paper Attention is All You Need. They proposed to generate \\(p_i\\) in the above equation using the sinusoidal function: \\[\n\\begin{cases}\np_{i,2t} &= \\sin(k/10000^{2t/d}) \\\\\np_{i,2t+1} &= \\cos(k/10000^{2t/d}) \\\\ \\tag{3}\n\\end{cases}\n\\]\nwhere \\(p_{i,2_t}\\) is the \\(2t\\)h element of the d-dimensional vector \\(p\\). The wavelengths form a geometric progression from \\(2π\\) to \\(10000 · 2π\\).\nMany people do not know sinusoidal encoding was suggested so the model can learn to attend to relative positions. Yes, relative positions! This is mostly because people do not read papers with “attention”. Let us take an example with \\(d=2\\) to prove it.\n\\[\n\\begin{align*}\n    d = 2 \\rightarrow i={[0, 1]}\n\\end{align*}\n\\]\nNow calculate the sinusoidal position encodings for \\(position=pos\\) using equation 3:\n\\[\n\\begin{align*}\n    PE_{pos,0} &= \\sin(pos/10000^{2*0/d}) = sin(pos) \\\\\n    PE_{pos,1} &= \\cos(pos/10000^{2*0/d}) = cos(pos) \\\\\n\\end{align*}\n\\]\nSimilarly, let us calculate the sinusoidal position encodings for an offset \\(k\\) i.e., \\(position=pos+k\\):\n\\[\n\\begin{align*}\n    PE_{pos+k, 0} &= \\sin((pos+k)/10000^{2*0/d}) = \\sin(pos+k) \\\\\n    PE_{pos+k, 1} &= \\cos((pos+k)/10000^{2*0/d}) = \\cos(pos+k) \\\\\n\\end{align*}\n\\]\nExpanding \\(sin(pos+k)\\) and \\(cos(pos+k)\\), we have:\n\\[\n\\begin{align*}\n    \\sin(pos+k) &= \\sin(pos)\\cos(k) + \\cos(pos)\\sin(k) \\\\\n                &= PE_{pos, 0} \\sin(k) + PE_{pos, 1} \\cos(k) \\tag{4} \\\\ \\\\\n    \\cos(pos+k) &= \\cos(pos)\\cos(k) - \\sin(pos)\\sin(k) \\\\\n                &= PE_{pos, 1} \\cos(k) - PE_{pos, 0} \\sin(k) \\tag{5} \\\\\n\\end{align*}\n\\]\nWe can combine equation (4) and equation (5) in a nice matrix notation:\n\\[\n\\begin{align*}\n\\begin{bmatrix}\n    PE_{pos+k,0} \\\\\n    PE_{pos+k,1}\n\\end{bmatrix} &=\n\\underbrace{\\begin{bmatrix}\n    \\cos(k)  & \\sin(k) \\\\\n    -\\sin(k) & \\cos(k)\n\\end{bmatrix}\n}_{Rotation \\ Matrix}\n\\begin{bmatrix}\n    PE_{pos,0} \\\\\n    PE_{pos,1}\n\\end{bmatrix}\n\\end{align*}\n\\]\nThat’s a transposed rotation matrix! So, our models might have been learning to attend relative positions since 2017 because for any fixed offset \\(k, \\ PE_{pos+k}\\) can be represented as a linear function of \\(PE_{pos}\\). If that is the case, what is wrong with sinusoidal position encoding? A few things…\nTake a look at the visualization below. We have a two-dimensional vector \\(x=[1, 1]\\), and we add sinusoidal position encoding for different positions to this vector.\nVideo\nThough we assume that the model can attend to relative positions easily, the one thing that is a bit bothering, as noted in the above visualization, is the stochasticity. Our pointer is moving almost randomly. The pattern here does not look good. But this doesn’t necessarily mean it is also bad for the model to learn, especially at scale. All we can say is that with so much stochasticity across different dimensions, the model may have a hard time learning to attend to relative positions.\nAnother thing to note from equation (2) is that sinusoidal positional encodings are additive. This means indirectly adding the “relatedness” of two tokens to the positional encoding. A side effect of this is that two highly related tokens can get high attention scores irrespective of the distance between them. This is still okay for NLP-related problems but doesn’t hold for other domains like protein sequencing.\nThe authors of the Attention is All You Need paper also hypothesized that hard-coded sinusoidal positional encodings may help to extrapolate to longer sequences compared to the length of the sequences seen during training. The hypothesis does not hold in practice.\n\n\nRotary Position Encoding: The Easy Way\nFrom the above discussion, two things are clear:\n\nEncoding relative positions is crucial for attention, but additive methods like sinusoidal encodings are not the best way to do it.\nIncorporating a rotation matrix in some form is a good choice for encoding relative positions.\n\nRoPE leverages the above findings. It rotates an encoded vector by some angle \\(\\theta\\) based on its position in a sequence. Is it any good? Suppose Python is the query token, love is the key token, and we want to compute attention scores between them in two different sequences as given below:\n\nPython is love\nCoding in Python is love\n\nThis is how the tokens will be rotated in each of the two sequences:\nVideo\nThe relative distance between our query token Python and the key token love is \\(2\\theta\\) in both sequences. One interesting aspect is that the number of tokens occurring after the query token (Python in this case) does not affect the embedding. The amount of rotation is purely dependent on the token’s position in the sequence. The inner product \\(q^Tk\\) remains the same in both sequences. For a 2D case, if we generalize the rotation used in RoPE, we can rewrite our rotation matrix as follows:\n\\[\n\\begin{align*}\n    R_\\Theta^p =\n    \\begin{bmatrix}\n    \\cos(p\\theta)  & -\\sin(p\\theta) \\\\\n    \\sin(p\\theta) & \\cos(p\\theta) \\tag{6}\n    \\end{bmatrix}\n\\end{align*}\n\\]\nwhere \\(p\\) is the token’s position in a sequence. Let us revisit equation(1) now:\n\\[\n\\begin{align*}\n    & q_m = f_q(x_m, m) \\\\\n    & k_n = f_k(x_n, n)  \\\\\n    & v_n = f_v(x_n, n)\n\\end{align*}\n\\]\nThe term \\(q^T_mk_n\\) enables knowledge conveyance between tokens at different positions in the attention mechanism. We are interested in finding a transformation with the hope that the inner product encodes position information only in the relative form:\n\\[\n\\begin{align*}\n\\langle f_q(\\boldsymbol{x}_m, m), f_k(\\boldsymbol{x}_n, n)\\rangle = g(\\boldsymbol{x}_m, \\boldsymbol{x}_n, m-n).\n\\end{align*}\n\\]\nLet us check how RoPE fulfills this criteria. Note that \\(m\\) and \\(n\\) represents the \\(mth\\) and \\(nth\\) positions respectively. Also, to clarify \\(q\\) and \\(k\\) represent a single query and a single key. Do not get confused by the notation.\n\\[\n\\begin{align*}\n    q_m &= f_q(x_m, m) = R_\\Theta^m \\ W_q x_m \\\\\n    k_n &= f_k(x_n, n) = R_\\Theta^n \\ W_k x_n \\\\ \\\\\n    q_m^T \\ k_n &= x_m^T W_q^T \\ (R_\\Theta^m)^T \\ \\ R_\\Theta^n \\ W_k x_n \\\\\n              &= x_m^T W_q^T \\ R_\\Theta^{-m}  \\ \\   R_\\Theta^n \\ W_k x_n \\quad\n                    \\text{(Transpose of a rotation matrix is its inverse)} \\\\\n              &= x_m^T W_q^T \\ R_\\Theta^{n-m} \\ W_k x_n \\\\\n\\end{align*}\n\\]\nSo, RoPE is encoding the relative position information in the attention calculation, which we wanted. Remember the point we made about stochasticity in sinusoidal encodings? Does RoPE fare better in that aspect? Here is how our vector moves in 2D if we use RoPE instead of sinusoidal encodings:\nVideo\nWe looked only at a 2D case, but we would want to generalize this for \\(d\\) dimensions with \\(d &gt; 2\\). Before doing that, let us look at it from another perspective, similar to what was presented in the Roformer paper.\n\n\nRotary Position Encoding: The Mathematical View\nWe are looking for something that can encode relative positions in the attention mechanism, and it should not be an additive method like sinusoidal position encoding. Mathematics is the solution to all our problems! We will exploit the geometrical properties of vectors, especially the complex form/representation of vectors. But why the complex form? We will get an answer to that question once we have finished our quest to find the desired transformation for our query and key vectors.\nLet us rewrite our query and key transforms presented in equation (1), in complex form:\n\\[\n\\begin{align*}\nq_m &= f_q(\\boldsymbol{x}_q, m) = R_q(\\boldsymbol{x}_q, m) \\ e^{i\\Theta_q(\\boldsymbol{x}_q,m)} \\\\\nk_n &= f_k(\\boldsymbol{x}_k, n) = R_k(\\boldsymbol{x}_k, n) \\ e^{i\\Theta_k(\\boldsymbol{x}_k,n)} \\tag{7}\n\\end{align*}\n\\]\nAssuming there exists a transformation \\(g\\) shown below capable of encoding relative position, we aim to find a solution of \\(f_q\\) and \\(f_k\\)\n\\[\n\\begin{align*}\ng(\\boldsymbol{x}_m, \\boldsymbol{x}_n, m-n) &= \\langle f_q(\\boldsymbol{x}_m, m), f_k(\\boldsymbol{x}_n, n)\\rangle \\\\\n    &=  \\langle R_q(\\boldsymbol{x}_q, m) \\ e^{i\\Theta_q(\\boldsymbol{x}_q, \\ m)},\n        R_k(\\boldsymbol{x}_k, n) \\ e^{i\\Theta_k(\\boldsymbol{x}_k,\\ n)} \\rangle \\\\\n    &=  R_g(\\boldsymbol{x}_q, \\boldsymbol{x}_k, n-m) \\\n        e^{i\\Theta_g(\\boldsymbol{x}_q, \\ \\boldsymbol{x}_k, \\ n-m)} \\tag{8} \\\\ \\\\\n\\end{align*}\n\\]\nwhere \\(R\\) and \\(\\Theta\\) represents radial and angular components respectively. Plugging them in equation (7), gives us:\n\\[\n\\begin{align*}\nR_q(\\boldsymbol{x}_q, m)R_k(\\boldsymbol{x}_k, n) &= R_g(\\boldsymbol{x}_q, \\boldsymbol{x}_k, n-m) \\\\\n\\Theta_k(\\boldsymbol{x}_k, n) - \\Theta_q(\\boldsymbol{x}_q, m) &= \\Theta_g(\\boldsymbol{x}_q, \\boldsymbol{x}_k, n-m) \\tag{9}\n\\end{align*}\n\\]\nAlso, when no positional information is provided, we expect the following conditions to be satisfied:\n\\[\n\\begin{align*}\n    \\boldsymbol{q} &= \\|q\\|e^{i\\theta_q} = f_q(\\boldsymbol{x}_q, 0) = R_q(\\boldsymbol{x}_q,0)e^{i\\Theta_q(\\boldsymbol{x}_q,0)} \\\\\n    \\boldsymbol{k} &= \\|k\\|e^{i\\theta_k} = f_q(\\boldsymbol{x}_k, 0) = R_k(\\boldsymbol{x}_k,0)e^{i\\Theta_k(\\boldsymbol{x}_k,0)} \\tag{10}\n\\end{align*}\n\\]\nWhen \\(m=n\\), we have:\n\\[\n\\begin{align*}\n    R_q(\\boldsymbol{x}_q, m)R_k(\\boldsymbol{x}_k, m) &= R_g(\\boldsymbol{x}_q, \\boldsymbol{x}_k, 0) \\ \\text{using eq. 8} \\\\\n    &= R_q(\\boldsymbol{x}_q, 0)R_k(\\boldsymbol{x}_k, 0) \\\\\n    & = \\|q\\|\\|k\\| \\tag{11a} \\\\\n    \\Theta_k(\\boldsymbol{x}_k, m) - \\Theta_q(\\boldsymbol{x}_q, m) &= \\Theta_g(\\boldsymbol{x}_q, \\boldsymbol{x}_k, 0) \\\\\n    &= \\Theta_k(\\boldsymbol{x}_k, 0) - \\Theta_q(\\boldsymbol{x}_q, 0) \\\\\n    &= \\theta_k - \\theta_q \\tag{11b}\n\\end{align*}\n\\]\nFrom equation \\((11a)\\), we have \\(R_g(\\boldsymbol{x}_q, \\boldsymbol{x}_k, n-m) = R_g(\\boldsymbol{x}_q, \\boldsymbol{x}_k, 0) = \\|q\\|\\|k\\|\\) implying that all radial components \\(R_g, R_q, R_k\\) are independent from the position information.\nSimilarly, from equation \\((11b)\\), we have \\(\\Theta_q(\\boldsymbol{x}_q, m) - \\theta_q = \\Theta_k(\\boldsymbol{x}_k, m) - \\theta_k \\text{ for all } q, k, m\\), which indicates that the angular components do not dependent on the query and the key vectors, and depends only on the position \\(m\\). We can simply this by rewriting the above equation:\n\\[\n\\begin{align*}\n    \\Theta_q(\\boldsymbol{x}_q, m) - \\theta_q &= \\Theta_k(\\boldsymbol{x}_k, m) - \\theta_k \\\\\n    \\Theta_q(\\boldsymbol{x}_q, m) - \\Theta_k(\\boldsymbol{x}_k, m) &= \\theta_q - \\theta_k \\\\\n    \\Theta_f(\\boldsymbol{x}_{\\{q, k\\}}, m) - \\theta_{\\{q, k\\}} &= \\phi(m) \\tag{12}\n\\end{align*}\n\\]\nSuppose we have \\(n=m+1\\), plugging this in equation (9), and using equation (12), we get:\n\\[\n\\phi(m+1) - \\phi(m) = \\Theta_g(\\boldsymbol{x}_q, \\boldsymbol{x}_k, 1) + \\theta_q - \\theta_k\n\\]\nSince RHS in the above equation is a constant and does not depend on \\(m\\), with continuous integer inputs, it produces an arithmetic progression that can be written as:\n\\[\n\\phi(m) = m\\theta + \\gamma \\tag{13}\n\\]\nwhere \\(\\theta, \\gamma \\in \\mathbb{R}\\) are constants and \\(\\theta\\) is non-zero. We can simply set \\(\\gamma=0\\).\nTherefore, for a 2D case, we can write \\(f_q\\) and \\(f_k\\) as:\n\\[\n\\begin{align*}\nf_q(\\boldsymbol{x}_m, m) &= (W_q\\boldsymbol{x}_m)e^{im\\theta} \\\\\nf_k(\\boldsymbol{x}_n, n) &= (W_k\\boldsymbol{x}_n)e^{in\\theta} \\\\\ng(\\boldsymbol{x}_m, \\boldsymbol{x}_n, m-n) &= Re[(W_q\\boldsymbol{x}_m)(W_k\\boldsymbol{x}_n)^*e^{i(m-n)\\theta}]\n\\end{align*}\n\\]\nwhere \\(Re[·]\\) is the real part of a complex number and \\((W_k x_n)^*\\) represents the conjugate complex number of \\((W_k x_n)\\). We can further write \\(f_{\\{q, k\\}}\\) in the form of matrix multiplication as:\n\\[\n\\begin{align*}\n    f_{\\{q,k\\}}(\\boldsymbol{x}_m, m) =\n    \\begin{pmatrix}\n        \\cos m\\theta & -\\sin m\\theta \\\\\n        \\sin m\\theta & \\cos m\\theta\n    \\end{pmatrix}\n    \\begin{pmatrix}\n        W_{\\{q,k\\}}^{(11)} & W_{\\{q,k\\}}^{(12)} \\\\\n        W_{\\{q,k\\}}^{(21)} & W_{\\{q,k\\}}^{(22)}\n    \\end{pmatrix}\n    \\begin{pmatrix}\n        x_m^{(1)} \\\\\n        x_m^{(2)}\n    \\end{pmatrix}  \\tag{13}\n\\end{align*}\n\\]\nRepresenting the vectors in their complex form helps us formulate the maths of it in a much simpler way.\n\n\nBeyond 2D\nGeneralizing our 2D results to \\(d\\) dimensions is easy. If our embeddings are d-dimensional (\\(d&gt;2\\), and d is even), we can split them into \\(d/2\\) blocks, and for each block, we can repeat the same thing we have for the 2D case. That way, we will end up with a diagonal rotation matrix, where the value of theta differs for each dimension. We can then apply the rotation matrix to each block independently and combine the results afterward. The rotation speed varies across the blocks.\n\\[f_{\\{q,k\\}}(x_m,m) = \\boldsymbol{R}^d_{\\Theta,m}\\boldsymbol{W}_{\\{q,k\\}}x_m \\tag{14}\\]\nwhere\n\\[\n\\boldsymbol{R}^d_{\\Theta,m} =\n        \\begin{pmatrix}\n            \\cos m\\theta_1 & -\\sin m\\theta_1 & 0 & 0 & \\cdots & 0 & 0 \\\\\n            \\sin m\\theta_1 & \\cos m\\theta_1 & 0 & 0 & \\cdots & 0 & 0 \\\\\n            0 & 0 & \\cos m\\theta_2 & -\\sin m\\theta_2 & \\cdots & 0 & 0 \\\\\n            0 & 0 & \\sin m\\theta_2 & \\cos m\\theta_2 & \\cdots & 0 & 0 \\\\\n            \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n            0 & 0 & 0 & 0 & \\cdots & \\cos m\\theta_{d/2} & -\\sin m\\theta_{d/2} \\\\\n            0 & 0 & 0 & 0 & \\cdots & \\sin m\\theta_{d/2} & \\cos m\\theta_{d/2}\n\\end{pmatrix}  \\tag{15}\n\\]\nRoPE is then applied to the query and key vectors for attention calculation as:\n\\[\n\\boldsymbol{q}_m^\\top \\boldsymbol{k}_n =\n(\\boldsymbol{R}_{\\Theta,m}^d \\boldsymbol{W}_q \\boldsymbol{x}_m)^\\top (\\boldsymbol{R}_{\\Theta,n}^d \\boldsymbol{W}_k \\boldsymbol{x}_n)\n= \\boldsymbol{x}_m^\\top \\boldsymbol{W}_q^\\top \\boldsymbol{R}_{\\Theta,n-m}^d \\boldsymbol{W}_k \\boldsymbol{x}_n\n\\tag{16}\n\\]\nHere is an example with \\(d=4\\). The query vector is split into blocks of size two, and rotation is applied to each group independently. Notice the difference in the speed of the two pointers shown below.\nVideo\nA recent paper from DeepMind found some interesting aspects about these frequencies. For example, in the case of Gemma-7B, these high frequencies are responsible for the diagonal head and the previous token head in some layers.\n\nOn the other hand, the low frequencies are not very sensitive to the relative distance. It helps the transformers to maintain semantic attention over a large context. This is why models with very long context windows tend to use a very high value for the base frequency \\((\\theta)\\) in practice. For example, with a context length of 128K, LLama 3 uses a base frequency of 500,000 leading the low frequencies to rotate at roughly \\(1/500,000\\) radians per token.\n\n\nOptimization\nThere is one major problem with equation (16). Our rotation matrix is sparse, and though sparsity is good in many places, it is not in this case. We are wasting memory because of that sparsity. Another issue is that equation (16) is not computationally efficient.\nWe can fix both the issue with a bit of cleverness. Taking advantage of that sparsity along with elementwise operations, we can compute the same thing more efficiently, as shown below:\n\\[\n\\boldsymbol{R}^d_{\\Theta,m} \\boldsymbol{x} =\n\\begin{pmatrix}\n    x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ \\vdots \\\\ x_{d-1} \\\\ x_d\n\\end{pmatrix}\n\\otimes\n\\begin{pmatrix}\n    \\cos m\\theta_1 \\\\ \\cos m\\theta_1 \\\\ \\cos m\\theta_2 \\\\ \\cos m\\theta_2 \\\\ \\vdots \\\\ \\cos m\\theta_{d/2} \\\\ \\cos m\\theta_{d/2}\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n    -x_2 \\\\ x_1 \\\\ -x_4 \\\\ x_3 \\\\ \\vdots \\\\ -x_d \\\\ x_{d-1}\n\\end{pmatrix}\n\\otimes\n\\begin{pmatrix}\n    \\sin m\\theta_1 \\\\ \\sin m\\theta_1 \\\\ \\sin m\\theta_2 \\\\ \\sin m\\theta_2 \\\\ \\vdots \\\\ \\sin m\\theta_{d/2} \\\\ \\sin m\\theta_{d/2}\n\\end{pmatrix}\n\\tag{17}\n\\]\n\n\n\nWhat else?\n\nIs RoPE all we need? Though RoPE seems the perfect encoding, it is far from ideal. For example, the idea of long-term decay does not hold in many situations with RoPE.\nWhat about other options like NoPE, HoPE, etc? None of these encodings is a clear winner in every case. In many situations, sinusoidal is more than sufficient. The advantage of using RoPE is that it is a solid baseline over others.\nDoes RoPE work well for other modalities? People like to slap RoPE in every possible situation. Have you ever wondered how RoPE behaves with image tokens? You may find a thing or two in there.\nWhat about extending the context length? We will talk about it in the next part\n\n\n\nReferences\n\nAttention Is All You Need\nRoformer\nRound and Round We Go!\nRotary Embeddings: A Relative Revolution\nRoPE explained\nHow Rotary Position Embedding Supercharges Modern LLMs\nManim examples"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About me",
    "section": "",
    "text": "Hello, I am Aakash Kumar Nain.\nI am a Machine Learning Engineer with 8 years of experience in the industry. Currently, I am working as a Senior ML Engineer at Merlyn Mind. I am also a Google Developers Expert in Machine Learning and JAX. I actively contribute to Keras, TensorFlow, and the JAX ecosystem. I am one of the core collaborators for the Keras core project, and I am also one of the maintainers of the TensorFlow-addons package.\nIn terms of work, my knowledge Machine Learning and Deep Learning spans in T-shape. I have worked in multiple domains, but Computer Vision with Deep Learning is my favorite field. I love simplifying Data Science and Machine Learning concepts, especially the maths behind them. Love and support OSS because, without OSS, we wouldn’t have witnessed so many breakthroughs in Machine Learning and Deep Learning. Python is my favorite language to work with, and I absolutely love coding in it. Always up for a discussion involving anything related to Machine Learning, Deep Learning, MLOps, and API design. Simply put: Python in breakfast, Machine Learning in lunch, and Deep Learning in dinner!\nFollow me on Twitter to get the latest updates related to Machine Learning."
  },
  {
    "objectID": "index.html#popular-projects",
    "href": "index.html#popular-projects",
    "title": "About me",
    "section": "Popular Projects",
    "text": "Popular Projects\nAnnotated Research Papers   \nMost researchers and Machine Learning engineers want to remain up-to-date with the latest research work, but the field of machine learning moves crazy fast. The number of papers uploaded on arXiv is witnessing exponential growth. Reading all these published papers is an impossible task. To help with this, I maintain this repository where I annotate and upload the papers that I find exceptionally good. Annotations make it easier to understand and grasp the main concepts explained in the paper. \nTF-JAX Tutorials   \nIt is a series of tutorials built to teach the fundamental concepts and the underlying working of two famous libraries: TensorFlow and JAX. These tutorials aren’t typical documentation-type tutorials. It doesn’t matter whether you are a beginner or an advanced user, these tutorials will give you a fresh perspective on building things using TensorFlow or JAX. \nDiffusion Models   \nDiffusion models are a class of likelihood-based generative models that recently have been used to produce very high-quality images compared to other existing generative models like GANs. It’s hard to find quality resources to learn about diffusion models. The mathematics behind the diffusion models is also a bit harder to understand. The material presented in this repo is enough to make you understand the working of diffusion models and the underlying math. \nMistral-7B in JAX   \nThis is a direct port of Mitral-7B model in JAX and Equinox. This port comes with two implementations: The first one shows how to port models from PyTorch to Equinox and JAX. This is a 1:1 mapping, and not fully optimized. The second implementation is fully optimized for JAX, targeted for more advanced users."
  },
  {
    "objectID": "index.html#keras-core-contributions",
    "href": "index.html#keras-core-contributions",
    "title": "About me",
    "section": "Keras Core Contributions",
    "text": "Keras Core Contributions\n\nJAX NN ops\nMerging layers\nMetrics\nLoss functions\nApplications\nData adapters\nCode examples"
  },
  {
    "objectID": "index.html#keras-contributions",
    "href": "index.html#keras-contributions",
    "title": "About me",
    "section": "Keras Contributions",
    "text": "Keras Contributions\n\nDenoising Diffusion Models\nOCR model for reading captchas\nHandwriting recognition\nImage captioning\nWGAN-GP\nCycleGAN\nModel interpretability with Integrated Gradients"
  },
  {
    "objectID": "index.html#equinox-contributions",
    "href": "index.html#equinox-contributions",
    "title": "About me",
    "section": "Equinox Contributions",
    "text": "Equinox Contributions\n\nMistral-7B implementation\nBug fix in RMSNorm Layer\nAMP in Normalization Layers"
  },
  {
    "objectID": "index.html#tensorflow-contributions",
    "href": "index.html#tensorflow-contributions",
    "title": "About me",
    "section": "TensorFlow Contributions",
    "text": "TensorFlow Contributions\n\nKappa\nGelu activation\nFocal loss\nThresholded Linear Unit"
  }
]