[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "papers\n\n\nsummary\n\n\nresearch\n\n\nLLMs\n\n\n \n\n\n\n\nNov 20, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nLLMs\n\n\n \n\n\n\n\nNov 13, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nvision\n\n\n \n\n\n\n\nNov 8, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\ntransformers\n\n\nscaling\n\n\n \n\n\n\n\nNov 4, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nVLMs\n\n\nMLLMs\n\n\n \n\n\n\n\nOct 28, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\ntransformers\n\n\nresearch\n\n\nLLMs\n\n\n \n\n\n\n\nOct 23, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\ntransformers\n\n\nresearch\n\n\nLLMs\n\n\nmodel_merging\n\n\n \n\n\n\n\nOct 15, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nagents\n\n\n \n\n\n\n\nSep 24, 2024\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html#paper-summaries",
    "href": "blog.html#paper-summaries",
    "title": "Blog Posts",
    "section": "",
    "text": "papers\n\n\nsummary\n\n\nresearch\n\n\nLLMs\n\n\n \n\n\n\n\nNov 20, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nLLMs\n\n\n \n\n\n\n\nNov 13, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nvision\n\n\n \n\n\n\n\nNov 8, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\ntransformers\n\n\nscaling\n\n\n \n\n\n\n\nNov 4, 2024\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nVLMs\n\n\nMLLMs\n\n\n \n\n\n\n\nOct 28, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\ntransformers\n\n\nresearch\n\n\nLLMs\n\n\n \n\n\n\n\nOct 23, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\ntransformers\n\n\nresearch\n\n\nLLMs\n\n\nmodel_merging\n\n\n \n\n\n\n\nOct 15, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npapers\n\n\nsummary\n\n\nresearch\n\n\nagents\n\n\n \n\n\n\n\nSep 24, 2024\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Cut Your Losses in Large-Vocabulary Language Models\n\n\n\n\n\n\n\n\n\nNov 20, 2024\n\n\n\n\n\n\n\n\nThe Super Weight in Large Language Models\n\n\n\n\n\n\n\n\n\nNov 13, 2024\n\n\n\n\n\n\n\n\nDepth Pro\n\n\n\n\n\n\n\n\n\nNov 8, 2024\n\n\n\n\n\n\n\n\nA Hitchhiker’s Guide to Scaling Law Estimation\n\n\n\n\n\n\n\n\n\nNov 4, 2024\n\n\n\n\n\n\n\n\nOmniParser for Pure Vision Based GUI Agent\n\n\n\n\n\n\n\n\n\nOct 28, 2024\n\n\n\n\n\n\n\n\nNormalized Transformer\n\n\n\n\n\n\n\n\n\nOct 23, 2024\n\n\n\n\n\n\n\n\nWhat Matters for Model Merging at Scale?\n\n\n\n\n\n\n\n\n\nOct 15, 2024\n\n\n\n\n\n\n\n\nAgent WorkFlow Memory\n\n\n\n\n\n\n\n\n\nSep 24, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/paper_summaries/agent_workflow_memory.html",
    "href": "posts/paper_summaries/agent_workflow_memory.html",
    "title": "Agent WorkFlow Memory",
    "section": "",
    "text": "arXiv annotated_paper\nFor almost six months, I have been vocal about the differentiation between tool usage with LLMs and an agent. Most people correlate pure tool usage with agentic behavior, but IMHO that is only a subset of it. If we want to deploy agents, we need some planning and reasoning capabilities. Reasoning is abstract and hard to solve. Planning on the other hand comes in many flavors. If we want to go to system 2, we must augment planning capabilities with memory. After all, an intelligent system should be able to extract and learn from past behavior.\nThe paper Agent WorkFlow Memory, which came out last week, tries to solve some of these problems, and I think their choice is a good direction.\n  \n\nWhy do we want to implement a memory module?\n\nCurrent language model-based agents struggle with long-horizon tasks and complex action trajectories.\nWhen presented with a new task, a common way to induce knowledge about the task is to perform in-context learning or fine-tuning.\nThis approach works for a given task but fails to generalize across different tasks even if they share the same pattern. For example, planning a trip to a foreign country vs planning a local trip share common workflows with minor differences in the booking procedure.\nInspired by how humans abstract common task routines from past experiences, we can implement a workflow memory module where each workflow represents a goal with a common routine extracted from available action trajectories.\n\n  \n\n\nAgent Workflow Memory\nLet us define the system first. Suppose we have an LLM L, responsible for carrying out the workflows, a text-based Memory M, where the base memory contains documentation of built-in actions such as CLICK and TYPE.\nUNTIL ai != STOP or curr_step &gt; MAX_STEPS_ALLOWED do;\n\nTo solve a task specified by an instruction in natural language q, the agent acts in an environment defined by a transition function T.\nAt each time step ti, the environment state si gives observation oi. This observation is then passed into the model to generate action ai via L(q, M, oi) → ai.\nThe action is executed in the environment and changes the state as T(si, ai) → si+1.\n\n\nEach completed task forms an experience. This experience is used to extend the workflow memory. For each experience that we store, it includes:\n\nThe instruction provided: q\nThe trajectory contains the steps used to solve the workflow. Each step p comprises the current state, and the action taken i.e. p=(o, a)\nAn induction module is to induce useful workflows W = {w} from the set of experiences E = {e} constructed from past or collected examples. These workflows are then added to the memory or subsequent task-solving.\n\n\n\nLM-based Workflow Induction\n\nThe induction module I induces a set of workflows W from one or more past agent experiences E.\nModule I prompts the agent to extract common sub-routines from one or more input experiences.\nIf we want to extract common patterns from different workflows, we need to provide more fine-grained instructions. For example, instead of prompting “buy a cover for pixel9”, the prompt should be “buy a black leather cover for my pixel from Amazon and deliver it to my address”. Fine-grained instructions help extract common tasks or patterns easily.\nAfter the workflows W are induced, they are then integrated into the agent as auxiliary memory, M + W →Mw.\nWhen solving a given instruction q, the agent then produces a series of actions defined by  L(q, Mw, o) = L(q, M + W, o) → a\n\n\n\n\nOffline Induction\n\nAssume that you have annotated examples that represent some experience for some workflow.\nInduction involves two standalone processes:\n\nConcatenate all training examples into a single prompt and feed them to the LM to create a set of workflows at ‘training’ time; I(Etrain) → Woffline\nIncorporates all induced workflows into the agent memory at inference time to solve test instructions L(q, M + W, otest ) → atest\n\n\n  \n\n\nOnline Induction\nAddresses the question: What if the agent has not seen this workflow in the past? Works in a supervision-free setting where only test queries are needed. Agents process test queries in a streaming fashion, where the agents conduct the loop of inducing, integrating, and utilizing workflows after running inference for each test task.\n\nAssume the default memory is M.\nAt test timestep t, instruction qt is passed to the agent. The agent tries to solve the task by creating a trajectory. This instruction paired with the trajectory is the experience formed at this timestep.\nUsing LLM as a judge, a binary label {0, 1} is assigned to the output of the trajectory denoting failure/success.\nEvery time an experience is considered a success, it is transformed into workflow(s) I(et) → {wt}. {wt} is added into the agent memory Mt + {wt} → M(t+1)"
  },
  {
    "objectID": "posts/paper_summaries/scaling_laws_estimation.html",
    "href": "posts/paper_summaries/scaling_laws_estimation.html",
    "title": "A Hitchhiker’s Guide to Scaling Law Estimation",
    "section": "",
    "text": "arXiv\nScaling laws have been discussed a lot in the past few years. The OG paper on scaling laws is still one of the best, but this latest paper from IBM and MIT provides a fresh perspective. Here is a quick summary in case you are interested:\n\nWhy should one care about scaling laws?\n\nDesign choices like architectural modification, data changes, etc., are crucial, but every option is expensive to evaluate at scale.\nWhat if we use smaller models to assess the design choices? Will the findings be upheld when using a larger model?\nA scaling law extrapolates the performance of a target model from the performance of a set of models with fewer parameters or smaller training sets. A high-quality scaling law accurately predicts the test performance of the target model.\n\n\n\n\nDefining a scaling law\n\nA scaling law estimates the loss of a costly model by training cheaper ones that share a pretraining procedure and differ by some hyperparameters, typically model size (num_params) and number of tokens seen during training (num_toks).\nThe authors define a scaled model family f as a set of models, with each f ∈ F differing only in size num_params(f ) and number of tokens num_toks(f ).\nThey divide this into subsets. First, the maximal parameter family max params (F) containing models in F with the largest number of parameters. This family will generally include the target model(s) whose behavior we wish to predict t ∈ Ftarget. Second, the q-maximal token family max num_toks (F, q) contains all models in f trained on at least a q-sized fraction of the training set.\nA scaling law L(f | F ) estimates the performance of a new model f given a model family F and is defined as shown below:\n\n  \nE is a baseline capturing the general performance of the scaled family. A and α describe the scaling effect of num_params, while B and β describe the scaling effect of num_toks. These parameters are estimated by first collecting a set of training models Ftrain, then minimizing the reconstruction error as shown above. L(f ) denotes the empirical negative log-likelihood of some held-out data under the model.\n\n\nHow Well Can I Expect a Scaling Law to Predict?\n\nTo estimate this, we need to know what is considered a meaningful change when comparing two models (of the same family). The authors found that any design change that yields less than a 4% change in performance has not been considered meaningful in the literature.\nAlso, the variance across the same model with restarts reaches a 3.5% difference. Thus, a 4% difference bounds the best goodness of fit we should expect or require of scaling laws.\nThe Absolute Relative Error (ARE) is bounded by 4% and can reach 20% with certain design choices.\n\n\n\n\nWhen I Train a New Model, Do I Even Need a New Scaling Law?\n\nDifferent model families exhibit different scaling behavior.\nEvery design choice like the choices of architecture, training procedure, or the dataset, affects the form of scaling laws. Hence, the behavior of a new model family may require a new scaling law.\nCan’t we borrow the scaling behaviors between two different model families even if it results in poor or biased estimates? To that end, the authors set the num_params scaling parameters (A, α) to fixed values reported in the paper titled Scaling data-constrained language models and estimate the remaining parameters for individual model families.\nThe authors found that predictions generalize, and a constant num_params scaling factor is enough for most models (except the encoder-decoder T5-Pile). However, error rates are bigger than in the source family, and predictions for larger models are worse.\n\n  \n\n\nCan I Train the Target Model a Bit Instead of Many Small Models?\n\nA reasonable assumption to make at this point is that training the target model for a bit can be much better than wasting time on training many small models.\nA good thing about the above assumption is that extrapolation is only required for num_tokens as the number of parameters is fixed now.\nAs of now, reliable estimates with this approach require up to 30% of the full training run, but this is a good option for future research. \n\n\n\nAre Even Simpler Baselines Enough?\n\nLet us take another example. Suppose we have two models belonging to the same small model family. One model has the best performance, and the other model has seen more data. Are these baselines enough to estimate the performance of the target model without fitting a scaling law?\nThe model with the best performance is expected to be closer to the performance of the target model. The authors found that the baselines suffer more than 15% error, mostly above 10%, rarely get below 5%, and 18% ARE is observed on average across all scaled families.\n\n  \n\n\nWhich is good for estimates: Intermediate checkpoints or the final checkpoint?\n\nA common methodology is to train different models with different numbers of tokens. Most people never use intermediate checkpoints as the assumption is the learning rate schedule renders losses from intermediate checkpoints uninformative.\nThe authors find that almost all, except for the first few checkpoints, help scale laws. Specifically, models trained on less than 10B tokens are noisy and should be excluded from scaling laws. These models or checkpoints can introduce significant ARE in estimating the performance of the target model.\n\n  \n\n\nHow Big a Model Should I Train?\n\nIt is implicit that bigger models where the num_params are closer to the target model are more effective for reliable scaling laws.\nHowever, the authors found that the effect is neither strong nor monotonic and varies between model families. For example, fitting on all F results in the lowest ARE in most cases. However, in the case of GPT, Gopher, and OPT, predicting with the smallest four models is enough to achieve less than 10% error. OTOH, the smallest models in Pythia are not predictive enough.\nThe authors suggest diversifying and training each model on differing hyperparameters (seed, num_params, num_tokens) and maximizing the information gained.\n\n  \n\n\nHow Many Models Are Required for Reliable Predictions?\n\nLarger models are better, but a group of smaller models is equally good.\nIncreasing the number of models helps decrease ARE. Overall, five models are a safe bet, and more would improve the robustness of the result. And these models can be small. \n\n\n\nWhat Parameters Do I Need to Estimate?\n\nThe authors computed the PCA of five learned parameters and found that three parameters explain 99.49% of the variance.\nThough the above finding holds for a large number of model families, exceptions exist, especially in encoder-decoder models like T5-Pile.\nOverall, scaling laws might have fewer degrees of freedom than described in the literature. num_params and num_tokens remain the most important ones. The authors left out the learning rate schedule for future research."
  },
  {
    "objectID": "posts/paper_summaries/omniparser.html",
    "href": "posts/paper_summaries/omniparser.html",
    "title": "OmniParser for Pure Vision Based GUI Agent",
    "section": "",
    "text": "arXiv\nWith the Ferret-v2 paper, Apple demonstrated the power of building multimodal models for mobile devices. Now, Microsoft has done the same, but for the desktop UI. Presenting Omniparser, the latest advancement from Microsoft for vision-based multimodal workflows. Here is a summary in case you are interested:\n\nWhy does UI understanding matter?\nSince the last few months, there has been a lot of buzz about agents and system 2. A common theme across these explorations is that we want to automate many complex yet mundane tasks with the current generation of LLMs and MLLMs.\nFor example, an automatic trip planner is an excellent idea. You provide the trip details e.g. starting-ending dates, places you want to explore, etc. The system takes this information as input and completes the whole task of planning the trip, including but not limited to booking flight tickets, hotels, etc.\nSome of the sub-tasks in the above example can be completed using DOM alone, but things like captcha completion require visual understanding.\n\n\nHypothesis\nMultimodal models like GPT-4V, when asked to predict the xy coordinates of the interactable regions on the screen directly from UI (e.g. screenshots of a web page) perform poorly because of their inability to correctly identify the semantic information associated with the icons and other elements on the screen.\n\n\nMethod\nTo elevate the above problem, the authors suggest to break down the task into two steps:\n\nUnderstand the content present on screen at the current step to figure out the interactable regions and their corresponding functionality.\nBased on the above information, predict the next action required to complete the task.\n\nTo achieve the above workflow, OmniParser combines the outputs from three different models:\n\nInteractable icon detection model.\nIcon description model.\nOCR module\n\nThe output is a structured DOM-like representation of the UI, with a screenshot overlaid with bounding boxes for potential interactable elements.\n  \n\n\nInteractable Region Detection\n\nUses the Set-of-Marks approach to overlay bounding boxes of interactable icons on top of UI screenshots.\nFinetune a detection model to extract interactable icons/buttons.\nThe authors curated a dataset of interactable icon detection dataset, containing 67k unique screenshot images, each labeled with bounding boxes of interactable icons derived from the DOM tree.\nFinetune the YOLOv8 model on the interactable icon region detection dataset for 20 epoch with a batch size of 256, learning rate of 1e−3, and the Adam optimizer on 4 GPUs.\nAn OCR module is also used to extract bounding boxes of text. The authors merge the bounding boxes from the OCR detection module and icon detection module while removing the boxes that have high overlap (90% as a threshold). Every bounding box is assigned a unique ID.\n\n\n\nIncorporating Local Semantics of Functionality\n\nIncorporate local semantics directly into the prompt.\nGenerate a description of functionality using another model for each of the interactable regions detected by the interactable region detection model. For text boxes, detected text and the labels are injected into the prompt.\nUses BLIP-v2 as the icon description model for generating descriptions for the interactable regions.\nFor fine-tuning BLIP-v2, the authors curate a dataset of 7k icon-description pairs using GPT-4o. For the description, the authors ask GPT-4o whether the object presented in the parsed bounding box is an app icon. If GPT-4o decides the image is an icon, it outputs a sentence description of the icon about the potential functionality. And if not, GPT-4o will output ’this is not an icon’, while still including this in the dataset.\nBLIP-v2 is fine-tuned on this dataset for 1 epoch with a constant learning rate of 1e−5, no weight decay, and Adam optimizer.\n\n  \n\n\nResults\n  \n\n\nFailure Modes\n\nRepeated Icons/Texts: When the screenshot contains repeated elements, and it is required to click/select one of the elements, GPT-4V still fails many times. More granularity in the description of the repeated elements may alleviate the problem.\nCoarse Prediction of Bounding Boxes: The current OCR cannot differentiate between normal text and hyperlinks, hence in some cases elements of the predicted coordinatese of the clickable texts may not be correct.\nIcon Misinterpretation: Given that the icon-description model only sees a part (elements with a bbox) of the full screenshot, the lack of a global view can lead to a wrong description of the detected interactable icons. For example, three dots can be pretty confusing for the current description model"
  },
  {
    "objectID": "posts/paper_summaries/nGPT.html",
    "href": "posts/paper_summaries/nGPT.html",
    "title": "Normalized Transformer",
    "section": "",
    "text": "arXiv\nWe have been using transformers daily for a couple of years now, but the advancements on the architectural side have remained minimal. This paper from Nvidia proposes a normalized Transformer (nGPT), which performs representation learning on a hypersphere. Here is a summary for the same:\n\nToken embeddings and output logits\n\nBoth input and output embedding matrices are normalized after each training step.\nThe logits are bounded in the range [-1, 1] because of the normalization, which limits the confidence of the probability distribution generated by the softmax.\nTo adjust this during training, the authors introduce a trainable scaling parameter sz that scales the logits element-wise.\n\n\n\nLayers and Blocks\n\nA typical transformer block looks like this where L layers of transformations are applied to the hidden state h, consisting of alternating the self-attention and MLP blocks: \nIf we are on a hypersphere, we can use spherical linear interpolation to compute the geodesic distance between two points a and b on the hypersphere. We can approximate SLERP with approximate linear interpolation (LERP) as shown below: \nIf point a is our hidden state h, and point b represents the point suggested by the attention or MLP block, we can represent our equations for the transformer blocks like this: \nHere αA ≥0 and αM ≥0, with dimensionality dmodel, are learnable parameters applied to the normalized outputs of the attention and MLP blocks, respectively. The norm function normalizes any vector x to have a unit norm and, unlike RMSNorm or LayerNorm, does not introduce any element-wise scaling factors. This normalization can be viewed as the retraction step in Riemannian optimization, mapping the updated solution back to the manifold.\n\n\n\nSelf-Attention Blocks\n\nThe qkv values produced by the weight matrics Wq, Wk, and Wv in the original transformer are unconstrained, leading to unbounded values in q.\nIn nGPT the authors normalize Wq, Wk, Wv and Wo along their embedding dimension so that the computed dot products with h can be interpreted as cosine similarity between unit norm vectors bounded in [−1, 1]. Thus, all attention matrices can be viewed as collections of normalized embedding vectors to be compared.\nThough each element of q and k is now bounded, the norms of these two vectors can still vary. Also, the addition of positional embeddings can further distort q and k. To this end, the authors additionally normalize q and k by introducing a scaling factor sqk for each head, ensuring that the dot product of every query and key is under control.\nIn the original Transformer, the query-key dot product is scaled by 1/√dk before applying softmax to account for the expected variance of dk in the dot product of non-normalized query and key vectors. In the normalized Transformer, the expected variance of the dot product between the normalized query and key vectors is 1/dk. The softmax scaling factor should instead be √dk to restore a variance of 1\n\n\n\n\nMLP\n\nThe input hidden state h of the MLP block of a classical transformer is first normalized using RMSNorm(or LayerNorm) and then passed through two separate linear projections, producing two intermediate vectors u and v, which are then combined using SwiGLU.\nThe weight matrices Wu and Wv in nGPT are normalized. The authors introduce scaling factors su and sν to control their impact. They also rescale ν by √dmodel to optimize SiLU performance.\n\n\n\n\nSummary of all modifications\n\nRemove all normalization layers like RMSNorm or LayerNorm.\nAfter each training step, normalize all matrices (Einput, Eoutput, Wq, Wk, Wv, Wo, Wu, Wν, and Wo) along their embedding dimension.\nReplace the updates as follows where αA (and also αM) is treated with αA,init = 0.05 (in order of 1/n_layers) and αA,scale = 1/√dmodel.\nChange the softmax scaling factor in attention from 1/√dk to √dk.\nImplement the rescaling and normalization of q and k where sqk is treated with sqk,init = 1 and sqk,scale = 1/√dmodel.\nImplement the rescaling of the intermediate state of the MLP block where su (and also sν) is treated with su,init = 1 and su,scale = 1\nImplement the rescaling of logits using equation 3, where sz is treated with sz,init = 1, sz,scale = 1/√dmodel.\nRemove weight decay and learning rate warmup.\n\n\n\n\nHow fast is nGPT compared to GPT?\nA lot! Training the 0.5B and 1B nGPT models is approximately 4x, 10x, and 20x faster at context lengths of 1k, 4k, and 8k tokens, respectively.\n\n\n\nWhat about the params and hparams?\n  \n\n\nThere should be a catch somewhere, right?\nOf course, there is always a catch! Quoting directly from the paper: The time cost per step for nGPT is approximately 80% higher with a 4k context length, and 60% higher with an 8k context length. This overhead is not only due to nGPT having 6 normalization steps (2 of them are applied for q and k) per layer instead of 2 but also because nGPT’s normalizations are not yet fully optimized, unlike GPT, where normalization layers are fused with other operations.\n\n\n\nBest thing about nGPT?\nIn standard GPTs, perplexity tends to increase dramatically when tested on sequences longer than pre-training length. In contrast, nGPT maintains a stable perplexity range even at extrapolated lengths."
  },
  {
    "objectID": "posts/paper_summaries/depthpro.html",
    "href": "posts/paper_summaries/depthpro.html",
    "title": "Depth Pro",
    "section": "",
    "text": "arXiv\nZero-shot monocular depth estimation in real-time is one of the most challenging problems in vision. Applications like novel view synthesis from a single image require a strong depth estimation model. The paper DepthPro from Apple presents another model that can perform zero-shot depth estimation on high-resolution images with low latency.\n\nDesired characteristics of a depth estimation model\n\nIt should not be restricted to a single domain and should produce zero-shot depth estimation on any image.\nThe model should produce metric depth maps in a zero-shot regime to accurately reproduce object shapes, scene layouts, and absolute scales.\nThe model should produce metric depth maps with absolute scale even if no camera intrinsics (such as focal length) are provided with the image. It helps enable novel view synthesis from an arbitrary image.\nThe model can operate on high-resolution images and should be able to produce high-quality depth maps even for complex objects like hair, fur, etc.\nIt should run with extremely low latency to support applications like view synthesis on demand.\n\nThe DepthPro model ticks all the things listed above. \n\n\nNetwork\n\nUses pretrained Vision Transformers (ViTs) as encoders.\nThe key idea is to apply plain ViT encoders on patches extracted at multiple scales and fuse the patch predictions into a single high-resolution dense prediction in an end-to-end trainable mode.\nTwo encoders in total: Patch encoder and image encoder.\nThe patch encoder is applied on patches extracted at multiple scales, allowing the model to learn scale-invariant representations as weights are shared across scales.\nThe image encoder is applied to the whole image, down sampled from the original resolution to the resolution accepted by the base encoder (384 x 384 in this case). The image encoder anchors the patch predictions in a global context.\nThe network operates at a fixed resolution of 1536 × 1536 chosen as a multiple of the ViT’s 384×384.\nThe original image with a resolution of 1536 x 1536 is processed with two more down sampled resolutions of 784 x 784, and 384 x 384. For each resolution, the image is divided into patches of size 384x384. At each scale, the patches are fed into the patch encoder that produces a feature tensor at a resolution of 24 × 24 per input patch. Intermediate features for the finest scale are also extracted to capture fine-grained details.\nThe feature patches are then merged into maps and are fed into a DPT-like decoder module.\n\n  \n\n\nTraining Objectives\n\nThe network f predicts a canonical inverse depth image C = f (I) for an input image I. The dense metric depth map is then obtained as Dm = fpx / wC, where fpx is the focal length and w is the width.\nAll objectives use canonical inverse depth as it prioritizes areas close to the camera over farther areas or the whole scene. This supports visual quality in applications such as novel view synthesis.\nFor all metric datasets, the authors compute mean absolute error (MAE) per pixel i and discard pixels with an error in the top 20% per image for real-world (not for synthetic) datasets.\nFor all non-metric datasets (i.e., those without reliable camera intrinsics or inconsistent scale), the authors normalize predictions and ground truth via the mean absolute deviation from the median before applying a loss. They also define a multi-scale derivative loss over M scales. The ∇∗ indicate a spatial derivative operator ∗, such as Scharr or Laplace (L), and p is the error norm. The scales j are computed by blurring and down sampling the inverse depth maps by a factor of 2 per scale.\n\n  \nI did not find this in the code, but I have asked the authors to give some implementation details. Let us see how it goes. \n\n\nTraining Curriculum\nThe authors note down three important observations before launching a training run: * Training on a large mix of real-world and synthetic datasets improves generalization. * Real-world datasets are messy, and many times the labels are noisy (missing areas, mismatched depth, or false measurements on object boundaries.) * Predictions get sharper throughout the training.\nBased on these observations, the authors design a two-stage training curriculum. In the first stage, they aim to learn robust features that allow the network to generalize across domains, and train the model on a mix of all labeled training sets. They use MAE for the metric datasets and the normalized MAE for non-metric datasets. They also apply scale-and-shift-invariant loss on gradients, but only to synthetic datasets.\nThe second stage of training is designed to sharpen boundaries and reveal fine details in the predicted depth maps. Given that the synthetic dataset provides high-quality pixel-accurate ground truth, the authors use it to minimize the effect of inaccurate ground truth. MAE is supplemented with Mean Absolute Gradient Error (MAGE), Mean Absolute Laplace Error (MALE), and the Mean Squared Gradient Error (MSGE).\n\n\n\nEvaluation metrics for sharp boundaries.\n\nCommon benchmarks for monocular depth prediction rarely take boundary sharpness into account. This may be attributed in part to the lack of diverse and realistic datasets with precise pixel-accurate ground-truth depth.\nThe authors aim to leverage existing high-quality annotations for matting, saliency, or segmentation as ground truth for depth boundaries.\nThey treat annotations for these tasks as binary maps, which define a foreground/background relationship between an object and its environment. To ensure that the relationship holds, they only consider pixels around edges in the binary map.\nThe hypothesis is that if the depth of pixel i and pixel j differs by more than t%, it suggests the presence of an occluding contour between those pixels. Occluding contour c~d(i, j)~, and overall precision and recall for the neighboring pixels are then calculated as shown below:\n\n  \n\nThe authors report the weighted F1 score with thresholds that range linearly from tmin = 5 to tmax = 25, with stronger weights towards high threshold values.\nThe beauty of this method is that it doesn’t require any manual edge annotations. It only needs pixel-wise ground truth that can be easily obtained for synthetic datasets.\nThe authors also apply non-maximum suppression to values of cˆd within the valid bounds of cˆd(i, j) connected components.\nOccluding contours from binary maps are similarly obtained \n\n\n\nFocal Length Estimation\n\nTrained the network to predict focal length to handle cases where the EXIF metadata is either missing or is noisy for the input image.\nThe focal estimation head is a small convolutional network that ingests frozen features from the depth estimation network and task-specific features from a separate ViT image encoder to predict the horizontal angular field of view.\nThis head is trained after the training for depth prediction is complete. \n\n\n\nResults\n  \n\n\nLimitations\nThe model is limited in dealing with translucent surfaces and volumetric scattering, where the definition of single-pixel depth is ill-posed and ambiguous."
  },
  {
    "objectID": "posts/paper_summaries/model_merging_at_scale.html",
    "href": "posts/paper_summaries/model_merging_at_scale.html",
    "title": "What Matters for Model Merging at Scale?",
    "section": "",
    "text": "arXiv annotated_paper\nYesterday, I read this banger paper titled: What Matters For Model Merging At Scale?. Though I recommend reading the full paper, I am including a summary here in case you are interested in the main points. I will also provide the link to an annotated version of this paper.\n  \n\nIntroduction\nModel merging is not a new concept. It has been tried and tested enough to create a better or more powerful model by combining two or more (expert) models. Model merging has several advantages:\n\nReduced storage and serving costs\nImproved generalization to new tasks due to combined capabilities.\nDecentralized and modular model development.\n\nOne potential gap in this area is the lack of a comprehensive study to evaluate its effectiveness as we scale the model size. Most people are either merging models at a small scale (7B-13B models) or merging a limited number of expert models. This paper provides insights into the scalability of model merging.\n\n\nProblem Statement\n\nFocuses on model merging with large models\nN expert tasks and a base model\nExpert for each of these N tasks obtained by fully fine-tuning the base model on a specific expert task.\nFour merging methods, including Averaging, Task Arithmetic, TIES, and DARE.\n\n\n\nExperimental Design for Large-Scale Evaluation of Model Merging\n\nData\n\nData settings from the T0 mixture containing 8 held-in and 4 held-out task categories.\nThe 8 held-in task categories (with a total of 16 datasets) include Multiple-choice QA, Extractive Qa, Closed-Book QA, Sentiment Analysis, Topic Classification, Structure-to-text, Summarization, and Paraphrase Identification. The 4 held-out task categories are Sentence Completion, Natural Language Inference, Co-reference Resolution, and Word Sense Disambiguation.\n\n\n\n\nModels\n\nPaLM-2 with sizes 1B, 8B, 24B, and 64B as the base models. For all these models, the authors also build an instruction-tuned version of PaLM-2-IT.\nFor each of the two base model types(non-IT Vs IT) and 4 model sizes, they perform full fine-tuning on the 8 held-in task categories resulting in 64 specialized expert models.\nThe authors create a large merging experiment grid with the two base models (PaLM-2 and PaLM-2-IT), four model sizes (1B, 8B, 24B, 64B), four Merging methods (Averaging, Task Arithmetic, Dare-TIES, and TIES), the number of constituent models (2, 4, 6, 8), and 3 seeds to randomly select the constituent tasks for the experiment resulting in a total of 384 merging experiments.\n\n\n\n\n\nEvaluation\n\nPerformance evaluated on both held-in and held-out tasks\n∼9000 model evaluations across all the experiments.\n\n\n\n\nMetrics\n\nFor held-in tasks, the merged model performance is normalized against the corresponding task expert model’s performance.\nFor held-out tasks, normalization is performed relative to the base model’s performance.\n\n\n\n\nExperimental Results and Insights\n\nInstruction-Tuned Models Facilitate Easier Merging\n\nMerging experiments done with fully fine-tuned experts from PaLM-2 and PaLM-2-IT\nHeld-in performance is measured over three trials to minimize the impact of selected expert models and their data distributions.\nPaLM-2-IT models consistently outperform PaLM-2 base models for all merging methods. The authors think large-scale instruction tuning further disentangles model weights, facilitating effective model merging and improving the base model zero-shot performance.\n\n\n\n\nModel Merging Becomes Easier With Bigger Models\n\nThe authors noticed that merged models outperform their corresponding base models in zero-shot generalization to held-out tasks irrespective of the model size, merging method, or number of constituent models.\nFor weak base models (PaLM-2), increasing model size significantly improved the merged model performance over the base model. Strong base models (PaLM-2-IT) show a different trend, and the zero-shot generalization improves monotonically with more expert models.\n\n\n\n\nBigger Model Sizes Can Merge More Experts\n\nFor weak base models (PaLM-2) that are of small size (1B-8B), merging more models leads to a significant drop in performance, whereas for strong base models (PaLM-2), the drop is negligible.\nThe above trend doesn’t hold for bigger model sizes (64B). Merging more experts for a weak base model (PaLM-2 64B) leads to significant improvements in performance, whereas for strong base models(PaLM-2-IT), it leads to better generalization.   \n\nMerging Methods Become Similar at Scale\n\nAt scale, all merging methods for strong base models exhibit very similar performance, suggesting that we can simply use Averaging strategy to get the optimal performance at scale.   \n\n\n\n\nResults"
  },
  {
    "objectID": "posts/paper_summaries/cut_your_losses.html",
    "href": "posts/paper_summaries/cut_your_losses.html",
    "title": "Cut Your Losses in Large-Vocabulary Language Models",
    "section": "",
    "text": "arXiv\nWe all want to save GPU memory and increase its utilization, right? This latest paper from Apple is exactly what we all need (apart from attention, of course!).\nAs we scale models, the vocabulary for these models will also grow (and we want to be in an ideal scenario). One side effect of this is that during the training, the memory footprint of the last layer/op responsible for cross-entropy calculation grows disproportionately. In the case of small models, the memory consumed in the final layer can be an order of magnitude higher than the memory consumption in the rest of the LLM combined. The authors propose Cut Cross-Entropy (CCE), a method that computes the cross-entropy loss without materializing the logits for all tokens into global memory.\nA natural question to ask is that someone should have noticed this before, and it can not be the first one. Well, the answer is yes. Let me put it this way: Optimizing the quadratic cost of attention has been the main focus. We made a couple of improvements in that area, and now we are shifting to this. Though this is not the first time someone has implemented a similar idea (I will provide links to a few references later in this thread), I believe this is the first feature-complete implementation of this concept.\n\nPreliminaries\nBefore discussing the idea presented in this paper, take a step back and look at some notations described below as a refresher.\n\nThe LLM parameterizes an autoregressive distribution over all possible tokens xi ∈ V given the preceding N−1 tokens. V is our vocabulary.\nWe can split the LLM into a “feature backbone” f: x1 … xi−1 → RD and a “classifier” C ∈ RD×|V|\nThe softmaxk(v) produces the probability over all vocabulary entries from the unnormalized log probabilities (logits).\nAt training time, the LLM maximizes the log-likelihood of the next token.\n\n\n\nCut-Cross Entropy\nCross-entropy loss can be written as a combination of two terms, as shown below. The first term is a combination of an indexing operation and matrix multiplication. The second term is a joint log-sum-exp and matrix multiplication operation.\n  \n\n\nMemory Efficient Indexed Matrix Multiplication\nA naive computation of indexed matrix multiplication expressed in the first term above involves either explicit computation of the logits CTE with an O(N|V|) memory cost or indexing into the classifier Cx = [Cx1…..CxN] with an O(ND) memory cost.\nTo achieve better efficiency, the authors fuse the classifier indexing Cx with the consecutive dot product between columns Cxi and Ei in a single CUDA/Triton kernel.\n\nThe kernel retrieves the value xi, the xi-th column from C, and the ith column from E and stores them in SRAM.\nPerform a dot product between the retrieved values and write the results to the global memory.\n\nAll the operations are performed in SRAM, and no GPU memory is allocated at any point. Also, the computations are performed block-wise for better GPU efficiency. The backward pass can be merged with the backward pass of the log-sum-exp operation, which will be discussed shortly.\n  \n\n\nMemory Efficient Linear log-sum-exp\n\nForward pass\n\nUses an old trick of blocks and parallelization.\nDivide the output O = CTE ∈ R|V|×N into a set of blocks of size MB × NB.\nRetrieve the corresponding parts En of E with size (D × NB) and blocks Cm of C with size (D × MB) using independent CUDA blocks. Perform the inner product Onm = CTmEn along the dimension D.\nLoop over smaller size (DB X NB) and (DB X MB) blocks and accumulate Onm = Σd(CTmdEnd) in SRAM.\nEach CUDA block then writes Onm back into global memory.\nThe same blocking and parallelization strategy is applied to produce log-sum-exp(CTE). Each block first computes a matrix multiplication, then the log-sum-exp (or LSE for short) along the vocabulary dimension m for its block, and finally updates log-sum-exp with its result.\nThe authors use a spin-lock on an atomic operation in global memory to synchronize the updates by different CUDA blocks for simplicity.\n\nBackward pass\n\n\nNeeds two gradient updates. The gradient is defined as shown above.\nTo calculate CTE, we can take the same approach used in the forward pass and can compute this in SRAM.\nSince softmax(CTE) = exp(CTE − LSE), we do not need to compute the normalization constant of the softmax, allowing us to reuse the global synchronization of the forward pass and compute the softmax efficiently in parallel.\nThe authors implement the second matrix multiplication in the GPU memory as a blockwise implementation would require storing or synchronizing S.\nThe authors use two techniques to improve the memory access pattern in this algo: gradient filtering and vocabulary sorting.\nGradient filtering is necessary because when items are in bf16, any value below ε = 2e−12 will likely be ignored due to truncation in the summation or rounding in the normalization, directly affecting the softmax matrix. For any column in that matrix, at most 1/ε = 4096 entries have non-trivial values and contribute to the gradient computation. All other values are either rounded to zero or truncated. This sparsity increases with the size of the vocab. The authors exploit this sparsity and skip gradient computation for any block whose corresponding softmax matrix Snm has only negligible elements. A threshold of ε = 2e−12 is chosen as the smallest value for no truncation.\nEfficient gradient filtering depends directly on the block-level sparsity of the softmax matrix. Ideally, blocks should be empty (hence skipped) or populated entirely but not partially. The authors group the non-trivial gradients by ordering the tokens by their average logit. In the case of a forward pass, they achieve this using an atomic addition to compute the average logit per token. In the case of the backward pass, divide the vocabulary dimension |V| into blocks with similar average logit instead of arbitrarily.\n\n\n  \n\n\nRuntime And Memory\n\nModel: Gemma 2B\nVocab size: 256, 000\nHidden dim: 2,304\nBatch size: 8,192 tokens\n\nHere are the results with different methods"
  },
  {
    "objectID": "posts/paper_summaries/the_super_weight.html",
    "href": "posts/paper_summaries/the_super_weight.html",
    "title": "The Super Weight in Large Language Models",
    "section": "",
    "text": "arXiv\nA few papers in the past showcased that at a certain scale, a small set of hidden state features contains outliers with enormous magnitude. These outliers account for a small percentage of all activations but are crucial for preserving the compressed model quality.\nThis paper from Apple takes that finding to the extreme, claiming that a tiny subset, at most six scalers, is more important than the rest of the others. The authors call them super weights, and pruning them destroys model quality.\n  \n\nSuper weights create super activations\n\nThe paper Massive Activations in Large Language Models showcased that LLMs contain massive activations that persist across many layers at the same position irrespective of the input. These activations have a constant magnitude and are crucial to the model’s performance.\nThe authors observed another interesting thing, i.e., the activation channel of these massive activations aligns with the channel of the super weights. To validate if both are related, the authors prune the super weight and check its effect on the magnitude of these activations.\nThey found that pruning the super weight reduces the magnitudes of these activations drastically. Hence, these activations are created by the super weight. They term these massive activations as super activations.\n\n\n\nIdentifying super weight by activation spikes\n\nGiven the findings in the above section, the authors hypothesize that super weight can be located by detecting the spikes in the input-output distributions of down_proj across the layers. This detection only requires a single input prompt rather than a set of validation data or use-case examples.\nSuppose X is the input with dimensions (L x H), and the weight matrix of the down_proj is W with dimensions (D x H). We can then compute output Y as Y= X WT. If Yij is a super activation, and Xik and Wik are outliers, we can say that Yij ≈ Xik Wjk. We can then identify the row from the super weight from the channel index given by the input distribution of the activations across all layers. Similarly, we can identify the column of the super weight from the channel index of the corresponding layer in the output distribution of the activations.\nThe authors found that the Phi-3-mini-4k-instruct contains the maximum number of super weights, a total of six.\nThe authors also found that super weights in the instruct-tuned models are located at the exact coordinates as in the pre-trained models, suggesting that instruct fine-tuning does not change the position of super weights.\n\n\n\nWhat impacts the model’s quality more: Super weights or super activations?\n\nExperimented with three conditions:\n\nOriginal model\nPrune the super weight (setting it to zero)\nPrune the super weight but restore the super activation\n\nWhen the authors prune the super weight, it significantly impairs quality - reducing accuracy on zero-shot datasets and increasing perplexity by orders of magnitude.\nPruning the super weight but restoring the super activation recovers the quality partially but only up to a (low) extent. It suggests that the super weight and super activation need special handling to preserve quality.\n\n  \n\n\nEffect of super weight on the output token probability distribution\n\nSo, what happens to the output of an LLM when you remove the super weight? The probability of generating the stopwords amplifies.\nIf super weights are so important, why don’t we amplify them more? The authors multiply the super weights by a scaling factor ranging from 0.0 to 3.0 to record the impact on the model performance. They noticed that even though the improvements are minuscule, there exists a consistent trend of improvement to a certain scaling factor.\n\n  \n\n\nWeight and Activation Quantization\nGiven that we know the importance of the super activation and super weights, how should we quantize these?\n\nActivation Quantization\n\nSimulate W8A8 quantization with FP16 arithmetic and focus solely on addressing one critical activation outlier. Experiments done using round-to-nearest quantization but with some modifications.\nReplace the super activation with the median value (REPLACE), quantize (Q), and dequantize (Q−1) activations. Then restore the super activation in fp16 (RESTORE)\n\nWeight Quantization\n\nProposed a simple method to improve INT4 quantization with large block sizes.\nIdentify the super weight as described in the previous section. Clip the outlier weight, including the super weight, to improve the inlier fit.\nQuantize (Q) and dequantize (Q−1) the clipped weights. Restore the half-precision super weight after dequantization (RESTORE)\n\n\n  \n\n\nWhich models did the authors user in their experiments?\nHere is table showcasing all the models used in the experiments. All these models are coming from HF.   \n\n\nHow does the quantization scheme proposed by the authors perform?\nThere have been other quantization schemes that were proposed for these outlier weights and the massive activations observed in these models. How does the new scheme fare with those?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About me",
    "section": "",
    "text": "Hello, I am Aakash Kumar Nain.\nI am a Machine Learning Engineer with 8 years of experience in the industry. Currently, I am working as a Senior ML Engineer at Merlyn Mind. I am also a Google Developers Expert in Machine Learning and JAX. I actively contribute to Keras, TensorFlow, and the JAX ecosystem. I am one of the core collaborators for the Keras core project, and I am also one of the maintainers of the TensorFlow-addons package.\nIn terms of work, my knowledge Machine Learning and Deep Learning spans in T-shape. I have worked in multiple domains, but Computer Vision with Deep Learning is my favorite field. I love simplifying Data Science and Machine Learning concepts, especially the maths behind them. Love and support OSS because, without OSS, we wouldn’t have witnessed so many breakthroughs in Machine Learning and Deep Learning. Python is my favorite language to work with, and I absolutely love coding in it. Always up for a discussion involving anything related to Machine Learning, Deep Learning, MLOps, and API design. Simply put: Python in breakfast, Machine Learning in lunch, and Deep Learning in dinner!\nFollow me on Twitter to get the latest updates related to Machine Learning."
  },
  {
    "objectID": "index.html#popular-projects",
    "href": "index.html#popular-projects",
    "title": "About me",
    "section": "Popular Projects",
    "text": "Popular Projects\nAnnotated Research Papers   \nMost researchers and Machine Learning engineers want to remain up-to-date with the latest research work, but the field of machine learning moves crazy fast. The number of papers uploaded on arXiv is witnessing exponential growth. Reading all these published papers is an impossible task. To help with this, I maintain this repository where I annotate and upload the papers that I find exceptionally good. Annotations make it easier to understand and grasp the main concepts explained in the paper. \nTF-JAX Tutorials   \nIt is a series of tutorials built to teach the fundamental concepts and the underlying working of two famous libraries: TensorFlow and JAX. These tutorials aren’t typical documentation-type tutorials. It doesn’t matter whether you are a beginner or an advanced user, these tutorials will give you a fresh perspective on building things using TensorFlow or JAX. \nDiffusion Models   \nDiffusion models are a class of likelihood-based generative models that recently have been used to produce very high-quality images compared to other existing generative models like GANs. It’s hard to find quality resources to learn about diffusion models. The mathematics behind the diffusion models is also a bit harder to understand. The material presented in this repo is enough to make you understand the working of diffusion models and the underlying math. \nMistral-7B in JAX   \nThis is a direct port of Mitral-7B model in JAX and Equinox. This port comes with two implementations: The first one shows how to port models from PyTorch to Equinox and JAX. This is a 1:1 mapping, and not fully optimized. The second implementation is fully optimized for JAX, targeted for more advanced users."
  },
  {
    "objectID": "index.html#keras-core-contributions",
    "href": "index.html#keras-core-contributions",
    "title": "About me",
    "section": "Keras Core Contributions",
    "text": "Keras Core Contributions\n\nJAX NN ops\nMerging layers\nMetrics\nLoss functions\nApplications\nData adapters\nCode examples"
  },
  {
    "objectID": "index.html#keras-contributions",
    "href": "index.html#keras-contributions",
    "title": "About me",
    "section": "Keras Contributions",
    "text": "Keras Contributions\n\nDenoising Diffusion Models\nOCR model for reading captchas\nHandwriting recognition\nImage captioning\nWGAN-GP\nCycleGAN\nModel interpretability with Integrated Gradients"
  },
  {
    "objectID": "index.html#equinox-contributions",
    "href": "index.html#equinox-contributions",
    "title": "About me",
    "section": "Equinox Contributions",
    "text": "Equinox Contributions\n\nMistral-7B implementation\nBug fix in RMSNorm Layer\nAMP in Normalization Layers"
  },
  {
    "objectID": "index.html#tensorflow-contributions",
    "href": "index.html#tensorflow-contributions",
    "title": "About me",
    "section": "TensorFlow Contributions",
    "text": "TensorFlow Contributions\n\nKappa\nGelu activation\nFocal loss\nThresholded Linear Unit"
  }
]