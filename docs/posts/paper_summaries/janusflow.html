<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Aakash Kumar Nain (@A_K_Nain)">
<meta name="dcterms.date" content="2024-11-25">

<title>Aakash Nain - JanusFlow</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Aakash Nain</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/AakashKumarNain" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/A_K_Nain" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/aakash-kumar-nain" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../archive.html" rel="" target="">
 <span class="menu-text">Archive</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="https://github.com/AakashKumarNain/annotated_research_papers" rel="" target="">
 <span class="dropdown-text">Annotated Research Papers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://www.kaggle.com/aakashnain/code?userId=536977" rel="" target="">
 <span class="dropdown-text">Kaggle Notebooks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/AakashKumarNain/TF_JAX_tutorials" rel="" target="">
 <span class="dropdown-text">TF-JAX Tutorials</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/AakashKumarNain/diffusion_models" rel="" target="">
 <span class="dropdown-text">Diffusion Models Tutorials</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#multimodal-understanding" id="toc-multimodal-understanding" class="nav-link active" data-scroll-target="#multimodal-understanding">MultiModal Understanding</a></li>
  <li><a href="#image-generation" id="toc-image-generation" class="nav-link" data-scroll-target="#image-generation">Image Generation</a></li>
  <li><a href="#training-schemes" id="toc-training-schemes" class="nav-link" data-scroll-target="#training-schemes">Training Schemes</a></li>
  <li><a href="#training-objective" id="toc-training-objective" class="nav-link" data-scroll-target="#training-objective">Training Objective</a></li>
  <li><a href="#hparams-used-in-training" id="toc-hparams-used-in-training" class="nav-link" data-scroll-target="#hparams-used-in-training">Hparams used in training</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">JanusFlow</h1>
<p class="subtitle lead">Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation</p>
  <div class="quarto-categories">
    <div class="quarto-category">papers</div>
    <div class="quarto-category">summary</div>
    <div class="quarto-category">research</div>
    <div class="quarto-category">MLLMs</div>
    <div class="quarto-category">generation</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Aakash Kumar Nain (<a href="https://x.com/A_K_Nain"><span class="citation" data-cites="A_K_Nain">@A_K_Nain</span></a>) </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 25, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p><a href="https://arxiv.org/abs/2411.07975">arXiv</a><br> <a href="https://github.com/AakashKumarNain/annotated_research_papers/blob/master/MLLMs/janusflow.pdf">annotated_paper</a></p>
<p>We all have been impressed by the quality of models produced by Deepseek. I thought Qwen was good, but the main highlight is JanusFlow. Apart from the MM1 paper from Apple, I believe JanusFlow is one of the best papers on modern MLLMs. It combines both image understanding and image generation in a single model. The JanusFlow paper is crisp and to the point.</p>
<section id="multimodal-understanding" class="level1">
<h1>MultiModal Understanding</h1>
<p>In multimodal understanding tasks, the LLM processes an input sequence consisting of interleaved text and image data.</p>
<ul>
<li>Text is tokenized into discrete tokens. Each token is transformed into an embedding of dimension ğ·<sub>ğ‘’ğ‘šğ‘</sub>.</li>
<li>An image encoder f_enc encodes images into feature maps of shape ğ»<sub>ğ‘–ğ‘š</sub> Ã— ğ‘Š<sub>ğ‘–ğ‘š</sub> Ã— ğ·<sub>ğ‘’ğ‘›ğ‘</sub>. The authors use a pre-trained SigLIP-Large-Patch/16 model as the image encoder (f<sub>enc</sub>).</li>
<li>The image feature map is flattened and projected through a linear transformation layer into a sequence of embeddings with shape (ğ»<sub>ğ‘–ğ‘š</sub> ğ‘Š<sub>ğ‘–ğ‘š</sub>) Ã— ğ·<sub>ğ‘’ğ‘šğ‘</sub>.</li>
<li>The text and image embeddings are concatenated to form the input sequence to the LLM. Special tokens |BOI| and |EOI| are inserted before and after the image tokens to help the model locate the image embeddings in the sequence.</li>
<li>Based on the above sequence of input embeddings, the LLM predicts the next token autoregressively.</li>
</ul>
<p><br> <img src="../paper_screenshots/janusflow/1.png" class="img-fluid"> <br></p>
</section>
<section id="image-generation" class="level1">
<h1>Image Generation</h1>
<p>The same LLM used for multimodal understanding employs rectified flow for image generation.</p>
<ul>
<li>Generation occurs in the latent space using a pre-trained SDXL-VAE.</li>
<li>The LLM takes a text sequence ğ‘¥<sub>ğ‘ğ‘œğ‘›</sub> as a condition and generates a corresponding image using rectified flow.</li>
<li>Start by sampling Gaussian noise z0 of shape ğ»<sub>ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘¡</sub> Ã— ğ‘Š<sub>ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘¡</sub> Ã— ğ·<sub>ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘¡</sub> in the latent space.</li>
<li>A generation encoder <em>g<sub>enc</sub></em> transforms the above image(noise in the beginning) into a sequence of embeddings (H<sub>ğ‘”ğ‘’ğ‘›</sub> W<sub>ğ‘”ğ‘’ğ‘›</sub>) Ã— ğ·<sub>ğ‘’ğ‘šğ‘</sub>. The authors use ConvNext blocks initialized from scratch in the generation encoder here.</li>
<li>The above sequence is then concatenated with a time embedding representing the current time step ğ‘¡ (ğ‘¡ = 0 at the beginning), resulting in a sequence of length (ğ»<sub>ğ‘”ğ‘’ğ‘›</sub> ğ‘Š<sub>ğ‘”ğ‘’ğ‘›</sub> + 1). Special token |BOI| is prepended to indicate the start of image generation in the sequence.</li>
<li>The LLM predicts the next token autoregressively. The output is then transformed to a velocity vector of shape ğ»<sub>ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘¡</sub> Ã— ğ‘Š<sub>ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘¡</sub> Ã— ğ·<sub>ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘¡</sub> by a generation decoder g<sub>dec</sub>. The authors use ConvNext blocks initialized from scratch in the generation decoder.</li>
<li>The transformed output state is then updated by a standard Euler solver: ğ‘§<sub>ğ‘¡+dğ‘¡</sub> = ğ‘§<sub>ğ‘¡</sub> + ğ‘£(ğ‘§<sub>ğ‘¡</sub>, ğ‘¡)dğ‘¡. The step size dt is defined by the user.</li>
<li>Use z<sub>dt</sub> as the input, replacing z<sub>0</sub> in the above steps iteratively till we obtain z<sub>1</sub>.</li>
<li>To improve the image generation quality, the authors use the good old classifier-free guidance (CFG). ğ‘¤ â©¾ 1 controls the magnitude of CFG, and increasing ğ‘¤ yields higher semantic alignment.</li>
</ul>
<p>Pay attention to the details here. Not only do the authors employ separate image encoders for understanding and generation, but they also use different kinds of models, SigLIP for understanding and ConvNext for generation.</p>
</section>
<section id="training-schemes" class="level1">
<h1>Training Schemes</h1>
<p>The authors adopt a three-stage sequential training:</p>
<ol type="1">
<li><strong>Adaptation:</strong> Randomly initialized components such as the linear layers, generation encoder, and generation decoder are trained in this stage.</li>
<li><strong>Unified pre-training:</strong> Once the adaptation phase is complete, the authors train the entire model except for the visual encoder. The training incorporates three data types; multimodal understanding, image generation, and text-only data. At the start, the data mixture contains more multimodal understanding data, and the authors then increase the ratio of image generation data gradually.</li>
<li><strong>Supervised Fine-tuning (SFT):</strong> At this stage, the SigLIP image encoder is also unfrozen. The model is fine-tuned using instruction-tuning data, which comprises dialogues, task-specific conversations, and high-quality text-conditioned image generation examples.</li>
</ol>
<p>This training scheme reminds me of the old transfer learning followed by the layer-wise fine-tuning paradigm widely used in the peak CNN era of 2014-18.</p>
<p><br> <img src="../paper_screenshots/janusflow/2.png" class="img-fluid"> <br></p>
<p>The first two stages use three types of data: multimodal understanding data, image generation data, and text-only data. The multimodal understanding data contains image-caption pairs, charts and tables, and task data (ShareGPT4V). The image generation data comes from different sources like LAION-Aesthetics, SAM, etc., and 2 million in-house samples. All samples go through a filtration process to ensure only high quality. For text-only data, the authors use the text corpus of DeepSeek-LLM. Similarly, the SFT uses three types of data: Multimodal instruction data, image generation data, and text-only data.</p>
</section>
<section id="training-objective" class="level1">
<h1>Training Objective</h1>
<p>Training JanusFlow involves two types of data, multimodal understanding data and image generation data. Both types of data contain two parts: â€œconditionâ€ and â€œresponseâ€. The data is formatted in pairs as ğ‘¥ = (ğ‘¥<sub>ğ‘ğ‘œğ‘›</sub>, ğ‘¥<sub>ğ‘Ÿğ‘’ğ‘ </sub>), where the superscript ğ‘ğ‘œğ‘› denotes â€œconditionâ€ and ğ‘Ÿğ‘’ğ‘  denotes â€œresponseâ€. The sequence length of x is denoted by L, while the sequence lengths of x<sub>con</sub> and x<sub>res</sub> are denoted by L<sub>con</sub> and L<sub>res</sub> respectively. ğœƒ denotes all the trainable parameters.</p>
<ul>
<li><strong>Autoregression Objective:</strong> For multimodal understanding tasks, ğ‘¥<sub>ğ‘Ÿğ‘’ğ‘ </sub> contains only text tokens, and the model is trained using the maximum likelihood principle where the loss is calculated over tokens in x<sub>res</sub>.</li>
<li><strong>Rectified Flow Objective:</strong> For image generation tasks, ğ‘¥<sub>ğ‘ğ‘œğ‘›</sub> consists of text tokens and ğ‘¥<sub>ğ‘Ÿğ‘’ğ‘ </sub> is the corresponding image, and the model is trained with the rectified flow objective. 10% of the text prompt is dropped during training to enable CFG.</li>
</ul>
<p><br> <img src="../paper_screenshots/janusflow/3.png" class="img-fluid"> <br></p>
<ul>
<li><strong>Representation Alignment Regularization:</strong> For generation tasks, features from the understanding encoder ğ‘“<sub>ğ‘’ğ‘›ğ‘</sub> are aligned with the intermediate features of the LLM as shown below. The function <code>sim(...)</code> computes the mean of element-wise cosine similarity between embeddings. This alignment loss helps the LLMâ€™s internal feature space (given noisy input ğ‘§<sub>ğ‘¡</sub>) align with the understanding encoderâ€™s semantic feature space, thereby improving generation quality when producing images from new random noise and text conditions during inference.</li>
</ul>
<p><br> <img src="../paper_screenshots/janusflow/4.png" class="img-fluid"> <br></p>
<p>All three objectives are applied across all training stages. Multimodal understanding tasks use L<sub>ğ´ğ‘…</sub>, while image generation tasks employ the combined loss L<sub>ğ‘…ğ¹</sub> + L<sub>ğ‘…ğ¸ğ‘ƒğ´</sub>.</p>
</section>
<section id="hparams-used-in-training" class="level1">
<h1>Hparams used in training</h1>
<p>Here is the list of all the hparams used in the training at different stages:</p>
<p><br> <img src="../paper_screenshots/janusflow/5.png" class="img-fluid"> <br></p>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<p>Here are the results for both image generation and multimodal understanding tasks. The one big complaint I have in these evaluations is the use of FID. I think it is high time to ditch the FID. It is meaningless most of the time.</p>
<p><br> <img src="../paper_screenshots/janusflow/6.png" class="img-fluid"> <br></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>