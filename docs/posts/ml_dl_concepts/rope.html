<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Aakash Kumar Nain (@A_K_Nain)">
<meta name="dcterms.date" content="2024-11-28">

<title>Aakash Nain - Rotary Position Embeddings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Aakash Nain</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/AakashKumarNain" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/A_K_Nain" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/aakash-kumar-nain" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../archive.html" rel="" target="">
 <span class="menu-text">Archive</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="https://github.com/AakashKumarNain/annotated_research_papers" rel="" target="">
 <span class="dropdown-text">Annotated Research Papers</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://www.kaggle.com/aakashnain/code?userId=536977" rel="" target="">
 <span class="dropdown-text">Kaggle Notebooks</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/AakashKumarNain/TF_JAX_tutorials" rel="" target="">
 <span class="dropdown-text">TF-JAX Tutorials</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/AakashKumarNain/diffusion_models" rel="" target="">
 <span class="dropdown-text">Diffusion Models Tutorials</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#why-bother-with-position-encoding" id="toc-why-bother-with-position-encoding" class="nav-link active" data-scroll-target="#why-bother-with-position-encoding">Why bother with position encoding?</a></li>
  <li><a href="#preliminary" id="toc-preliminary" class="nav-link" data-scroll-target="#preliminary">Preliminary</a></li>
  <li><a href="#desired-properties-of-a-position-encoding" id="toc-desired-properties-of-a-position-encoding" class="nav-link" data-scroll-target="#desired-properties-of-a-position-encoding">Desired properties of a position encoding</a></li>
  <li><a href="#sinusoidal-positional-encoding" id="toc-sinusoidal-positional-encoding" class="nav-link" data-scroll-target="#sinusoidal-positional-encoding">Sinusoidal Positional Encoding</a></li>
  <li><a href="#rotary-position-encoding-the-easy-way" id="toc-rotary-position-encoding-the-easy-way" class="nav-link" data-scroll-target="#rotary-position-encoding-the-easy-way">Rotary Position Encoding: The Easy Way</a></li>
  <li><a href="#rotary-position-encoding-the-mathematical-view" id="toc-rotary-position-encoding-the-mathematical-view" class="nav-link" data-scroll-target="#rotary-position-encoding-the-mathematical-view">Rotary Position Encoding: The Mathematical View</a></li>
  <li><a href="#beyond-2d" id="toc-beyond-2d" class="nav-link" data-scroll-target="#beyond-2d">Beyond 2D</a></li>
  <li><a href="#optimization" id="toc-optimization" class="nav-link" data-scroll-target="#optimization">Optimization</a></li>
  <li><a href="#what-else" id="toc-what-else" class="nav-link" data-scroll-target="#what-else">What else?</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Rotary Position Embeddings</h1>
<p class="subtitle lead">A figure among cyphers: Part-1</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Aakash Kumar Nain (<a href="https://x.com/A_K_Nain"><span class="citation" data-cites="A_K_Nain">@A_K_Nain</span></a>) </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 28, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="why-bother-with-position-encoding" class="level1">
<h1>Why bother with position encoding?</h1>
<p>Self-attention in transformer-based models is one of the most used operations in the history of deep learning. Though self-attention is extremely powerful, one of the weaknesses of self-attention is that it treats a sequence as a set of tokens. It is not position-aware, and it is permutation equivariant.</p>
<p>The order of words in a language matters; hence, we need a mechanism to insert word order information in our model. This is where position encoding kicks in.</p>
</section>
<section id="preliminary" class="level1">
<h1>Preliminary</h1>
<p>Let <span class="math inline">\(S_N=\{w_i\}^N_i=1\)</span> be a sequence of <span class="math inline">\(N\)</span> input tokens with <span class="math inline">\(w_i\)</span> being the ith token. Each token(word in this case) in the sequence is defined by a <span class="math inline">\(d\)</span>-dimensional vector embedding containing no position information. The self-attention layer first incorporates position information into the word embeddings and then transforms them into queries, keys, and value representations. We can define these transforms as shown below:</p>
<p><span class="math display">\[
\begin{align}
    &amp; q_m = f_q(x_m, m) \\
    &amp; k_n = f_k(x_n, n) \tag{1} \\
    &amp; v_n = f_v(x_n, n)
\end{align}
\]</span></p>
<p>where <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> represents the <span class="math inline">\(mth\)</span> and <span class="math inline">\(nth\)</span> positions respectively.</p>
</section>
<section id="desired-properties-of-a-position-encoding" class="level1">
<h1>Desired properties of a position encoding</h1>
<p>If we encode positions for the model, what would be the ideal design? The simplest thing we can do is initialize a learnable position embedding and let the model figure out the values during training. Not a bad idea for starters, but can we do better? What are the desirable properties of an encoding scheme if it is not learned implicitly? Here are a few of them:</p>
<ol type="1">
<li><strong>A unique value for every position in a sequence:</strong> The encoding scheme should assign a unique value for every position irrespective of the sequence length. <br><br></li>
<li><strong>Consistent relative distance between two positions:</strong> Irrespective of the length of a sequence, the relative distance between two positions should be consistent across sequences. For example, the relative distance between encodings of the 2nd position and 3rd position should be the same in sequences of different lengths. <br><br></li>
<li><strong>Long-term decay:</strong> An inductive bias about words is that words far distant from the current word carry less relevant information. That means our position encoding should follow a long-term decay effect for relatively distant positions. <br><br></li>
<li><strong>Extensible:</strong> What if we encounter a lengthier sequence at test time than the length of any sequence encountered during training? Ideally, we want our encoding scheme to be extensible, without much effort and breaking any other assumption. <br><br></li>
<li><strong>Deterministic:</strong> Determinism is mostly a nice-to-have property. It can help debug a few aspects if we encounter something unexpected.</li>
</ol>
<p>There are different encoding schemes, e.g., absolute position encoding, binary encoding, relative position encoding, rotary position encoding, etc. Here, we will discuss the two most widely used position encodings: sinusoidal position encoding and rotary position encoding. Assuming the reader is familiar with concepts like absolute and binary position encodings, we will skip that discussion here. We could have skipped sinusoidal position encoding, but it lays the foundation of rotary position encoding. Also, we will try to keep the notations in line with the <a href="https://arxiv.org/abs/2104.09864">Roformer</a> paper as much as possible. <br></p>
</section>
<section id="sinusoidal-positional-encoding" class="level1">
<h1>Sinusoidal Positional Encoding</h1>
<p>A typical choice for equation(1) is to formulate it this way: <span class="math display">\[
f_t(x_i, i) = W_t(x_i + p_i) \tag{2} \\
\]</span></p>
<p>where <span class="math inline">\(t \in (q, k, v)\)</span>, and <span class="math inline">\(p_i\)</span> is represents a <span class="math inline">\(d\)</span> dimensional vector depending of the position of token <span class="math inline">\(x_i\)</span>. It simply means that we encode the position and add it to the token embedding.</p>
<p>Sinusoidal encoding was proposed in the paper Attention is All You Need. They proposed to generate <span class="math inline">\(p_i\)</span> in the above equation using the sinusoidal function: <span class="math display">\[
\begin{cases}
p_{i,2t} &amp;= \sin(k/10000^{2t/d}) \\
p_{i,2t+1} &amp;= \cos(k/10000^{2t/d}) \\ \tag{3}
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(p_{i,2_t}\)</span> is the <span class="math inline">\(2t\)</span>h element of the d-dimensional vector <span class="math inline">\(p\)</span>. The wavelengths form a geometric progression from <span class="math inline">\(2π\)</span> to <span class="math inline">\(10000 · 2π\)</span>.</p>
<p>Many people do not know sinusoidal encoding was suggested so the model can learn to attend to relative positions. Yes, relative positions! This is mostly because people do not read papers with “attention”. Let us take an example with <span class="math inline">\(d=2\)</span> to prove it.</p>
<p><span class="math display">\[
\begin{align*}
    d = 2 \rightarrow i={[0, 1]}
\end{align*}
\]</span></p>
<p>Let us calculate the sinusoidal position encodings for <span class="math inline">\(position=pos\)</span> using equation 3:</p>
<p><span class="math display">\[
\begin{align*}
    PE_{pos,0} &amp;= \sin(pos/10000^{2*0/d}) = sin(pos) \\
    PE_{pos,1} &amp;= \cos(pos/10000^{2*0/d}) = cos(pos) \\
\end{align*}
\]</span></p>
<p>Now, let us calculate the sinusoidal position encodings for an offset $k $ i.e., <span class="math inline">\(position=pos+k\)</span>:</p>
<p><span class="math display">\[
\begin{align*}
    PE_{pos+k, 0} &amp;= \sin((pos+k)/10000^{2*0/d}) = \sin(pos+k) \\
    PE_{pos+k, 1} &amp;= \cos((pos+k)/10000^{2*0/d}) = \cos(pos+k) \\
\end{align*}
\]</span></p>
<p><br>Expanding <span class="math inline">\(sin(pos+k)\)</span> and <span class="math inline">\(cos(pos+k)\)</span>, we have:</p>
<p><span class="math display">\[
\begin{align*}
    \sin(pos+k) &amp;= \sin(pos)\sin(k) + \cos(pos)\cos(k) \\
                &amp;= PE_{pos, 0} \sin(k) + PE_{pos, 1} \cos(k) \tag{4} \\ \\
    \cos(pos+k) &amp;= \cos(pos)\cos(k) - \sin(pos)\sin(k) \\
                &amp;= PE_{pos, 1} \cos(k) - PE_{pos, 0} \sin(k) \tag{5} \\
\end{align*}
\]</span></p>
<p>We can combine equation (4) and equation (5) in a nice matrix notation:</p>
<p><span class="math display">\[
\begin{align*}
\begin{bmatrix}
    PE_{pos+k,0} \\
    PE_{pos+k,1}
\end{bmatrix} &amp;=
\underbrace{\begin{bmatrix}
    \cos(k)  &amp; \sin(k) \\
    -\sin(k) &amp; \cos(k)
\end{bmatrix}
}_{Rotation \ Matrix}
\begin{bmatrix}
    PE_{pos,0} \\
    PE_{pos,1}
\end{bmatrix}
\end{align*}
\]</span></p>
<p>That’s a transposed rotation matrix! So, our models have been learning to attend relative positions since 2017. If that is the case, what is wrong with sinusoidal position encoding?</p>
<p>Take a look at the visualization below. We have a two-dimensional vector <span class="math inline">\(x=[1, 1]\)</span>, and we add sinusoidal position encoding for different positions to this vector.</p>
<p><video src="./animations/rope/SinuSoidalEncoding.mp4" class="img-fluid" autoplay="true," loop="true" controls=""><a href="./animations/rope/SinuSoidalEncoding.mp4">Video</a></video></p>
<p>Though we assume that the model can attend to relative positions easily, the one thing that is a bit bothering is the stochasticity. Our pointer is moving almost randomly. The pattern here does not look good. But this doesn’t necessarily mean it is also bad for the model to learn, especially at scale. All we can say is that with so much stochasticity across different dimensions, the model will have a hard time learning that relative positions can be attended using a rotation matrix and that the model may start memorizing.</p>
<p>Another thing to note from equation (2) is that sinusoidal positional encodings are additive. This means indirectly adding the “relatedness” of two tokens to the positional encoding. A side effect of this is that two highly related tokens can get high attention scores irrespective of the distance between them. This is still okay for NLP-related problems but doesn’t hold for other domains like protein sequencing.</p>
<p>The authors of the <strong>Attention is All You Need</strong> paper also hypothesized that hard-coded sinusoidal positional encodings may help to extrapolate to longer sequences compared to the length of the sequences seen during training. The hypothesis does not hold in practice.</p>
</section>
<section id="rotary-position-encoding-the-easy-way" class="level1">
<h1>Rotary Position Encoding: The Easy Way</h1>
<p>From the above discussion, two things are clear:</p>
<ol type="1">
<li>Encoding relative positions is crucial for attention, but additive methods like sinusoidal encodings are not the best way to do it.</li>
<li>Leveraging a rotation matrix in some form is a good choice for encoding relative positions.</li>
</ol>
<p>RoPE leverages the above findings. It rotates an encoded vector by some angle θ based on its position in a sequence. Take a look at the visualization below. Here, we are using a 2D vector to demonstrate the rotation of the encoded vector for the token <em>Python</em> in a sequence. One interesting aspect is that the number of tokens occurring after the query token (<em>Python</em> in this case) does not affect the embedding. The amount of rotation is purely dependent on the token’s position in the sequence.</p>
<p>Is it any good? Let us take an example in 2D to demonstrate this. Suppose <em>Python</em> is the query token, <em>love</em> is the key token, and we want to compute attention scores between them in two different sequences as given below:</p>
<ul>
<li>Python is love</li>
<li>Coding in Python is love</li>
</ul>
<p>This is how the tokens will be rotated in each of the two sequences:</p>
<p><video src="./animations/rope/TextAnimation2.mp4" class="img-fluid" autoplay="true," loop="true" controls=""><a href="./animations/rope/TextAnimation2.mp4">Video</a></video></p>
<p>The relative distance between our query token <em>Python</em> and the key token <em>love</em> is 2θ. The inner product <span class="math inline">\(q^Tk\)</span> remains the same in both sequences. For a 2D case, if we generalize the rotation used in RoPE, we can rewrite our rotation matrix as follows:</p>
<p><span class="math display">\[
\begin{align*}
    R_\Theta^p =
    \begin{bmatrix}
    \cos(p\theta)  &amp; -\sin(p\theta) \\
    \sin(p\theta) &amp; \cos(p\theta) \tag{6}
    \end{bmatrix}
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(p\)</span> is the token’s position in a sequence. Let us revisit equation(1) now:</p>
<p><span class="math display">\[
\begin{align*}
    &amp; q_m = f_q(x_m, m) \\
    &amp; k_n = f_k(x_n, n)  \\
    &amp; v_n = f_v(x_n, n)
\end{align*}
\]</span></p>
<p>The term <span class="math inline">\(q^T_mk_n\)</span> enables knowledge conveyance between tokens at different positions in the attention mechanism. We are interested in finding a transformation with the hope that the inner product encodes position information only in the relative form:</p>
<p><span class="math display">\[
\begin{align*}
\langle f_q(\boldsymbol{x}_m, m), f_k(\boldsymbol{x}_n, n)\rangle = g(\boldsymbol{x}_m, \boldsymbol{x}_n, m-n).
\end{align*}
\]</span></p>
<p>Let us check how RoPE fulfills this criteria. Note that <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> represents the <span class="math inline">\(mth\)</span> and <span class="math inline">\(nth\)</span> positions respectively. Also, to clarify <span class="math inline">\(q\)</span> and <span class="math inline">\(k\)</span> represent a single query and a single key. Do not get confused by the notation.</p>
<p><span class="math display">\[
\begin{align*}
    q_m &amp;= f_q(x_m, m) = R_\Theta^m \ W_q x_m \\
    k_n &amp;= f_k(x_n, n) = R_\Theta^n \ W_k x_n \\ \\
    q_m^T \ k_n &amp;= x_m^T W_q^T \ (R_\Theta^m)^T \ \ R_\Theta^n \ W_k x_n \\
              &amp;= x_m^T W_q^T \ R_\Theta^{-m}  \ \   R_\Theta^n \ W_k x_n \quad
                    \text{(Transpose of a rotation matrix is its inverse)} \\
              &amp;= x_m^T W_q^T \ R_\Theta^{n-m} \ W_k x_n \\
\end{align*}
\]</span></p>
<p>So, RoPE is encoding the relative position information in the attention calculation, which we wanted. Remember the point we made about stochasticity in sinusoidal encodings? Does RoPE fare better in tha aspect? Here is how our vector moves in 2D if we use RoPE instead of sinusoidal encodings:</p>
<p><video src="./animations/rope/Rope2DOneVector.mp4" class="img-fluid" autoplay="true," loop="true" controls=""><a href="./animations/rope/Rope2DOneVector.mp4">Video</a></video></p>
<p>We looked only at a 2D case and want to generalize this for <span class="math inline">\(d\)</span> dimensions with <span class="math inline">\(d &gt; 2\)</span>. But before doing that, let us look at it from another angle, similar to what was presented in the Roformer paper.</p>
</section>
<section id="rotary-position-encoding-the-mathematical-view" class="level1">
<h1>Rotary Position Encoding: The Mathematical View</h1>
<p>We are looking for something that can encode relative positions in the attention mechanism, and it should not be an additive method like sinusoidal position encoding. Mathematics is the answer to all our problems! We will exploit the geometrical properties of vectors, especially the complex form/representation of vectors. But why the complex form? We will get an answer to this question once we have finished our quest to find the desired transformation for our query and key vectors.</p>
<p>Let us rewrite our query and key transforms presented in equation 1, in complex form:</p>
<p><span class="math display">\[
\begin{align*}
q_m &amp;= f_q(\boldsymbol{x}_q, m) = R_q(\boldsymbol{x}_q, m) \ e^{i\Theta_q(\boldsymbol{x}_q,m)} \\
k_n &amp;= f_k(\boldsymbol{x}_k, n) = R_k(\boldsymbol{x}_k, n) \ e^{i\Theta_k(\boldsymbol{x}_k,n)} \tag{7}
\end{align*}
\]</span></p>
<p>Assuming there exists a transformation <span class="math inline">\(g\)</span> shown below capable of encoding relative position, we aim to find a solution of <span class="math inline">\(f_q\)</span> and <span class="math inline">\(f_k\)</span></p>
<p><span class="math display">\[
\begin{align*}
g(\boldsymbol{x}_m, \boldsymbol{x}_n, m-n) &amp;= \langle f_q(\boldsymbol{x}_m, m), f_k(\boldsymbol{x}_n, n)\rangle \\
    &amp;=  \langle R_q(\boldsymbol{x}_q, m) \ e^{i\Theta_q(\boldsymbol{x}_q, \ m)},
        R_k(\boldsymbol{x}_k, n) \ e^{i\Theta_k(\boldsymbol{x}_k,\ n)} \rangle \\
    &amp;=  R_g(\boldsymbol{x}_q, \boldsymbol{x}_k, n-m) \
        e^{i\Theta_g(\boldsymbol{x}_q, \ \boldsymbol{x}_k, \ n-m)} \tag{8} \\ \\
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(R\)</span> and <span class="math inline">\(\Theta\)</span> represents radical and angular components respectively. Plugging them in equation 7, gives us:</p>
<p><span class="math display">\[
\begin{align*}
R_q(\boldsymbol{x}_q, m)R_k(\boldsymbol{x}_k, n) &amp;= R_g(\boldsymbol{x}_q, \boldsymbol{x}_k, n-m) \\
\Theta_k(\boldsymbol{x}_k, n) - \Theta_q(\boldsymbol{x}_q, m) &amp;= \Theta_g(\boldsymbol{x}_q, \boldsymbol{x}_k, n-m) \tag{9}
\end{align*}
\]</span></p>
<p>Also, when no positional information is provided, we expect the following conditions to be satisfied:</p>
<p><span class="math display">\[
\begin{align*}
    \boldsymbol{q} &amp;= \|q\|e^{i\theta_q} = f_q(\boldsymbol{x}_q, 0) = R_q(\boldsymbol{x}_q,0)e^{i\Theta_q(\boldsymbol{x}_q,0)} \\
    \boldsymbol{k} &amp;= \|k\|e^{i\theta_k} = f_q(\boldsymbol{x}_k, 0) = R_k(\boldsymbol{x}_k,0)e^{i\Theta_k(\boldsymbol{x}_k,0)} \tag{10}
\end{align*}
\]</span></p>
<p>When <span class="math inline">\(m=n\)</span>, we have:</p>
<p><span class="math display">\[
\begin{align*}
    R_q(\boldsymbol{x}_q, m)R_k(\boldsymbol{x}_k, m) &amp;= R_g(\boldsymbol{x}_q, \boldsymbol{x}_k, 0) \ \text{using eq. 8} \\
    &amp;= R_q(\boldsymbol{x}_q, 0)R_k(\boldsymbol{x}_k, 0) \\
    &amp; = \|q\|\|k\| \tag{11a} \\
    \Theta_k(\boldsymbol{x}_k, m) - \Theta_q(\boldsymbol{x}_q, m) &amp;= \Theta_g(\boldsymbol{x}_q, \boldsymbol{x}_k, 0) \\
    &amp;= \Theta_k(\boldsymbol{x}_k, 0) - \Theta_q(\boldsymbol{x}_q, 0) \\
    &amp;= \theta_k - \theta_q \tag{11b}
\end{align*}
\]</span></p>
<p><br>From equation <span class="math inline">\((11a)\)</span>, we have <span class="math inline">\(R_g(\boldsymbol{x}_q, \boldsymbol{x}_k, n-m) = R_g(\boldsymbol{x}_q, \boldsymbol{x}_k, 0) = \|q\|\|k\|\)</span> implying that all radial components <span class="math inline">\(R_g, R_q, R_k\)</span> are independent from the position information.</p>
<p>Similarly, from equation <span class="math inline">\((11b)\)</span>, we have <span class="math inline">\(\Theta_q(\boldsymbol{x}_q, m) - \theta_q = \Theta_k(\boldsymbol{x}_k, m) - \theta_k \text{ for all } q, k, m\)</span>, which indicates that the angular components do not dependent on the query and the key vectors, and depends only on the position <span class="math inline">\(m\)</span>. We can simply this by rewriting the above equation:</p>
<p><span class="math display">\[
\begin{align*}
    \Theta_q(\boldsymbol{x}_q, m) - \theta_q &amp;= \Theta_k(\boldsymbol{x}_k, m) - \theta_k \\
    \Theta_q(\boldsymbol{x}_q, m) - \Theta_k(\boldsymbol{x}_k, m) &amp;= \theta_q - \theta_k \\
    \Theta_f(\boldsymbol{x}_{\{q, k\}}, m) - \theta_{\{q, k\}} &amp;= \phi(m) \tag{12}
\end{align*}
\]</span></p>
<p><br>Suppose we have <span class="math inline">\(n=m+1\)</span>, plugging this in equation (9), and using equation (12), we get:</p>
<p><span class="math display">\[
\phi(m+1) - \phi(m) = \Theta_g(\boldsymbol{x}_q, \boldsymbol{x}_k, 1) + \theta_q - \theta_k
\]</span></p>
<p><br>Since RHS in the above equation is a constant and does not depend on <span class="math inline">\(m\)</span>, with continuous integer inputs, it produces an arithmetic progression that can be written as:</p>
<p><span class="math display">\[
\phi(m) = m\theta + \gamma \tag{13}
\]</span></p>
<p>where <span class="math inline">\(\theta, \gamma \in \mathbb{R}\)</span> are constants and <span class="math inline">\(\theta\)</span> is non-zero. We can simply set <span class="math inline">\(\gamma=0\)</span>.</p>
<p>Therefore, for a 2D case, we can write <span class="math inline">\(f_q\)</span> and <span class="math inline">\(f_k\)</span> as:</p>
<p><span class="math display">\[
\begin{align*}
f_q(\boldsymbol{x}_m, m) &amp;= (W_q\boldsymbol{x}_m)e^{im\theta} \\
f_k(\boldsymbol{x}_n, n) &amp;= (W_k\boldsymbol{x}_n)e^{in\theta} \\
g(\boldsymbol{x}_m, \boldsymbol{x}_n, m-n) &amp;= Re[(W_q\boldsymbol{x}_m)(W_k\boldsymbol{x}_n)^*e^{i(m-n)\theta}]
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(Re[·]\)</span> is the real part of a complex number and <span class="math inline">\((W_k x_n)^*\)</span> represents the conjugate complex number of <span class="math inline">\((W_k x_n)\)</span>. We can further write <span class="math inline">\(f_{\{q, k\}}\)</span> in the form of matrix multiplication as:</p>
<p><span class="math display">\[
\begin{align*}
    f_{\{q,k\}}(\boldsymbol{x}_m, m) =
    \begin{pmatrix}
        \cos m\theta &amp; -\sin m\theta \\
        \sin m\theta &amp; \cos m\theta
    \end{pmatrix}
    \begin{pmatrix}
        W_{\{q,k\}}^{(11)} &amp; W_{\{q,k\}}^{(12)} \\
        W_{\{q,k\}}^{(21)} &amp; W_{\{q,k\}}^{(22)}
    \end{pmatrix}
    \begin{pmatrix}
        x_m^{(1)} \\
        x_m^{(2)}
    \end{pmatrix}  \tag{13}
\end{align*}
\]</span></p>
</section>
<section id="beyond-2d" class="level1">
<h1>Beyond 2D</h1>
<p>Generalizing our 2D results to <span class="math inline">\(d\)</span> dimensions is easy. If our embeddings are d-dimensional (<span class="math inline">\(d&gt;2\)</span>, and d is even), we can split them into <span class="math inline">\(d/2\)</span> blocks, and for each block, we can repeat the same thing we have for the 2D case. That way, we will end up with a diagonal rotation matrix, where the value of theta differs for each dimension. We can then apply the rotation matrix to each block independently and combine the results afterward. The rotation speed varies across the blocks.</p>
<p><span class="math display">\[f_{\{q,k\}}(x_m,m) = \boldsymbol{R}^d_{\Theta,m}\boldsymbol{W}_{\{q,k\}}x_m \tag{14}\]</span></p>
<p>where</p>
<p><span class="math display">\[
\boldsymbol{R}^d_{\Theta,m} =
        \begin{pmatrix}
            \cos m\theta_1 &amp; -\sin m\theta_1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
            \sin m\theta_1 &amp; \cos m\theta_1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
            0 &amp; 0 &amp; \cos m\theta_2 &amp; -\sin m\theta_2 &amp; \cdots &amp; 0 &amp; 0 \\
            0 &amp; 0 &amp; \sin m\theta_2 &amp; \cos m\theta_2 &amp; \cdots &amp; 0 &amp; 0 \\
            \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
            0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \cos m\theta_{d/2} &amp; -\sin m\theta_{d/2} \\
            0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \sin m\theta_{d/2} &amp; \cos m\theta_{d/2}
\end{pmatrix}  \tag{15}
\]</span></p>
<p><br>RoPE is then applied to the query and key vectors for attention calculation as:</p>
<p><span class="math display">\[
\boldsymbol{q}_m^\top \boldsymbol{k}_n =
(\boldsymbol{R}_{\Theta,m}^d \boldsymbol{W}_q \boldsymbol{x}_m)^\top (\boldsymbol{R}_{\Theta,n}^d \boldsymbol{W}_k \boldsymbol{x}_n)
= \boldsymbol{x}_m^\top \boldsymbol{W}_q^\top \boldsymbol{R}_{\Theta,n-m}^d \boldsymbol{W}_k \boldsymbol{x}_n
\tag{16}
\]</span></p>
<p><br>Here is an example with <span class="math inline">\(d=4\)</span>. The query vector is split into blocks of size two. Similarly, the key vector is divided, and rotation is applied to each group independently. Notice the difference in the speed of the two pointers shown below.</p>
<p><video src="./animations/rope/Rope2DTwoVectors.mp4" class="img-fluid" autoplay="true," loop="true" controls=""><a href="./animations/rope/Rope2DTwoVectors.mp4">Video</a></video></p>
<p><br>A recent <a href="https://arxiv.org/abs/2410.06205">paper</a> from DeepMind found some interesting aspects about these frequencies. For example, in the case of Gemma-7B, these high frequencies are responsible for the <em>diagonal head</em> and the <em>previous token head</em> in some layers.</p>
<p><img src="./images/rope/frquency.png" class="img-fluid" width="750"></p>
<p><br>On the other hand, the low frequencies are not very sensitive to the relative distance. It helps the transformers to maintain <em>semantic attention</em> over a large context. This is why models with very long context windows tend to use a very high value for the base frequency (theta) in practice. For example, with a context length of 128k, LLama 3 uses a base frequency of 500,000 leading the low frequencies to rotate at roughly <span class="math inline">\(1/500,000\)</span> radians per token.</p>
</section>
<section id="optimization" class="level1">
<h1>Optimization</h1>
<p>There is one major problem with equation 16. Our rotation matrix is sparse, and though sparsity is good in many places, it is not in this case. We are wasting memory because of that sparsity. Another issue is that equation 16 is also not computationally efficient.</p>
<p>We can fix both the issue with a bit of cleverness. Taking advantage of that sparsity along with elementwise operations, we can do that computation more efficiently, as shown below:</p>
<p><span class="math display">\[
\boldsymbol{R}^d_{\Theta,m} \boldsymbol{x} =
\begin{pmatrix}
    x_1 \\ x_2 \\ x_3 \\ x_4 \\ \vdots \\ x_{d-1} \\ x_d
\end{pmatrix}
\otimes
\begin{pmatrix}
    \cos m\theta_1 \\ \cos m\theta_1 \\ \cos m\theta_2 \\ \cos m\theta_2 \\ \vdots \\ \cos m\theta_{d/2} \\ \cos m\theta_{d/2}
\end{pmatrix}
+
\begin{pmatrix}
    -x_2 \\ x_1 \\ -x_4 \\ x_3 \\ \vdots \\ -x_d \\ x_{d-1}
\end{pmatrix}
\otimes
\begin{pmatrix}
    \sin m\theta_1 \\ \sin m\theta_1 \\ \sin m\theta_2 \\ \sin m\theta_2 \\ \vdots \\ \sin m\theta_{d/2} \\ \sin m\theta_{d/2}
\end{pmatrix}
\tag{17}
\]</span></p>
<p><br></p>
</section>
<section id="what-else" class="level1">
<h1>What else?</h1>
<ol type="1">
<li><strong>Is RoPE all we need?</strong> Though RoPE seems the perfect encoding, it is far from ideal. For example, the idea of long-term decay does not hold in many situations with RoPE.</li>
<li><strong>What about other options like NoPE, HoPE, etc?</strong> None of these encodings is a clear winner in every case. In many situations, sinusoidal is more than sufficient. The advantage of using RoPE is that it is a solid baseline over others.</li>
<li><strong>Does RoPE work well for other modalities?</strong> People like to slap RoPE in every possible situation. Have you ever wondered how RoPE behaves with image tokens? You may find a thing or two in there.</li>
<li><strong>What about extending the context length?</strong> We will talk about it in the next part</li>
</ol>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
<li><a href="https://arxiv.org/abs/2104.09864">Roformer</a></li>
<li><a href="https://arxiv.org/abs/2410.06205">Round and Round We Go!</a></li>
<li><a href="https://blog.eleuther.ai/rotary-embeddings/">Rotary Embeddings: A Relative Revolution</a></li>
<li><a href="https://www.youtube.com/watch?v=GQPOtyITy54">RoPE explained</a></li>
<li><a href="https://www.youtube.com/watch?v=SMBkImDWOyQ">How Rotary Position Embedding Supercharges Modern LLMs</a></li>
<li><a href="https://docs.manim.community/en/stable/examples.html">Manim examples</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>