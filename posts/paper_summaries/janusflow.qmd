---
title: "JanusFlow"
subtitle: "Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding
and Generation"
author: "Aakash Kumar Nain ([@A_K_Nain](https://x.com/A_K_Nain))"
date: "2024-11-25"
categories: [papers, summary, research, MLLMs, generation]
image: ""
format:
  html:
    theme: default
    fontsize: 2em
    backgroundcolor: rgb(255, 255, 255);
    code-fold: true
    code-summary: "Show the code"
    highlight-style: oblivion
    css: styles.css
execute: 
  echo: false
---

[arXiv](https://arxiv.org/abs/2411.07975)<br>
[annotated_paper](https://github.com/AakashKumarNain/annotated_research_papers/blob/master/MLLMs/janusflow.pdf)


We all have been impressed by the quality of models produced by Deepseek. I thought Qwen was good, but
the main highlight is JanusFlow. Apart from the MM1 paper from Apple, I believe JanusFlow is one of the
best papers on modern MLLMs. It combines both image understanding and image generation in a single model.
The JanusFlow paper is crisp and to the point.


# MultiModal Understanding

In multimodal understanding tasks, the LLM processes an input sequence consisting of interleaved text and
image data.

* Text is tokenized into discrete tokens. Each token is transformed into an embedding of dimension ğ·ğ‘’ğ‘šğ‘.
* An image encoder f_enc encodes images into feature maps of shape ğ»ğ‘–ğ‘š Ã— ğ‘Šğ‘–ğ‘š Ã— ğ·ğ‘’ğ‘›ğ‘. The authors use a
pre-trained SigLIP-Large-Patch/16 model as the image encoder (f_enc).
* The image feature map is flattened and projected through a linear transformation layer into a sequence
of embeddings with shape (ğ»_ğ‘–ğ‘š ğ‘Š_ğ‘–ğ‘š) Ã— ğ·ğ‘’ğ‘šğ‘.
* The text and image embeddings are concatenated to form the input sequence to the LLM. Special tokens
|BOI| and |EOI| are inserted before and after the image tokens to help the model locate the image
embeddings in the sequence.
* Based on the above sequence of input embeddings, the LLM predicts the next token autoregressively.

<br>
![](../paper_screenshots/janusflow/1.png)
<br>

# Image Generation

The same LLM used for multimodal understanding employs rectified flow for image generation.

* Generation occurs in the latent space using a pre-trained SDXL-VAE.
* The LLM takes a text sequence ğ‘¥_ğ‘ğ‘œğ‘› as a condition and generates a corresponding image using rectified
flow.
* Start by sampling Gaussian noise z0 of shape ğ»_ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘¡ Ã— ğ‘Š_ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘¡ Ã— ğ·_ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘¡ in the latent space.
* A generation encoder g_enc transforms the above image(noise in the beginning) into a sequence of
embeddings (H_ğ‘”ğ‘’ğ‘› W_ğ‘”ğ‘’ğ‘›) Ã— ğ·ğ‘’ğ‘šğ‘. The authors use ConvNext blocks initialized from scratch in the
generation encoder here.
* The above sequence is then concatenated with a time embedding representing the current time step 
ğ‘¡ (ğ‘¡ = 0 at the beginning), resulting in a sequence of length 
(ğ»_ğ‘”ğ‘’ğ‘› â€¢ ğ‘Š_ğ‘”ğ‘’ğ‘› + 1). Special token |BOI| is prepended to indicate the start of image generation in the
sequence.
* The LLM predicts the next token autoregressively. The output is then transformed to a velocity vector
of shape ğ»_ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘¡ Ã— ğ‘Š_ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘¡ Ã— ğ·_ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘¡ by a generation decoder g_dec. The authors use ConvNext blocks
initialized from scratch in the generation decoder.
* The transformed output state is then updated by a standard Euler solver: ğ‘§(ğ‘¡+dğ‘¡) = ğ‘§_ğ‘¡ + ğ‘£(ğ‘§_ğ‘¡, ğ‘¡)dğ‘¡.
The step size dt is defined by the user.
* Use z_dt as the input, replacing z0 in the above steps iteratively till we obtain z1.
* To improve the image generation quality, the authors use the good old classifier-free guidance (CFG).
ğ‘¤ â©¾ 1 controls the magnitude of CFG, and increasing ğ‘¤ yields higher semantic alignment.


Pay attention to the details here. Not only do the authors employ separate image encoders for
understanding and generation, but they also use different kinds of models, SigLIP for understanding and
ConvNext for generation.
