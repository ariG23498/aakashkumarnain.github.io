---
title: "AIMv2"
subtitle: "Multimodal Autoregressive Pre-training of Large Vision Encoders"
author: "Aakash Kumar Nain ([@A_K_Nain](https://x.com/A_K_Nain))"
date: "2024-11-27"
categories: [papers, summary, research, MLLMs]
image: ""
format:
  html:
    theme: default
    fontsize: 2em
    backgroundcolor: rgb(255, 255, 255);
    code-fold: true
    code-summary: "Show the code"
    highlight-style: oblivion
    css: styles.css
execute: 
  echo: false
---

[arXiv](https://arxiv.org/abs/2411.14402)<br>

The multimodality space is now evolving in a much better way. The focus has shifted to finding the bottlenecks and fixing things on the fundamental level in multimodality. This paper from Apple introduces AIMv2, and effort is in a similar direction, except that they only do it for the autoregressive models.

<br>
![](../paper_screenshots/aim_v2/1.png)
<br>

AIMv2, a family of open vision models pre-trained to generate image patches and text tokens autoregressively. Unlike the other models in this category, it does not require extremely large batch sizes or specialized inter-batch communication methods.


# Pretraining

* Integrates both images and text into a unified sequence.
* An image is split into `I` non-overlapping patches and the text is broken down into subwords. These sequences are concatenated, allowing text tokens to attend to image tokens. The image patches are normalized.
* (image, text) is chosen as the desired sequence to enable stronger conditioning on the visual features.
NTP is applied to the above sequence regardless of the modality.
* Separate loss functions for the image and text parts are defined as shown below:
*  <br>![](../paper_screenshots/aim_v2/2.png)<br>
* The overall objective is to minimize $L= L(text) + α ∗ L(img)$. $L(text)$ is a standard cross-entropy loss applied to the text domain, whereas $L(img)$ is an l2 pixel-level regression loss for the image domain. The model predicts the next token for the text and the next patch for the image part.
* Separate linear layers are used for different modalities to map the output of the multimodal decoder to appropriate output dimensions for image patches and vocabulary, respectively.
