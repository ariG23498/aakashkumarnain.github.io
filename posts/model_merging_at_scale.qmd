---
title: "What Matters for Model Merging at Scale?"
subtitle: ""
author: "Aakash Kumar Nain ([@A_K_Nain](https://x.com/A_K_Nain))"
date: "2024-10-15"
categories: [papers, summary, transformers, research, LLMs, model_merging]
image: ""
format:
  html:
    theme: default
    fontsize: 2em
    backgroundcolor: rgb(255, 255, 255);
    code-fold: true
    code-summary: "Show the code"
    highlight-style: oblivion
execute: 
  echo: false
---

Yesterday, I read this banger paper titled: **What Matters For Model Merging At Scale?**.
Though I recommend reading the full paper, I am including a summary here in case you are interested
in the main points. I will also provide the link to an annotated version of this paper.

<br><br>
![](paper_screenshots/model_merging_at_scale/1.png)
<br><br>

# Introduction
Model merging is not a new concept. It has been tried and tested enough to create a better or
more powerful model by combining two or more (expert) models. Model merging has several advantages:

* Reduced storage and serving costs
* Improved generalization to new tasks due to combined capabilities.
* Decentralized and modular model development.


One potential gap in this area is the lack of a comprehensive study to evaluate its effectiveness as
we scale the model size. Most people are either merging models at a small scale (7B-13B models) or
merging a limited number of expert models. This paper provides insights into the scalability of
model merging.

<br><br>
# Problem Statement
* Focuses on model merging with large models
* N expert tasks and a base model
* Expert for each of these N tasks obtained by fully fine-tuning the base model on a specific expert task.
* Four merging methods, including *Averaging*, *Task Arithmetic*, *TIES*, and *DARE*.