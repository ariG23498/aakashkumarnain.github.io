---
title: "Rotary Position Embeddings"
author: "Aakash Kumar Nain ([@A_K_Nain](https://x.com/A_K_Nain))"
date: "2024-11-28"
categories: [llms, embeddings, position encoding, advanced]
image: ""
format:
  html:
    theme: default
    fontsize: 2em
    backgroundcolor: rgb(255, 255, 255);
    code-fold: true
    code-summary: "Show the code"
    css: styles.css
execute: 
  echo: false
---

# Why bother with position encoding?

Self-attention in transformer-based models is one of the most used operations in the history of deep
learning. Though self-attention is extremely powerful, one of the weaknesses of self-attention is that
it treats a sequence as a set of tokens. It is not position-aware, and it is permutation equivariant. 

The order of words in a language matters; hence, we need a mechanism to insert word order information
in our model. This is where position encoding kicks in.


# Preliminary

Let $S_N=\{w_i\}^N_i=1$ be a sequence of `N` input tokens with $w_i$ being the ith token. Each token(word in this case) in the sequence is defined by a `d`-dimensional vector embedding containing no position information. The self-attention layer first incorporates position information into the word embeddings and then transforms them into queries, keys, and value representations. We can define these
transforms as shown below:

$$
\begin{align}
& q_m = f_q(x_m, m) \\
& k_n = f_k(x_n, n) \tag{1} \\ 
& v_n = f_v(x_n, n) 
\end{align}
$$

where $m$ and $n$ represents the $mth$ and $nth$ positions respectively.


# Desired properties of a position encoding

If we encode positions for the model, what would be the ideal design? The simplest thing we can do is
initialize a learnable position embedding and let the model figure out the values during training. Not a
bad idea for starters, but can we do better? What are the desirable properties of an encoding scheme if it
is not learned implicitly? Here are a few of them:

1. **A unique value for every position in a sequence:** The encoding scheme should assign a unique value
for every position irrespective of the sequence length. <br><br>
2. **Consistent relative distance between two positions:** Irrespective of the length of a sequence, the
relative distance between two positions should be consistent across sequences. For example, the relative
distance between encodings of the 2nd position and 3rd position should be the same in sequences of
different lengths. <br><br>
3. **Long-term decay:** An inductive bias about words is that words far distant from the current word
carry less relevant information. That means our position encoding should follow a long-term decay effect
for relatively distant positions. <br><br>
4. **Extensible:** What if we encounter a lengthier sequence at test time than the length of any sequence
encountered during training? Ideally, we want our encoding scheme to be extensible, without much effort
and breaking any other assumption. <br><br>
5. **Deterministic:** Determinism is mostly a nice-to-have property. It can help debug a few aspects if
we encounter something unexpected.


There are different encoding schemes, e.g., absolute position encoding, binary encoding, relative
position encoding, rotary position encoding, etc. Here, we will discuss the two most widely used position
encoding: sinusoidal position encoding and rotary position encoding. Assuming the reader is familiar with concepts like relative position encoding, we will skip that discussion here. We could have skipped sinusoidal position encoding, but it lays the foundation of rotary position encoding. Also, we will try to keep the notations in line with the Roformer paper as much as possible. <br>


# Sinusoidal Positional Encoding

A typical choice for equation(1) is to formulate it this way:
$$
f_t(x_i, i) = W_t(x_i + p_i) \tag{2} \\
$$

Sinusoidal encoding was proposed in the paper Attention is All You Need. They proposed to generate $p_i$ in the above equation using the sinusoidal function:
$$
\begin{cases}
p_{i,2t} &= \sin(k/10000^{2t/d}) \\
p_{i,2t+1} &= \cos(k/10000^{2t/d}) \\ \tag{3}
\end{cases}
$$

where $p_i,2_t$ is the $2t$h element of the d-dimensional vector $p$. The wavelengths form a geometric progression from $2π$ to $10000 · 2π$. 

where $t \in (q, k, v)$, and $p_i$ is represents a `d` dimensional vector depending of the position of token $x_i$.

Many people do not know sinusoidal encoding was suggested so the model can learn to attend to relative positions. Yes, relative positions! This is mostly because people do not read papers with "attention". Let us take an example with $d=2$.

$$
\begin{align*}
    d = 2 \rightarrow i={[0, 1]}
\end{align*}
$$

Let us calculate the position encoding values for $position=pos$ using equation 3:

$$
\begin{align*}
    PE_{pos,0} &= \sin(pos/10000^{2*0/d}) = sin(pos) \\
    PE_{pos,1} &= \cos(pos/10000^{2*0/d}) = cos(pos) \\
\end{align*}
$$

Now, let us calculate the position encoding for an offset $k$ i.e., $position=pos+k$:

$$
\begin{align*}
    PE_{pos+k, 0} &= \sin((pos+k)/10000^{2*0/d}) = \sin(pos+k) \\
    PE_{pos+k, 1} &= \cos((pos+k)/10000^{2*0/d}) = \cos(pos+k) \\
\end{align*}
$$

<br>Expanding $sin(pos+k)$ and $cos(pos+k)$, we have:

$$
\begin{align*}
    \sin(pos+k) &= \sin(pos)\sin(k) + \cos(pos)\cos(k) \\
                &= PE_{pos, 0} \sin(k) + PE_{pos, 1} \cos(k) \tag{4} \\ \\
    \cos(pos+k) &= \cos(pos)\cos(k) - \sin(pos)\sin(k) \\
                &= PE_{pos, 1} \cos(k) - PE_{pos, 0} \sin(k) \tag{5} \\
\end{align*}
$$

We can combine equation(4) and equation(5) in a nice matrix notation:

$$
\begin{align*}
\begin{bmatrix}
    PE_{pos+k,0} \\
    PE_{pos+k,1}
\end{bmatrix} &= 
\underbrace{\begin{bmatrix}
    \cos(k)  & \sin(k) \\ 
    -\sin(k) & \cos(k)
\end{bmatrix}
}_{Rotation \ Matrix}
\begin{bmatrix}
    PE_{pos,0} \\
    PE_{pos,1}
\end{bmatrix}
\end{align*}
$$

That's a rotation matrix! So, our models have been learning to attend relative positions since 2017. If that is the case, what is wrong with sinusoidal position encoding?

Before we answer that question, take a look at the visualization below. There is a two-dimensional vector x=(1, 1), and we add the sinusoidal position encoding for different positions to this vector.

![](./animations/sinsuoidal.mp4){autoplay=true}

<br><br>

Though we assume that the model can attend to relative positions easily, the one thing that is a bit bothering is the stochasticity. Our pointer is moving almost randomly. The pattern here does not look so good, but it doesn't necessarily mean it is also bad for the model to learn, especially at scale. All we can say is that with so much stochasticity across different dimensions, the model will have a hard time learning that relative positions can be attended using a rotation matrix and that the model may start memorizing. 

Another thing to note from equation(2) is that sinusoidal positional encodings are additive. This means indirectly adding the  "relatedness" of two tokens to the positional encoding. A side effect of this is that two highly related tokens can get high attention scores irrespective of the distance between them. This is still okay for NLP-related problems but doesn't hold for other domains like protein sequencing.

The authors of the Attention is All You Need paper also hypothesized that hard-coded sinusoidal positional encodings may help to extrapolate to longer sequences compared to the length of the sequences seen during training. The hypothesis does not hold in practice.

<br><br>



