---
title: "Rotary Position Embeddings"
author: "Aakash Kumar Nain ([@A_K_Nain](https://x.com/A_K_Nain))"
date: "2024-11-28"
categories: [llms, embeddings, position encoding, advanced]
image: ""
format:
  html:
    theme: default
    fontsize: 2em
    backgroundcolor: rgb(255, 255, 255);
    code-fold: true
    code-summary: "Show the code"
execute: 
  echo: false
---

# Why bother with position encoding?

Self-attention in transformer-based models is one of the most used operations in the history of deep
learning. Though self-attention is extremely powerful, one of the weaknesses of self-attention is that
it treats a sequence as a set of tokens. It is not position-aware, and it is permutation equivariant. 

The order of words in a language matters; hence, we need a mechanism to insert word order information
in our model. This is where position encoding kicks in.


# Desired properties of a position encoding

If we encode positions for the model, what would be the ideal design? The simplest thing we can do is
initialize a learnable position embedding and let the model figure out the values during training. Not a
bad idea for starters, but can we do better? What are the desirable properties of an encoding scheme if it
is not learned implicitly? Here are a few of them:

1. **A unique value for every position in a sequence:** The encoding scheme should assign a unique value
for every position irrespective of the sequence length. 
2. **Consistent relative distance between two positions:** Irrespective of the length of a sequence, the
relative distance between two positions should be consistent across sequences. For example, the relative
distance between encodings of the 2nd position and 3rd position should be the same in sequences of
different lengths. 
3. **Long-term decay:** An inductive bias about words is that words far distant from the current word
carry less relevant information. That means our position encoding should follow a long-term decay effect
for relatively distant positions.
4. **Extensible:** What if we encounter a lengthier sequence at test time than the length of any sequence
encountered during training? Ideally, we want our encoding scheme to be extensible, without much effort
and breaking any other assumption.
5. **Deterministic:** Determinism is mostly a nice-to-have property. It can help debug a few aspects if
we encounter something unexpected.


There are different encoding schemes, e.g., absolute position encoding, binary encoding, relative
position encoding, rotary position encoding, etc. Here, we will discuss the two most widely used position
encoding: sinusoidal position encoding and rotary position encoding. I could have covered relative
position encoding, but I assume the reader is familiar with these concepts. If not, I will provide some
resources about them. Also, I could have skipped sinusoidal position encoding, but it lays the foundation
of rotary position encoding.


# Preliminary

Let $S_N=\{w_i\}^N_i=1$ be a sequence of `N` input tokens with $w_i$ being the ith token. Each token(word in this case) in the sequence is defined by a `d`-dimensional vector embedding containing no position information. The self-attention layer first incorporates position information into the word embeddings and then transforms them into queries, keys, and value representations. We can define these
transforms as shown below:

$$
\begin{align}
& q_m = f_q(x_m, m) \\
& k_n = f_k(x_n, n) \tag{1} \\ 
& v_n = f_v(x_n, n) 
\end{align}
$$

where $m$ and $n$ represents the $mth$ and $nth$ positions respectively. We can simplify these
transformations in a single equation as:

$$
f_t(x_i, i) = W_t(x_i + p_i) \tag{2} \\
$$

where $t \in (q, k, v)$, and $p_i$ is represents a `d` dimensional vector depending of the position of token $x_i$.

